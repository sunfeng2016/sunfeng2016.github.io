<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《机器学习》西瓜书学习笔记（二）-- 模型评估与选择]]></title>
    <url>%2F2019%2F08%2F05%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[《机器学习》西瓜书学习笔记（二）模型评估与选择 经验误差与过拟合 错误率 (error rate)： 分类错误样本数占总样本数的比例 准确率 (accuracy)： 分类正确样本数占总样本数的比例 误差 (error)： 学习器的实际预测输出与真实输出之间的差异 训练误差 (training error)/经验误差(empirical error)： 学习器在训练集上的误差 泛化误差(generalization error): 学习器在新样本上的误差 过拟合(overfitting): 学习能力过于强大。学习器把训练样本学得太好，导致将训练样本中自身含有的特点当成所有潜在样本都会具有的一般性质，从而训练后使得泛化性能下降 欠拟合(underfitting): 学习能力低下，对训练样本的一般性质尚未学好 评估方法理想： 通过评估学习器的泛化误差，选出泛化误差最小的学习器 实际： 泛化误差 只能通过测试集上的 测试误差 作为近似 机器学习的目的是产生泛化能力好的模型，那么什么样的模型才是泛化能力好的模型呢? 这需要按照一定的评估方法和度量指标去衡量。 给定一个包含m个样例的数据集 $ D = \{(x_1, y_1), (x_2, y_2), … ,(x_m, y_m)\}$ ,通过对D进行适当的处理，从中产生出训练集S和测试集T，测试集应该尽可能与训练集互斥，常见的方法有以下三种： 1.留出法 (hold-out)留出法： 直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个集合作为测试集T，即 D=S∪T，S∩T=∅. 注意点： 要保持数据分布的一致性 分层采样 采用多次随机划分取均值的评估方法 训练集的比例应当适当(2/3 ~ 4/5) 2.交叉验证法 (cross validation)交叉验证法： 将数据集平均分成 K 份，并尽量保证每份数据分布一致。依次用其中 K - 1 份作为训练集，剩下的一份作为测试集。这样就有 K 组训练/测试集。从而可以进行 K 次训练和测试，返回K次测试结果的均值，也称为”K折交叉验证法” 注意点： K折交叉验证要随机使用不同的划分重复p次，最终取p次K折交叉验证的均值 留一法： 若令K = m, 则称为“留一法 (LOO)” 优点 不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集，即每个子集只含有一个样本 被实际评估的模型与期望评估的用D训练出的模型很相似，因为使用的训练集与初始数据集相比只少了一个样本 缺点 当数据集比较大时，训练m个模型的计算开销比较大 3.自助法 (bootstrapping)自助法： 给定包含m个样本的数据集 D，从 D 中进行有放回地采样产生包含 m 个样本的数据集 D’，这样 D 中大概有36.8%的样本不会出现在 D’ 中，将 D’ 用作训练集，D - D’ 用作测试集 (即在D’ 中没出现的样本) 优点： 实际评估模型和期望评估模型都使用 m 个训练样本 保证了仍有数据总量约1/3的、没在训练集中出现的样本用于测试 自助法在数据量较小，难以有效划分训练集/测试集时很有用 缺点： 自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差 性能度量性能度量(performance measure)： 衡量模型泛化能力的评价标准 回归任务 均方误差 分类任务 错误率和精度 查准率、查全率和F1 ROC和AUC 代价敏感错误率和代价曲线 1. 回归任务的性能度量 —— 均方误差离散样本： E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}{(f(x_i)-y_i)^{2}}连续样本： 设 数据分布 D 和 概率密度 p(·) E(f;D) = \int_{x~D}{(f(x) - y)^2p(x)dx}性能度量方法： 通常，均方误差大的模型性能差，均方误差小的模型性能好。 2. 分类任务的性能度量1 —— 错误率与精度错误率(error rate)： 分类错误的样本占样本总数的比例 E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)\ne y_i)}精度(Accuracy)： 分类正确的样本占样本总数的比例 acc(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)= y_i)} = 1 - E(f;D)性能度量方法： 通常，错误率低精度高的模型性能好，错误率高精度低的模型性能差。 3. 分类任务的性能度量2 —— 查准率、查全率与F1 查准率/准确率(Precision)： 【真正例样本数】与【预测结果是正例的样本数】的比值 P = \frac{TP}{TP + FP}查全率/召回率(Recall)： 【真正例样本数】与【真实情况是正例的样本数】的比值 R = \frac{TP}{TP + FN}解释： 查准率是在讲，挑出的好瓜里头，有多少是真正的好瓜，因此，若希望选出的瓜中好瓜的比例尽可能高，则查准率要高。 查全率是在讲，挑出的真正好瓜，占总共好瓜数的多少，因此，若希望将好瓜尽可能多的选出来，则查全率要高。 性能度量方法： 直接观察数值 建立P-R图 当曲线没有交叉的时候： 外侧学习器的性能优于内侧 当曲线有交叉的时候： 第一种方法是比较曲线下面积，但这个值不易估算 第二种方法是比较两条曲线的平衡点，平衡点是“查准率 = 查全率”时的取值，在图中表示为曲线和对角线的交点，平衡点在外侧的曲线的学习器性能优于内侧 第三种方法是F1度量和Fβ度量。F1是基于查准率与查全率的调和平均定义的，Fβ则是加权调和平均。 F1: 基于查准率和查全率的调和平均 \frac{1}{F1} = \frac{1}{2} · (\frac{1}{P} + \frac{1}{R}) F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}Fβ: 基于查准率和查全率的加权调和平均 \frac{1}{F_\beta} = \frac{1}{1 + \beta^2} · (\frac{1}{P} + \frac{\beta^2}{R}) F_\beta = \frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}说明： β &gt; 0 度量了查全率和查准率的相对重要性 β &gt; 1 时查全率有更大的影响 β &lt; 1 时查准率有更大的影响 4. 分类任务的性能度量3 —— ROC与AUC与P-R图相同，ROC图通过对测试样本设置不同的阈值并与预测值比较，划分出正例和反例。再计算出真正例率和假正例率。P-R图逐个将样本作为正例，ROC图逐次与阈值进行比较后划分正例。本质上，都是将测试样本进行排序。 真正例率(TPR): 【真正例样本数】与【真实情况是正例的样本数】的比值 TPR = \frac{TP}{TP + FN}假正例率(FPR): 【假正例样本数】与【真实情况是反例的样本数】的比值 FPR = \frac{FP}{TN + FP}ROC: 全称是“受试者工作特征” (Receiver Operating Characteristic)曲线，以真正例率为纵轴，以假正例率为横轴 性能度量方法： 绘制ROC曲线 当曲线没有交叉的时候： 外侧曲线的学习器性能优于内侧 当曲线有交叉的时候： 比较ROC面积，如AUC 5. 分类任务的性能度量4 —— 代价敏感错误率与代价曲线前面介绍的性能度量，大都隐式地假设了“均等代价”，而为权衡不同类型错误所造成的不同损失，应为错误赋予：“非均等代价”。 下图为二分类代价矩阵，其中 $cost_{ij}$ 表示将第i类样本预测为第j类样本的代价 代价敏感(cost-sensitive)错误率: E(f;D;cost)=\frac{1}{m}(\sum_{x_i \in D^+} II (f(x_i) \ne y_i) \times cost_{01} + \sum_{x_i \in D^-} II(f(x^i) \ne y_i) \times cost_{10})性能度量的方法：绘制代价曲线 代价曲线的横轴是正例概率代价 $P(+)cost$，纵轴是归一化代价 $cost_{norm}$ P(+)_{cost} = \frac{p\times cost_{01}}{p\times{cost_{01}} + (1-p)\times{cost_{10}}} cost_{norm} = \frac{FNR \times p\times cost_{01} + FPR \times (1 - p) \times cost_{10}}{p \times cost_{01} + (1 - p) \times cost_{10}} 比较检验 \begin{pmatrix} m\\ m^\prime \end{pmatrix} 偏差与方差“偏差—方差分解” ：是解释学习器泛化性能的重要工具，在解释这个概念之前，先明确以下各变量名： 测试样本：x 测试样本x在数据集中的标记：$y_D$ 测试样本x的真实标记：$y$ 训练集：$D$ 从训练集$D$中学的模型：$f$ 模型$f$在测试样本x上的真实输出：$f(x; D)$ 数据分布：Ɗ 1. $\overline f(x)$ (预测输出的期望) ：学习算法的期望预测 \overline f(x) = E_Ɗ[f(x;D)]【所有可能的训练数据集】（数据分布 Ɗ）训练出的【所有模型预测输出】的期望值。 $Variance$ (方差)**：使用样本数相同的不同训练集产生的方差 var(x) = E_Ɗ[(f(x;D) - \overline f(x))^2]【在分布Ɗ上】的【不同训练集训练出的模型】的【预测输出】与【期望预测输出值】的比较。 3. $Bias$ (偏差)：期望输出与真实标记之间的差别 bias^2(x) = (\overline f(x) - y)^2用【所有可能的训练数据集】（数据分布Ɗ）训练出的【所有模型预测输出】的期望与【真实模型的输出值】比较得出的差异。 4. $\varepsilon^2$ (噪声)：数据集标记和真实标记的方差 \varepsilon ^2 = E_Ɗ[(y_Ɗ - y)^2]噪声数据主要来源是训练数据的错误标签的情况以及输入数据某一维不确定的情况。 二项分布参数p的检验设某事件发生的概率为 p ， p 未知，作 m 次独立试验，每次观察该事件是否发生，以X记该事件发生的次数，则X服从二项分布 B(m, p)，现根据X检验如下假设： H_0: p \le p_0\\ H_1: p > p_0由二项分布本身的特性可知：p越小，X取到较小值的概率越大。因此，对于上述假设，一个直观上合理的检验为： \varphi: 当 X \le C 时接受H_0，否则拒绝H_0其中，$C \in N$表示事件最大发生次数。此检验对应的功效函数为： \begin{align} \beta_\varphi(p) &= P(X > C)\\ &= 1 - P(X \le C)\\ &= 1 -\sum_{i = 0}^{C} \begin{pmatrix} m\\ i \end{pmatrix} p^i(1-p)^{m-i}\\ &=\sum_{i=C+1}^{m} \begin{pmatrix} m\\ i \end{pmatrix} p^i(1-p)^{m-i}\\ \end{align}由于“p越小，X取到较小值的概率越大“可以等价表示为：$P(X \le C)$ 是关于p的减函数，所以$\beta_\varphi (p) = P(X &gt; C) = 1 - P(X \le C)$ 是关于p的增函数，那么当 $p \le p_0$ 时，$\beta_\varphi (p_0) $ 即为 $\beta_\varphi (p) $ 的上确界。 又因为，检验水平 $\alpha$ 默认取最小可能水平，所以在给定检验水平 $\alpha$ 时，可以通过如下方程解得满足检验水平$\alpha$ 的整数C: \alpha = sup\{\beta_\varphi(p)\}显然，当$p \le p_0$时： \begin{align} \alpha &= sup\{\beta_\varphi(p)\}\\ &= \beta_\varphi(p_0)\\ &= \sum_{i = C+1}^{m} \begin{pmatrix} m\\ i \end{pmatrix} (p_0)^i(1 - p_0)^{m-i} \end{align}对于此方程，通常不一定正好解得一个整数C使得方程成立，常见的情况是存在这样一个 $\overline {C}$ 使得： \sum_{i = \overline C+1}^{m} \begin{pmatrix} m\\ i \end{pmatrix} (p_0)^i(1 - p_0)^{m-i} < \alpha \sum_{i = \overline C}^{m} \begin{pmatrix} m\\ i \end{pmatrix} (p_0)^i(1 - p_0)^{m-i} > \alpha此时，C只能取 $\overline {C}$ 或者 $\overline {C} + 1$ ，若 C 取 $\overline C$ ，则相当于升高了校验水平 $\alpha$ , 若 C 取 $\overline{C} + 1$ 则相当于降低了检验水平 $\alpha$ ，具体如何取舍需要结合实际情况，但是通常为了减小犯第一类错误的概率，会倾向于令 C 取 $\overline C + 1$. 下面考虑如何求解 $\overline C$: 易证 $\beta_{\varphi}(p_0)$ 是关于 C 的减函数，所以再结合上述关于 $\overline C$ 的两个不等式，易推得： \overline C = minC \quad s.t.\ \sum_{i = C+1}^{m}{p_0^i + (1-p_0)^{m-i}} < \alpha]]></content>
      <categories>
        <category>《机器学习》西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》西瓜书学习笔记（一）-- 绪论]]></title>
    <url>%2F2019%2F08%2F05%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%20%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[《机器学习》西瓜书学习笔记（一）绪论 引言 学习算法(learning algorithm): 机器学习研究的主要内容，是关于在计算机上从数据产生“模型”的算法，即“学习算法”. 学习算法的作用： 基于提供的经验数据产生模型 面对新情况，模型可提供相应的判断 模型(model): 泛指从数据中学到的结果 学习器(learner): 学习算法在给定参数空间上的实例化 基本术语 数据集(data set): 一组记录的集合 示例(instance)/样本(sample)/特征向量(feature vector): 每条记录（关于一个事件或对象的描述）或空间中的每一个点（对应一个坐标向量） 属性(attribute)/特征(feature): 反映事件或对象在某方面的表现或性质的事项 属性值(attribute value): 属性上的取值 属性空间(attribute space)/样本空间(sample space)/输入空间(input space): 属性张成的空间 维数(dimensionality): 属性的个数 学习(learning)/训练(training): 从数据中学的模型的过程 训练数据(training data): 训练过程中使用的数据 训练样本(training sample): 训练数据中的每个样本 训练集(training set): 训练样本组成的集合 假设(hypothesis): 学得模型对应了关于数据的某种潜在的规律 真相/真实(ground-truth): 这种潜在规律的自身 预测(prediction): 获得训练样本的结果信息，才能建立“预测”的模型 标记(label): 关于示例结果的信息 样例(example): 拥有了标记信息的示例 标记空间(label space): 所有标记的集合 分类(classification): 预测的离散值 二分类(binary classification) 正类(positive class) 负类(negative class) 多分类(multi-class classification) 回归(regression): 预测的连续值 测试(testing): 学的模型后，使用其进行预测的过程 测试样本(testing sample): 被预测的样本 学习任务分类： 监督学习(supervised learning): 有标记 分类 回归 无监督学习(unsupervised learning): 无标记 聚类(clustering) 泛化(generalization)能力: 学得模型适用于新样本的能力 假设空间 科学推理: 归纳(induction): 特殊 —-&gt; 一般，泛化 假设(deduction): 一般 —-&gt; 特殊，特化 归纳学习： 广义：从样例中学习 狭义：从训练数据学得概念，概念学习、概念形成 归纳偏好 归纳偏好: 机器学习算法在学习过程中对某种类型假设的偏好 任何一个有效的机器学习算法必有其归纳偏好 “奥卡姆剃刀”原则: 若有多个假设与观察一致，则选最简单的那个 “没有免费的午餐”定理(NFL定理): 总误差与学习算法无关]]></content>
      <categories>
        <category>《机器学习》西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer]]></title>
    <url>%2F2019%2F08%2F03%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%2C%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer%2F</url>
    <content type="text"><![CDATA[Planar data classification with a hidden layer Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. You will learn how to: Implement a 2-class classification neural network with a signal hidden layer Use units with a non-linear activation function, such as tanh Compute the cross entropy loss Implement forward and backward propagation PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental packages for scientific computing with Python. sklearn provides simple and efficient tools for data mining and data and analysis. matplotlib is a library for plotting graphs in Python. testCases provides some test examples to assess the correctness of your functions planar_utils provide various useful functions used in this assignment Code: 12345678910# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases_v2 import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsnp.random.seed(1) # set a seed so that the results are consistent DatasetFirst, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables X and Y. Code: 123456789101112# load datasetdef load_dataset(): ''' load planar dataset Arguments: Returns: @X: your dataset features @Y: your dataset labels ''' X, Y = load_planar_dataset() # load dataset return X, Y Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. Code: 123# Visualize the data:plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);plt.show() Result: You have: a numpy array (matrix) X that contains your features (x1, x2) a numpy array (vector) Y that contains your labels (red: 0, blue: 1) Let’s first get a better sense of what our data is like. Exercise: How many training examples do you have? In addition, what is the shape of the variables Xand Y ? Code: 123456shape_X = X.shapeshape_Y = Y.shapem = X.shape[1] # training set sizeprint ('The shape of X is: ' + str(shape_X))print ('The shape of Y is: ' + str(shape_Y))print ('I have m = %d training examples!' % (m)) Result: 123The shape of X is: (2, 400)The shape of Y is: (1, 400)I have m = 400 training examples! Simple Logistic RegressionBefore building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. Code: 123# Train the logistic regression classifierclf = sklearn.linear_model.LogisticRegressionCV();clf.fit(X.T, Y.T); You can now plot the decision boundary of these models. Run the code below. Code: 12345678# Plot the decision boundary for logistic regressionplot_decision_boundary(lambda x: clf.predict(x), X, Y)plt.title("Logistic Regression")plt.show()# Print accuracyLR_predictions = clf.predict(X.T)print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + "(percentage of correctly labelled datapoints)") Result: 1Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints) Interpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now. Neural Network modelLogistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer. Here is our model: Mathematically: For one example x(i): Given the predictions on all the examples, you can also compute the cost J as follows: J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}Reminder: The general methodology to build a Neural Network is to: Define the neural network structure (# of input units, # of hidden units, etc). Initialize the model’s parameters Loop: Implement forward propagation Compute loss Implement backward propagation to get the gradients Update parameters (gradient descent) You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data. Defining the neural network structureExercise: Define three variables: 123- n_X: the size of the input layer- n_h: the size of the hidden layer (set this to 4)- n_y: the size of the output layer Hint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4. Code: 12345678910111213141516171819# Defining the neural network structuredef layer_sizes(X, Y): ''' Defining the neural network structure Arguments: @x: input dataset of shape (input size, number of examples) @y: labels of shape (output size, number of examples) Returns: @n_x: the size of input layer @n_h: the size of hidden layer @n_y: the size of output layer ''' n_x = X.shape[0] n_h = 4 n_y = Y.shape[0] return (n_x, n_h, n_y) Test: 12345X_assess, Y_assess = layer_sizes_test_case()(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)print("The size of the input layer is: n_x = " + str(n_x))print("The size of the hidden layer is: n_h = " + str(n_h))print("The size of the output layer is: n_y = " + str(n_y)) Result: 123The size of the input layer is: n_x = 5The size of the hidden layer is: n_h = 4The size of the output layer is: n_y = 2 Initialize the model’s parametersExercise: Implement the function initialize_parameters(). Instructions: Make sure your parameters’ sizes are right. Refer to the neural network figure above if you needed. You will initialize the weights matrices with random values. Use: np.random.randn (a, b) * 0.01 to randomly initialize a matrix of shape (a, b). You will initialize the bias vectors as zeros. Use: np.zeros((a, b)) to initialize a matrix of shape (a, b) with zeros. Code: 1234567891011121314151617181920212223242526272829303132333435# Initialize the model's parametersdef initialize_parameters(n_x, n_h, n_y): ''' Initialize the model's parameters Argument: @n_x: the size of the input layer @n_h: the size of the hidden layer @n_y: the size of the output layer Returns: @params: python dictionary containing your parameters: @W1: weight matrix of shape (n_h, n_x) @b1: bias vector of shape (n_h, 1) @W2: weight matrix of shape (n_y, n_h) @b2: bias vector of shape (n_y, 1) ''' np.random.seed(2) # set a seed so that the result are consisent W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) assert(W1.shape == (n_h, n_x)) assert(b1.shape == (n_h, 1)) assert(W2.shape == (n_y, n_h)) assert(b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters Test: 1234567n_x, n_h, n_y = initialize_parameters_test_case()parameters = initialize_parameters(n_x, n_h, n_y)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 12345678910W1 = [[-0.00416758 -0.00056267] [-0.02136196 0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]b1 = [[ 0.] [ 0.] [ 0.] [ 0.]]W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]]b2 = [[ 0.]] The LoopExercise 1: Implement forward_propagation() . Instructions: Look above at the mathematical representation of your classifier. You can use the function sigmoid() . It is built-in (imported) in the notebook (planar_utils.py). You can use the function tanh() . It is part of the numpy library. The steps you have to implement are: Retrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters()) by using paramters[&quot;...&quot;] . Implement Forward Propagation. Compute Z[1], A[1], Z[2] and A[2] (the vector of all your predictions on all the examples in the training set). Values needed in the back propagation are stored in “cache“. The cache will be given as an input to the back propagation function. Code: 123456789101112131415161718192021222324252627282930313233# Implement forward propagationdef forward_propagation(X, parameters): ''' implement forward propagation Argument: @X: input data of size (n_x, m) @parameters: python dictionary containing your parameters Returns: @A2: The sigmoid output of the second activation @cache: a dictionary containing "Z1", "A1", "Z2" and "A2" ''' # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Implement forward propagation to calculate A2 Z1 = np.dot(W1, X) + b1 A1 = tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cache Test: 12345X_assess, parameters = forward_propagation_test_case()A2, cache = forward_propagation(X_assess, parameters)# Note: we use the mean here just to make sure that your output matches ours. print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2'])) Result: 10.262818640198 0.091999045227 -1.30766601287 0.212877681719 Now that you have computed A[2] (in the Python variable “A2”), which contains a[2](i) for every examples, you can compute the cost function as follows: J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}Exercise 2: Implement compute_cost() to compute the value of the cost J. Instructions: There are many ways to implement the cross-entropy loss (交叉熵损失). To help you, we give you how we would have implemented: -\sum_{i = 0}^{m}{y^{(i)}log(a^{[2](i)})}12logprobs = np.multiply(np.log(A2), Y)cost = -np.sum(logprobs) # no need to use a for loop Code: 123456789101112131415161718192021222324# Implement compute costdef compute_cost(A2, Y, parameters): ''' Computes the cross-entropy cost Arguments: @A2: The sigmoid output of the second activation, of shape(1, number of examples) @Y: "true" labels vector of shape (1, number of examples) @parameters: python dictionary containing your parameters W1, b1, W2, b2 Returns: @cost: cross-entropy cost ''' m = Y.shape[1] #number of examples # Compute the cross-entropy cost logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y) cost = -1 / m * np.sum(logprobs) cost = np.squeeze(cost) # make sure cost is the dimension we expect # E.g. turns [[17]] into 17 assert(isinstance(cost, float)) return cost Test: 123A2, Y_assess, parameters = compute_cost_test_case()print("cost = " + str(compute_cost(A2, Y_assess, parameters))) Result: 1cost = 0.693058761 Using the cache computed during forward propagation, you can now implement backward propagation. Exercise 3: Implement the function backward_propagation(). Instructions: Backward propagation is usually the hardest (most mathematical) part in deep learning. o help you, here again is the slide from the lecture on backward propagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation. Tips: To compute dZ1 you’ll need to compute g[1]′(Z[1]). Since g[1](.)is the tanh activation function, if a=g[1](z) then g[1]′(z)=1−a2. So you can compute g[1]′(Z[1]) using (1 - np.power(A1, 2)). Code: 1234567891011121314151617181920212223242526272829303132333435363738# Implement the function backward propagrationdef backward_propagation(parameters, cache, X, Y): ''' Implement the backward propagation Arguments: @parameters: python dictionary containing out parameters (W1, b1, W2, b2) @cache: a dictionary containing "Z1", "A1", "Z2", "A2". @X: input data of shape (2, number of examples) @Y: "true" labels vector of shape(1, number of examples) Returns: @grads: python dictionary containing your gradients with respect to different parameters ''' m = X.shape[1] # retrieve W1 and W2 from the dictionary parameters W1 = parameters["W1"] W2 = parameters["W2"] # retrieve A1 and A2 from dictionary "cache" A1 = cache["A1"] A2 = cache["A2"] # Backward propagation: calculate dW1, db1, dW2, db2 dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis = 1, keepdims = True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis = 1, keepdims = True) grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return grads Test: 1234567parameters, cache, X_assess, Y_assess = backward_propagation_test_case()grads = backward_propagation(parameters, cache, X_assess, Y_assess)print ("dW1 = "+ str(grads["dW1"]))print ("db1 = "+ str(grads["db1"]))print ("dW2 = "+ str(grads["dW2"]))print ("db2 = "+ str(grads["db2"])) Result: 12345678910dW1 = [[ 0.00301023 -0.00747267] [ 0.00257968 -0.00641288] [-0.00156892 0.003893 ] [-0.00652037 0.01618243]]db1 = [[ 0.00176201] [ 0.00150995] [-0.00091736] [-0.00381422]]dW2 = [[ 0.00078841 0.01765429 -0.00084166 -0.01022527]]db2 = [[-0.16655712]] Exercise 4: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2). General gradient descent rule: θ=θ−α∂J∂θ where α is the learning rate and θ represents a parameter. Illustration: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Code: 12345678910111213141516171819202122232425262728293031323334def update_parameters(parameters, grads, learning_rate = 1.2): ''' Update parameters using the gradient descent update rule Arguments: @parameters: python dictionary containing your parameters @grads: python dictionary containing your gradient with respect to different parameters @learning_rate: the learning rate used to update parameters Returns: @parameters: python dictionary containing your updated parameters ''' # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Retrieve each grident from the dictionary "grads" dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # Update rule for each parameter W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters Test: 1234567parameters, grads = update_parameters_test_case()parameters = update_parameters(parameters, grads)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 12345678910W1 = [[-0.00643025 0.01936718] [-0.02410458 0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]b1 = [[ -1.02420756e-06] [ 1.27373948e-05] [ 8.32996807e-07] [ -3.20136836e-06]]W2 = [[-0.01041081 -0.04463285 0.01758031 0.04747113]]b2 = [[ 0.00010457]] Integrate part 5.1, 5.2 and 5.3 in nn_model()Question: Build your neural network model in nn_model(). Instructions: The neural network model has to use the previous functions in the right order. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Merge all function into the nerual network modeldef nn_model(X, Y, n_h, num_iterations = 10000, print_cost = False): ''' Build your neural network in nn_model Arguments: @X: dataset of shape (2, number of examples) @Y: labels of shape (1, number of exampless) @n_h: size of the hidden layer @num_iterations: Number of iterations in gradient descent loop @print_cost: if True, print the cost every 100 iterations Returns: @parameters: parameters learnt by the model. They can then be used to predict ''' np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize paramters, then retrieve W1, b1, W2, b2. # Inputs: "n_x, n_h, n_y" # Outputs: " parameters(W1, b1, W2, b2)" parameters = initialize_parameters(n_x, n_h, n_y) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. # Inputs: "X, parameters". # Outputs: "A2, cache" A2, cache = forward_propagation(X, parameters) # Cost Function. # Inputs: "A2, Y, parameters" # Output: "cost" cost = compute_cost(A2, Y, parameters) # Backward propagation. # Inputs: "parameters, cache, X, Y" # Outputs: "grads" grads = backward_propagation(parameters, cache, X, Y) # Update parameters by using gradient descent. # Inputs: "parameters, grads" # Outputs: "parameters" parameters = update_parameters(parameters, grads, learning_rate=1.2) # Print the cost every 1000 iterations: if print_cost and i % 1000 == 0: print("Cost after iterations %i: %f" %(i, cost)) return parameters Test: 123456X_assess, Y_assess = nn_model_test_case()parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 1234567891011121314151617181920Cost after iteration 0: 0.692739Cost after iteration 1000: 0.000218Cost after iteration 2000: 0.000107Cost after iteration 3000: 0.000071Cost after iteration 4000: 0.000053Cost after iteration 5000: 0.000042Cost after iteration 6000: 0.000035Cost after iteration 7000: 0.000030Cost after iteration 8000: 0.000026Cost after iteration 9000: 0.000023W1 = [[-0.65848169 1.21866811] [-0.76204273 1.39377573] [ 0.5792005 -1.10397703] [ 0.76773391 -1.41477129]]b1 = [[ 0.287592 ] [ 0.3511264 ] [-0.2431246 ] [-0.35772805]]W2 = [[-2.45566237 -3.27042274 2.00784958 3.36773273]]b2 = [[ 0.20459656]] PredictionsQuestion: Use your model to predict by building predict(). Use forward propagation to predict results. Reminder: Code: 12345678910111213141516# Use forward propagation to predict resultsdef predict(parameters, X): ''' Using the learned parameters, predicts a class for each example in X Arguments: @parameters: python dictionary containing your paramters @X: input data of size (n_x, m) Returns: @predictions: vector of predictions of our model (red: 0 / bule: 1) ''' # Computes probabilities using forward propagation, and classfies to 0/1 using 0.5 as threshold A2 = forward_propagation(X, parameters)[0] predictions = (A2 &gt; 0.5) return predictions Test: 1234parameters, X_assess = predict_test_case()predictions = predict(parameters, X_assess)print("predictions mean = " + str(np.mean(predictions))) Result: 1predictions mean = 0.666666666667 It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of `n_h` hidden units. Code: 12345678910# Build a model with a n_h-dimensional hidden layerparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# Plot the decision boundaryplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))# Print accuracypredictions = predict(parameters, X)print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%') Result: 1234567891011Cost after iteration 0: 0.693048Cost after iteration 1000: 0.288083Cost after iteration 2000: 0.254385Cost after iteration 3000: 0.233864Cost after iteration 4000: 0.226792Cost after iteration 5000: 0.222644Cost after iteration 6000: 0.219731Cost after iteration 7000: 0.217504Cost after iteration 8000: 0.219454Cost after iteration 9000: 0.218607Accuracy: 90% Interpretation: Accuracy is really high compared to Logistic Regression. The model has learned the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. Now, let’s try out several hidden layer sizes. Tuning hidden layer size(optional/ungraded exercise)Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes. Code: 123456789101112# This may take about 2 minutes to runplt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) # ？？？ plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) # ??? predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print ("Accuracy for &#123;&#125; hidden units: &#123;&#125; %".format(n_h, accuracy)) Result: 1234567Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 90.25 % Interpretation: The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting. You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. You’ve learnt to: Build a complete neural network with a hidden layer Make a good use of a non-linear unit Implemented forward propagation and backpropagation, and trained a neural network See the impact of varying the hidden layer size, including overfitting. Nice work! Source Code]]></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
      <tags>
        <tag>Coursera</tag>
        <tag>Neural Network</tag>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何上传本地代码到github]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github%2F</url>
    <content type="text"><![CDATA[如何上传本地代码到github 第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示： 点击Clone or download按钮，复制弹出的地址git@github.com:***/***.git 注意要用SSH地址。 第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令1git init 第三步：将项目的所有文件添加到仓库中1git add . 第四步：将添加的文件提交到仓库中1git commit -m &quot;注释语句&quot; 第五步：将本地仓库关联到github上1git remote add origin git@github.com:***/test.git 第六步：上传之前，先要pull一下，执行如下命令：1git pull origin master 第七步：上传代码到github远程仓库1git push -u origin master 祝你成功！]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset]]></title>
    <url>%2F2019%2F08%2F01%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%2F</url>
    <content type="text"><![CDATA[Logistic Regression with a Neural Network mindset Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning. Instructions: Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. You will learn to: Build the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main function, in the right order. PackagesFirst, let’s run the cell below to import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. code ————-&gt; 1234567import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset Overview of the Problem setProblem Statement: You are given a dataset (“data.h5”) containing: a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px) You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. Let’s get more familiar with the dataset. Load the data by running the following code. 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing). Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. code: 1234567891011121314151617181920# Data preprocessingdef data_Preprocess(): ''' load dataset and preprocess dataset Argument: Return: @train_set_x: your train set features @train_set_y: your train set labels @test_set_x: your test set features @test_set_y: your test set labels ''' train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() # load dataset from dataset files train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # vectorize the features of each examples train_set_x = train_set_x_flatten/255 test_set_x = test_set_x_flatten/255 # normalize the features vector return train_set_x, train_set_y, test_set_x, test_set_y, classes **What you need remember:** Common steps for pre-processing a new dataset are: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1) “Standardize” the data General Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-cat images. You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! Mathematical expression of the algorithm: For one examples x(i): The cost is the computed by summing over all training examples: Key steps: In this exercise, you will carry out the following steps: Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyze the results and conclude Building the parts of our algorithmThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the model’s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) You often build 1-3 separately and integrate them into one function we call model(). Helper functionsExercise: using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}to make predictions. Use np.exp(). code: 123456789101112# Helper functionsdef sigmoid(z): ''' Compute the sigmoid of z Arguments: @z: A scalar or numpy array of any size Return: @s: sigmoid(z) ''' s = 1 / (1 + np.exp(-z)) return s Initializing parametersExercise: Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation. code: 1234567891011121314151617# Initializing parametersdef initialize_with_zeros(dim): ''' This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0 Argument: @dim: size of the w vector we want Returns: @w: initialized vector of shape (dim, 1) @b: initialized scalar (corresponds to the bias) ''' w = np.zeros((dim, 1)) b = 0 assert(w.shape == (dim, 1)) assert(isinstance(b, float) or isinstance(b, int)) return w, b Forward and Backward propagationNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Exercise: Implement a function propagation() that computes the cost function and its gradient. Hints(提示): Forward Propagation: You get x You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$ You calculate the cost function: $J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 - y^{(i)})log(1 - a^{(i)}) }$ Here are the two formulas you will be using: code: 123456789101112131415161718192021222324252627282930313233# Forward and Backword propagationdef propagate(w, b, X, Y): ''' Implement the cost function and its gradient for the propagation explained above Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of size (num_px * num_px * 3, number of examples) @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: @cost: negative log-likelihood cost for logistic regression @dw: gradient of the loss with respect to w, thus same shape as w @db: gradient of the loss with respect to b, thus same shape as b ''' m = X.shape[1] # nx A = sigmoid(np.add(np.dot(w.T, X), b)) # compute activation cost = -(np.dot(Y, np.log(A).T) + np.dot(1 - Y, np.log(1 - A).T)) / m # compute cost dw = np.dot(X, (A-Y).T) / m # compute dw db = np.sum(A - Y) / m # compute db assert(dw.shape == w.shape) assert(db.dtype == float) cost = np.squeeze(cost) # 把shape中为1的维度去掉 assert(cost.shape == ()) # 判断剩下的是否为空 grads = &#123;"dw": dw, "db": db&#125; return grads, cost Optimization You have initialized your parameters. You are also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent. Exercise: Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate. Code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#Optimizationdef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): ''' This function optimizes w and b by running a gradient descent algorithm Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples) @num_iterations: number of iterations of the optimization loop @learning_rate: learning rate of the gradient descent update rule @print_cost: True to print the loss every 100 steps Returns: @params: dictionary containing the weights w and bias b @grads: dictionary containing the gradients of the weights and bias with respect to the cost function @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve Tips: You basically need to write down two steps and iterate through them: (1) Calculate the cost and the gradient for the current parameters. Use propagate() (2) Update the parameters using gradient descent rule for w and b ''' costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # Update rule w = w - learning_rate * dw b = b - learning_rate * db # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %d: %f" %(i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs PredictExercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the predict() function. There is two steps to computing predictions: Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$ Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_Prediction. Code: 12345678910111213141516171819202122232425262728293031323334#Graded function: predictdef predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) Returns: @Y_prediction: a numpy array (vector) containing all prediction (0 / 1) for the examples in X ''' m = X.shape[1] # number of examples Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.add(np.dot(w.T, X), b)) # (1, m) for i in range(A.shape[1]): # Convert probabilities A[0, i] to actual predictions p[0, i] if A[0, i] &lt;= 0.5: Y_prediction[0, i] = 0 else: Y_prediction[0, i] = 1 assert(Y_prediction.shape == (1, m)) return Y_prediction **What you need remember:** You've implemented several functions that: initialize (w, b) Optimize the loss iteratively to learn parameters (w, b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w, b) to predict the labels for a given set of examples Merge all functions into a modelYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. Exercise: Implement the model function. Use the following notation: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() Code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Merge all functions into a modeldef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): ''' Builds the logistic regression model by calling the function you have implemented previously Arguments: @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train) @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train) @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test) @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test) @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize() @print_cost: Set to true to print the cost every 100 iterations Returns: @d: dictionary containing information about the model ''' # initailize parameters with zeros w,b = initialize_with_zeros(X_train.shape[0]) # num_px * num_px * 3, w: (dim, 1), b: a scalar # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123;"costs" : costs, "Y_prediction_test" : Y_prediction_test, "Y_prediction_train" : Y_prediction_train, "w" : w, "b" : b, "learning_rate" : learning_rate, "num_iterations" : num_iterations &#125; return d Run the following cell to train your model: 1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =2000, learning_rate = 0.005, print_cost = True) The results are as follows: 12345678910111213141516171819202122Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 % Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week! Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Let's also plot the cost function and the gradients: 1234567# Plot learning curve (with costs)costs = np.squeeze(d['costs'])plt.plot(costs)plt.ylabel('cost')plt.xlabel('iterations (per hundreds)')plt.title("Learning rate =" + str(d["learning_rate"]))plt.show() Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. Further analysisCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α. Choice of learning rateReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate. Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. Code: 1234567891011121314151617learning_rates = [0.01, 0.001, 0.0001]models = &#123;&#125;for i in learning_rates: print ("learning rate is: " + str(i)) models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False) print ('\n' + "-------------------------------------------------------" + '\n')for i in learning_rates: plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))plt.ylabel('cost')plt.xlabel('iterations')legend = plt.legend(loc='upper center', shadow= True)frame = legend.get_frame()frame.set_facecolor('0.90')plt.show() Result: 1234567891011121314151617learning rate is: 0.01train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001train accuracy: 68.42105263157895 %test accuracy: 36.0 %------------------------------------------------------- Interpretation: Different learning rates give different costs and thus different predictions results. If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) Source Code]]></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
        <tag>Deep Learning</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo教程：使用Hexo写博客]]></title>
    <url>%2F2019%2F07%2F31%2Fhello-hexo-markdown%2F</url>
    <content type="text"><![CDATA[尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。 新建文档假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可： $ hexo new “hello hexo markdown” 上述命令的结果是在 ./hexo/source/_posts 路径下新建了一个 hello-hexo-markdown.md 文件。 然后，我们就可以打开编辑器尽情地写作了。 文档格式我们使用文本编辑器打开刚刚新建的 hello-hexo-markdown.md 文件，会发现其中已经存在内容： 12345---title: hello hexo markdowndate: 2019-07-31 20:44:47tags:--- 这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 --- 分隔的区域。其中， title的值是当前文档名，也是将来在网页中显示的文章标题 date值是我们新建文档时的当时地区时间 tags值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下： 12345678---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown--- 上面的配置参数为这篇文档贴上了 hello、hexo、mardown 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。 除了以上这些，还有很多预先定义的参数Front_mtter，我们这里选取一个常用且较为典型的配置参数categories讲解一下。 文章分类categories 是用来给文章分类的，它跟 tags 不同的是其具有顺序性和层次性。 例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，categories 有点儿像新建文件夹对文档进行分门别类的归置。 categories 的用法同 tags 一样，只不过斗个 categories 值是分先后顺序的。 引用资源写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。 这时，有两种解决办法： 使用绝对路径引用资源，在 Web 世界中就是资源的 URL 使用相对路径引用资源 文章资料文件夹如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。 使用文本编辑器打开站点根目录下的 _ config.yml 文件，将 post_asset_folder 值设置为 true。 1post_asset_folder: true 上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 hexo new &lt;title&gt; 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。 相对路径引用的标签插件通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题： 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 比如说：当你打开文章资源文件夹功能后，你把一个 example.jpg 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 ![](/example.jpg) ，它将 不会 出现在首页上。（但是它会在文章中按你期待的方式工作） ！！！注意： 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 ./资源名称，不用在引用路径中添加同名文件夹目录层级。 正确的引用图片方式是使用下列的标签插件而不是 markdown ： 1&#123;% asset_img example.jpg This is an example image %&#125; 通过这种方式，图片将会同时出现在文章和主页以及归档页中。 文章摘要有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。 所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。 设置文章摘要，我们只需在想显示为摘要的内容之后添 &lt;!-- more --&gt; 即可。像下面这样： 1234567891011---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown---我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧&lt;!-- more --&gt;紧接着文章摘要的正文内容 这样，&lt;!-- more --&gt; 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。 注意！文章摘要在文章详情页是正文中最前面的内容。 生成文件清除缓存文件为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令： 1$ hexo clean 上述命令会清除在本地站点文件下的缓存文件(db.json)和已有的静态文件(public). 生成静态文件写好MarkDown文档之后，我们就可以使用以下命令生成静态文件： 1$ hexo generate 然后我们就可以启动 Hexo 服务器，使用浏览器打开 http://localhost:4000 查看效果了。 示范下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
