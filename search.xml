<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classfication with a hidden layer]]></title>
    <url>%2F2019%2F08%2F03%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%2C%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classfication%20with%20a%20hidden%20layer%2F</url>
    <content type="text"></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何上传本地代码到github]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github%2F</url>
    <content type="text"><![CDATA[如何上传本地代码到github 第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示： 点击Clone or download按钮，复制弹出的地址**git@github.com:***/***.git** 注意要用SSH地址。 第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令1git init 第三步：将项目的所有文件添加到仓库中1git add . 第四步：将添加的文件提交到仓库中1git commit -m &quot;注释语句&quot; 第五步：将本地仓库关联到github上1git remote add origin git@github.com:***/test.git 第六步：上传之前，先要pull一下，执行如下命令：1git pull origin master 第七步：上传代码到github远程仓库1git push -u origin master 祝你成功！]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset]]></title>
    <url>%2F2019%2F08%2F01%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%2F</url>
    <content type="text"><![CDATA[Logistic Regression with a Neural Network mindset Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning. Instructions: Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. You will learn to: Build the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main function, in the right order. PackagesFirst, let’s run the cell below to import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. code ———&gt; 1234567import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset Overview of the Problem setProblem Statement: You are given a dataset (“data.h5”) containing: a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px) You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. Let’s get more familiar with the dataset. Load the data by running the following code. 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing). Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. code: 1234567891011121314151617181920# Data preprocessingdef data_Preprocess(): ''' load dataset and preprocess dataset Argument: Return: @train_set_x: your train set features @train_set_y: your train set labels @test_set_x: your test set features @test_set_y: your test set labels ''' train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() # load dataset from dataset files train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # vectorize the features of each examples train_set_x = train_set_x_flatten/255 test_set_x = test_set_x_flatten/255 # normalize the features vector return train_set_x, train_set_y, test_set_x, test_set_y, classes What you need remember: Common steps for pre-processing a new dataset are: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1) “Standardize” the data General Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-cat images. You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! Mathematical expression of the algorithm: For one examples x(i): The cost is the computed by summing over all training examples: Key steps: In this exercise, you will carry out the following steps: Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyze the results and conclude Building the parts of our algorithmThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the model’s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) You often build 1-3 separately and integrate them into one function we call model(). Helper functionsExercise: using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute$$sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}$$to make predictions. Use np.exp(). code: 123456789101112# Helper functionsdef sigmoid(z): ''' Compute the sigmoid of z Arguments: @z: A scalar or numpy array of any size Return: @s: sigmoid(z) ''' s = 1 / (1 + np.exp(-z)) return s Initializing parametersExercise: Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation. code: 1234567891011121314151617# Initializing parametersdef initialize_with_zeros(dim): ''' This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0 Argument: @dim: size of the w vector we want Returns: @w: initialized vector of shape (dim, 1) @b: initialized scalar (corresponds to the bias) ''' w = np.zeros((dim, 1)) b = 0 assert(w.shape == (dim, 1)) assert(isinstance(b, float) or isinstance(b, int)) return w, b Forward and Backward propagationNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Exercise: Implement a function propagation() that computes the cost function and its gradient. Hints(提示): Forward Propagation: You get x You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$ You calculate the cost function: $J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 - y^{(i)})log(1 - a^{(i)}) }$ Here are the two formulas you will be using: code: 123456789101112131415161718192021222324252627282930313233# Forward and Backword propagationdef propagate(w, b, X, Y): ''' Implement the cost function and its gradient for the propagation explained above Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of size (num_px * num_px * 3, number of examples) @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: @cost: negative log-likelihood cost for logistic regression @dw: gradient of the loss with respect to w, thus same shape as w @db: gradient of the loss with respect to b, thus same shape as b ''' m = X.shape[1] # nx A = sigmoid(np.add(np.dot(w.T, X), b)) # compute activation cost = -(np.dot(Y, np.log(A).T) + np.dot(1 - Y, np.log(1 - A).T)) / m # compute cost dw = np.dot(X, (A-Y).T) / m # compute dw db = np.sum(A - Y) / m # compute db assert(dw.shape == w.shape) assert(db.dtype == float) cost = np.squeeze(cost) # 把shape中为1的维度去掉 assert(cost.shape == ()) # 判断剩下的是否为空 grads = &#123;"dw": dw, "db": db&#125; return grads, cost Optimization You have initialized your parameters. You are also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent. Exercise: Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate. Code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#Optimizationdef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): ''' This function optimizes w and b by running a gradient descent algorithm Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples) @num_iterations: number of iterations of the optimization loop @learning_rate: learning rate of the gradient descent update rule @print_cost: True to print the loss every 100 steps Returns: @params: dictionary containing the weights w and bias b @grads: dictionary containing the gradients of the weights and bias with respect to the cost function @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve Tips: You basically need to write down two steps and iterate through them: (1) Calculate the cost and the gradient for the current parameters. Use propagate() (2) Update the parameters using gradient descent rule for w and b ''' costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # Update rule w = w - learning_rate * dw b = b - learning_rate * db # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %d: %f" %(i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs PredictExercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the predict() function. There is two steps to computing predictions: Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$ Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_Prediction. Code: 12345678910111213141516171819202122232425262728293031323334#Graded function: predictdef predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) Returns: @Y_prediction: a numpy array (vector) containing all prediction (0 / 1) for the examples in X ''' m = X.shape[1] # number of examples Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.add(np.dot(w.T, X), b)) # (1, m) for i in range(A.shape[1]): # Convert probabilities A[0, i] to actual predictions p[0, i] if A[0, i] &lt;= 0.5: Y_prediction[0, i] = 0 else: Y_prediction[0, i] = 1 assert(Y_prediction.shape == (1, m)) return Y_prediction What you need remember: You’ve implemented several functions that: initialize (w, b) Optimize the loss iteratively to learn parameters (w, b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w, b) to predict the labels for a given set of examples Merge all functions into a modelYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. Exercise: Implement the model function. Use the following notation: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() Code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Merge all functions into a modeldef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): ''' Builds the logistic regression model by calling the function you have implemented previously Arguments: @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train) @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train) @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test) @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test) @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize() @print_cost: Set to true to print the cost every 100 iterations Returns: @d: dictionary containing information about the model ''' # initailize parameters with zeros w,b = initialize_with_zeros(X_train.shape[0]) # num_px * num_px * 3, w: (dim, 1), b: a scalar # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123;"costs" : costs, "Y_prediction_test" : Y_prediction_test, "Y_prediction_train" : Y_prediction_train, "w" : w, "b" : b, "learning_rate" : learning_rate, "num_iterations" : num_iterations &#125; return d Run the following cell to train your model: 1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =2000, learning_rate = 0.005, print_cost = True) The results are as follows: 12345678910111213141516171819202122Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 % Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week! Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Let’s also plot the cost function and the gradients: 1234567# Plot learning curve (with costs)costs = np.squeeze(d['costs'])plt.plot(costs)plt.ylabel('cost')plt.xlabel('iterations (per hundreds)')plt.title("Learning rate =" + str(d["learning_rate"]))plt.show() Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. Further analysisCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α. Choice of learning rateReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate. Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. Code: 1234567891011121314151617learning_rates = [0.01, 0.001, 0.0001]models = &#123;&#125;for i in learning_rates: print ("learning rate is: " + str(i)) models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False) print ('\n' + "-------------------------------------------------------" + '\n')for i in learning_rates: plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))plt.ylabel('cost')plt.xlabel('iterations')legend = plt.legend(loc='upper center', shadow= True)frame = legend.get_frame()frame.set_facecolor('0.90')plt.show() Result: 1234567891011121314151617learning rate is: 0.01train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001train accuracy: 68.42105263157895 %test accuracy: 36.0 %------------------------------------------------------- Interpretation: Different learning rates give different costs and thus different predictions results. If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) Source Code]]></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
        <tag>Deep Learning</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo教程：使用Hexo写博客]]></title>
    <url>%2F2019%2F07%2F31%2Fhello-hexo-markdown%2F</url>
    <content type="text"><![CDATA[尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。 新建文档假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可： $ hexo new “hello hexo markdown” 上述命令的结果是在 ./hexo/source/_posts 路径下新建了一个 hello-hexo-markdown.md 文件。 然后，我们就可以打开编辑器尽情地写作了。 文档格式我们使用文本编辑器打开刚刚新建的 hello-hexo-markdown.md 文件，会发现其中已经存在内容： 12345---title: hello hexo markdowndate: 2019-07-31 20:44:47tags:--- 这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 --- 分隔的区域。其中， title的值是当前文档名，也是将来在网页中显示的文章标题 date值是我们新建文档时的当时地区时间 tags值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下： 12345678---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown--- 上面的配置参数为这篇文档贴上了 hello、hexo、mardown 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。 除了以上这些，还有很多预先定义的参数Front_mtter，我们这里选取一个常用且较为典型的配置参数categories讲解一下。 文章分类categories 是用来给文章分类的，它跟 tags 不同的是其具有顺序性和层次性。 例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，categories 有点儿像新建文件夹对文档进行分门别类的归置。 categories 的用法同 tags 一样，只不过斗个 categories 值是分先后顺序的。 引用资源写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。 这时，有两种解决办法： 使用绝对路径引用资源，在 Web 世界中就是资源的 URL 使用相对路径引用资源 文章资料文件夹如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。 使用文本编辑器打开站点根目录下的 _ config.yml 文件，将 post_asset_folder 值设置为 true。 1post_asset_folder: true 上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 hexo new &lt;title&gt; 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。 相对路径引用的标签插件通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题： 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 比如说：当你打开文章资源文件夹功能后，你把一个 example.jpg 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 ![](/example.jpg) ，它将 不会 出现在首页上。（但是它会在文章中按你期待的方式工作） ！！！注意： 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 ./资源名称，不用在引用路径中添加同名文件夹目录层级。 正确的引用图片方式是使用下列的标签插件而不是 markdown ： 1&#123;% asset_img example.jpg This is an example image %&#125; 通过这种方式，图片将会同时出现在文章和主页以及归档页中。 文章摘要有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。 所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。 设置文章摘要，我们只需在想显示为摘要的内容之后添 &lt;!-- more --&gt; 即可。像下面这样： 1234567891011---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown---我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧&lt;!-- more --&gt;紧接着文章摘要的正文内容 这样，&lt;!-- more --&gt; 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。 注意！文章摘要在文章详情页是正文中最前面的内容。 生成文件清除缓存文件为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令： 1$ hexo clean 上述命令会清除在本地站点文件下的缓存文件(db.json)和已有的静态文件(public). 生成静态文件写好MarkDown文档之后，我们就可以使用以下命令生成静态文件： 1$ hexo generate 然后我们就可以启动 Hexo 服务器，使用浏览器打开 http://localhost:4000 查看效果了。 示范下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
