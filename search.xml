<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer]]></title>
    <url>%2F2019%2F08%2F03%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%2C%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer%2F</url>
    <content type="text"><![CDATA[Planar data classification with a hidden layer Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. You will learn how to: Implement a 2-class classification neural network with a signal hidden layer Use units with a non-linear activation function, such as tanh Compute the cross entropy loss Implement forward and backward propagation PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental packages for scientific computing with Python. sklearn provides simple and efficient tools for data mining and data and analysis. matplotlib is a library for plotting graphs in Python. testCases provides some test examples to assess the correctness of your functions planar_utils provide various useful functions used in this assignment Code: 12345678910# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases_v2 import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsnp.random.seed(1) # set a seed so that the results are consistent DatasetFirst, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables X and Y. Code: 123456789101112# load datasetdef load_dataset(): ''' load planar dataset Arguments: Returns: @X: your dataset features @Y: your dataset labels ''' X, Y = load_planar_dataset() # load dataset return X, Y Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. Code: 123# Visualize the data:plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);plt.show() Result: You have: a numpy array (matrix) X that contains your features (x1, x2) a numpy array (vector) Y that contains your labels (red: 0, blue: 1) Let’s first get a better sense of what our data is like. Exercise: How many training examples do you have? In addition, what is the shape of the variables Xand Y ? Code: 123456shape_X = X.shapeshape_Y = Y.shapem = X.shape[1] # training set sizeprint ('The shape of X is: ' + str(shape_X))print ('The shape of Y is: ' + str(shape_Y))print ('I have m = %d training examples!' % (m)) Result: 123The shape of X is: (2, 400)The shape of Y is: (1, 400)I have m = 400 training examples! Simple Logistic RegressionBefore building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. Code: 123# Train the logistic regression classifierclf = sklearn.linear_model.LogisticRegressionCV();clf.fit(X.T, Y.T); You can now plot the decision boundary of these models. Run the code below. Code: 12345678# Plot the decision boundary for logistic regressionplot_decision_boundary(lambda x: clf.predict(x), X, Y)plt.title("Logistic Regression")plt.show()# Print accuracyLR_predictions = clf.predict(X.T)print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + "(percentage of correctly labelled datapoints)") Result: 1Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints) Interpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now. Neural Network modelLogistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer. Here is our model: Mathematically: For one example x(i): Given the predictions on all the examples, you can also compute the cost J as follows: J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}Reminder: The general methodology to build a Neural Network is to: Define the neural network structure (# of input units, # of hidden units, etc). Initialize the model’s parameters Loop: Implement forward propagation Compute loss Implement backward propagation to get the gradients Update parameters (gradient descent) You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once you’ve built nn_model() and learned the right parameters, you can make predictions on new data. Defining the neural network structureExercise: Define three variables: 123- n_X: the size of the input layer- n_h: the size of the hidden layer (set this to 4)- n_y: the size of the output layer Hint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4. Code: 12345678910111213141516171819# Defining the neural network structuredef layer_sizes(X, Y): ''' Defining the neural network structure Arguments: @x: input dataset of shape (input size, number of examples) @y: labels of shape (output size, number of examples) Returns: @n_x: the size of input layer @n_h: the size of hidden layer @n_y: the size of output layer ''' n_x = X.shape[0] n_h = 4 n_y = Y.shape[0] return (n_x, n_h, n_y) Test: 12345X_assess, Y_assess = layer_sizes_test_case()(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)print("The size of the input layer is: n_x = " + str(n_x))print("The size of the hidden layer is: n_h = " + str(n_h))print("The size of the output layer is: n_y = " + str(n_y)) Result: 123The size of the input layer is: n_x = 5The size of the hidden layer is: n_h = 4The size of the output layer is: n_y = 2 Initialize the model’s parametersExercise: Implement the function initialize_parameters(). Instructions: Make sure your parameters’ sizes are right. Refer to the neural network figure above if you needed. You will initialize the weights matrices with random values. Use: np.random.randn (a, b) * 0.01 to randomly initialize a matrix of shape (a, b). You will initialize the bias vectors as zeros. Use: np.zeros((a, b)) to initialize a matrix of shape (a, b) with zeros. Code: 1234567891011121314151617181920212223242526272829303132333435# Initialize the model's parametersdef initialize_parameters(n_x, n_h, n_y): ''' Initialize the model's parameters Argument: @n_x: the size of the input layer @n_h: the size of the hidden layer @n_y: the size of the output layer Returns: @params: python dictionary containing your parameters: @W1: weight matrix of shape (n_h, n_x) @b1: bias vector of shape (n_h, 1) @W2: weight matrix of shape (n_y, n_h) @b2: bias vector of shape (n_y, 1) ''' np.random.seed(2) # set a seed so that the result are consisent W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) assert(W1.shape == (n_h, n_x)) assert(b1.shape == (n_h, 1)) assert(W2.shape == (n_y, n_h)) assert(b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters Test: 1234567n_x, n_h, n_y = initialize_parameters_test_case()parameters = initialize_parameters(n_x, n_h, n_y)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 12345678910W1 = [[-0.00416758 -0.00056267] [-0.02136196 0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]b1 = [[ 0.] [ 0.] [ 0.] [ 0.]]W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]]b2 = [[ 0.]] The LoopExercise 1: Implement forward_propagation() . Instructions: Look above at the mathematical representation of your classifier. You can use the function sigmoid() . It is built-in (imported) in the notebook (planar_utils.py). You can use the function tanh() . It is part of the numpy library. The steps you have to implement are: Retrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters()) by using paramters[&quot;...&quot;] . Implement Forward Propagation. Compute Z[1], A[1], Z[2] and A[2] (the vector of all your predictions on all the examples in the training set). Values needed in the back propagation are stored in “cache“. The cache will be given as an input to the back propagation function. Code: 123456789101112131415161718192021222324252627282930313233# Implement forward propagationdef forward_propagation(X, parameters): ''' implement forward propagation Argument: @X: input data of size (n_x, m) @parameters: python dictionary containing your parameters Returns: @A2: The sigmoid output of the second activation @cache: a dictionary containing "Z1", "A1", "Z2" and "A2" ''' # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Implement forward propagation to calculate A2 Z1 = np.dot(W1, X) + b1 A1 = tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cache Test: 12345X_assess, parameters = forward_propagation_test_case()A2, cache = forward_propagation(X_assess, parameters)# Note: we use the mean here just to make sure that your output matches ours. print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2'])) Result: 10.262818640198 0.091999045227 -1.30766601287 0.212877681719 Now that you have computed A[2] (in the Python variable “A2”), which contains a[2](i) for every examples, you can compute the cost function as follows: J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}Exercise 2: Implement compute_cost() to compute the value of the cost J. Instructions: There are many ways to implement the cross-entropy loss (交叉熵损失). To help you, we give you how we would have implemented: -\sum_{i = 0}^{m}{y^{(i)}log(a^{[2](i)})}12logprobs = np.multiply(np.log(A2), Y)cost = -np.sum(logprobs) # no need to use a for loop Code: 123456789101112131415161718192021222324# Implement compute costdef compute_cost(A2, Y, parameters): ''' Computes the cross-entropy cost Arguments: @A2: The sigmoid output of the second activation, of shape(1, number of examples) @Y: "true" labels vector of shape (1, number of examples) @parameters: python dictionary containing your parameters W1, b1, W2, b2 Returns: @cost: cross-entropy cost ''' m = Y.shape[1] #number of examples # Compute the cross-entropy cost logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y) cost = -1 / m * np.sum(logprobs) cost = np.squeeze(cost) # make sure cost is the dimension we expect # E.g. turns [[17]] into 17 assert(isinstance(cost, float)) return cost Test: 123A2, Y_assess, parameters = compute_cost_test_case()print("cost = " + str(compute_cost(A2, Y_assess, parameters))) Result: 1cost = 0.693058761 Using the cache computed during forward propagation, you can now implement backward propagation. Exercise 3: Implement the function backward_propagation(). Instructions: Backward propagation is usually the hardest (most mathematical) part in deep learning. o help you, here again is the slide from the lecture on backward propagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation. Tips: To compute dZ1 you’ll need to compute g[1]′(Z[1]). Since g[1](.)is the tanh activation function, if a=g[1](z) then g[1]′(z)=1−a2. So you can compute g[1]′(Z[1]) using (1 - np.power(A1, 2)). Code: 1234567891011121314151617181920212223242526272829303132333435363738# Implement the function backward propagrationdef backward_propagation(parameters, cache, X, Y): ''' Implement the backward propagation Arguments: @parameters: python dictionary containing out parameters (W1, b1, W2, b2) @cache: a dictionary containing "Z1", "A1", "Z2", "A2". @X: input data of shape (2, number of examples) @Y: "true" labels vector of shape(1, number of examples) Returns: @grads: python dictionary containing your gradients with respect to different parameters ''' m = X.shape[1] # retrieve W1 and W2 from the dictionary parameters W1 = parameters["W1"] W2 = parameters["W2"] # retrieve A1 and A2 from dictionary "cache" A1 = cache["A1"] A2 = cache["A2"] # Backward propagation: calculate dW1, db1, dW2, db2 dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis = 1, keepdims = True) dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis = 1, keepdims = True) grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return grads Test: 1234567parameters, cache, X_assess, Y_assess = backward_propagation_test_case()grads = backward_propagation(parameters, cache, X_assess, Y_assess)print ("dW1 = "+ str(grads["dW1"]))print ("db1 = "+ str(grads["db1"]))print ("dW2 = "+ str(grads["dW2"]))print ("db2 = "+ str(grads["db2"])) Result: 12345678910dW1 = [[ 0.00301023 -0.00747267] [ 0.00257968 -0.00641288] [-0.00156892 0.003893 ] [-0.00652037 0.01618243]]db1 = [[ 0.00176201] [ 0.00150995] [-0.00091736] [-0.00381422]]dW2 = [[ 0.00078841 0.01765429 -0.00084166 -0.01022527]]db2 = [[-0.16655712]] Exercise 4: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2). General gradient descent rule: θ=θ−α∂J∂θ where α is the learning rate and θ represents a parameter. Illustration: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Code: 12345678910111213141516171819202122232425262728293031323334def update_parameters(parameters, grads, learning_rate = 1.2): ''' Update parameters using the gradient descent update rule Arguments: @parameters: python dictionary containing your parameters @grads: python dictionary containing your gradient with respect to different parameters @learning_rate: the learning rate used to update parameters Returns: @parameters: python dictionary containing your updated parameters ''' # Retrieve each parameter from the dictionary "parameters" W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # Retrieve each grident from the dictionary "grads" dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # Update rule for each parameter W1 = W1 - learning_rate * dW1 b1 = b1 - learning_rate * db1 W2 = W2 - learning_rate * dW2 b2 = b2 - learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters Test: 1234567parameters, grads = update_parameters_test_case()parameters = update_parameters(parameters, grads)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 12345678910W1 = [[-0.00643025 0.01936718] [-0.02410458 0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]b1 = [[ -1.02420756e-06] [ 1.27373948e-05] [ 8.32996807e-07] [ -3.20136836e-06]]W2 = [[-0.01041081 -0.04463285 0.01758031 0.04747113]]b2 = [[ 0.00010457]] Integrate part 5.1, 5.2 and 5.3 in nn_model()Question: Build your neural network model in nn_model(). Instructions: The neural network model has to use the previous functions in the right order. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Merge all function into the nerual network modeldef nn_model(X, Y, n_h, num_iterations = 10000, print_cost = False): ''' Build your neural network in nn_model Arguments: @X: dataset of shape (2, number of examples) @Y: labels of shape (1, number of exampless) @n_h: size of the hidden layer @num_iterations: Number of iterations in gradient descent loop @print_cost: if True, print the cost every 100 iterations Returns: @parameters: parameters learnt by the model. They can then be used to predict ''' np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize paramters, then retrieve W1, b1, W2, b2. # Inputs: "n_x, n_h, n_y" # Outputs: " parameters(W1, b1, W2, b2)" parameters = initialize_parameters(n_x, n_h, n_y) # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation. # Inputs: "X, parameters". # Outputs: "A2, cache" A2, cache = forward_propagation(X, parameters) # Cost Function. # Inputs: "A2, Y, parameters" # Output: "cost" cost = compute_cost(A2, Y, parameters) # Backward propagation. # Inputs: "parameters, cache, X, Y" # Outputs: "grads" grads = backward_propagation(parameters, cache, X, Y) # Update parameters by using gradient descent. # Inputs: "parameters, grads" # Outputs: "parameters" parameters = update_parameters(parameters, grads, learning_rate=1.2) # Print the cost every 1000 iterations: if print_cost and i % 1000 == 0: print("Cost after iterations %i: %f" %(i, cost)) return parameters Test: 123456X_assess, Y_assess = nn_model_test_case()parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Result: 1234567891011121314151617181920Cost after iteration 0: 0.692739Cost after iteration 1000: 0.000218Cost after iteration 2000: 0.000107Cost after iteration 3000: 0.000071Cost after iteration 4000: 0.000053Cost after iteration 5000: 0.000042Cost after iteration 6000: 0.000035Cost after iteration 7000: 0.000030Cost after iteration 8000: 0.000026Cost after iteration 9000: 0.000023W1 = [[-0.65848169 1.21866811] [-0.76204273 1.39377573] [ 0.5792005 -1.10397703] [ 0.76773391 -1.41477129]]b1 = [[ 0.287592 ] [ 0.3511264 ] [-0.2431246 ] [-0.35772805]]W2 = [[-2.45566237 -3.27042274 2.00784958 3.36773273]]b2 = [[ 0.20459656]] PredictionsQuestion: Use your model to predict by building predict(). Use forward propagation to predict results. Reminder: Code: 12345678910111213141516# Use forward propagation to predict resultsdef predict(parameters, X): ''' Using the learned parameters, predicts a class for each example in X Arguments: @parameters: python dictionary containing your paramters @X: input data of size (n_x, m) Returns: @predictions: vector of predictions of our model (red: 0 / bule: 1) ''' # Computes probabilities using forward propagation, and classfies to 0/1 using 0.5 as threshold A2 = forward_propagation(X, parameters)[0] predictions = (A2 &gt; 0.5) return predictions Test: 1234parameters, X_assess = predict_test_case()predictions = predict(parameters, X_assess)print("predictions mean = " + str(np.mean(predictions))) Result: 1predictions mean = 0.666666666667 It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of `n_h` hidden units. Code: 12345678910# Build a model with a n_h-dimensional hidden layerparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# Plot the decision boundaryplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))# Print accuracypredictions = predict(parameters, X)print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%') Result: 1234567891011Cost after iteration 0: 0.693048Cost after iteration 1000: 0.288083Cost after iteration 2000: 0.254385Cost after iteration 3000: 0.233864Cost after iteration 4000: 0.226792Cost after iteration 5000: 0.222644Cost after iteration 6000: 0.219731Cost after iteration 7000: 0.217504Cost after iteration 8000: 0.219454Cost after iteration 9000: 0.218607Accuracy: 90% Interpretation: Accuracy is really high compared to Logistic Regression. The model has learned the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. Now, let’s try out several hidden layer sizes. Tuning hidden layer size(optional/ungraded exercise)Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes. Code: 123456789101112# This may take about 2 minutes to runplt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) # ？？？ plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) # ??? predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print ("Accuracy for &#123;&#125; hidden units: &#123;&#125; %".format(n_h, accuracy)) Result: 1234567Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 90.25 % Interpretation: The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting. You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. **You've learnt to:** Build a complete neural network with a hidden layer Make a good use of a non-linear unit Implemented forward propagation and backpropagation, and trained a neural network See the impact of varying the hidden layer size, including overfitting. Nice work! Source Code]]></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
      <tags>
        <tag>Coursera</tag>
        <tag>Neural Network</tag>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何上传本地代码到github]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github%2F</url>
    <content type="text"><![CDATA[如何上传本地代码到github 第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示： 点击Clone or download按钮，复制弹出的地址git@github.com:***/***.git 注意要用SSH地址。 第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令1git init 第三步：将项目的所有文件添加到仓库中1git add . 第四步：将添加的文件提交到仓库中1git commit -m &quot;注释语句&quot; 第五步：将本地仓库关联到github上1git remote add origin git@github.com:***/test.git 第六步：上传之前，先要pull一下，执行如下命令：1git pull origin master 第七步：上传代码到github远程仓库1git push -u origin master 祝你成功！]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset]]></title>
    <url>%2F2019%2F08%2F01%2F%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%2F</url>
    <content type="text"><![CDATA[Logistic Regression with a Neural Network mindset Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning. Instructions: Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so. You will learn to: Build the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main function, in the right order. PackagesFirst, let’s run the cell below to import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. code ————-&gt; 1234567import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset Overview of the Problem setProblem Statement: You are given a dataset (“data.h5”) containing: a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px) You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. Let’s get more familiar with the dataset. Load the data by running the following code. 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing). Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. code: 1234567891011121314151617181920# Data preprocessingdef data_Preprocess(): ''' load dataset and preprocess dataset Argument: Return: @train_set_x: your train set features @train_set_y: your train set labels @test_set_x: your test set features @test_set_y: your test set labels ''' train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() # load dataset from dataset files train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # vectorize the features of each examples train_set_x = train_set_x_flatten/255 test_set_x = test_set_x_flatten/255 # normalize the features vector return train_set_x, train_set_y, test_set_x, test_set_y, classes **What you need remember:** Common steps for pre-processing a new dataset are: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1) “Standardize” the data General Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-cat images. You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! Mathematical expression of the algorithm: For one examples x(i): The cost is the computed by summing over all training examples: Key steps: In this exercise, you will carry out the following steps: Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyze the results and conclude Building the parts of our algorithmThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the model’s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) You often build 1-3 separately and integrate them into one function we call model(). Helper functionsExercise: using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}to make predictions. Use np.exp(). code: 123456789101112# Helper functionsdef sigmoid(z): ''' Compute the sigmoid of z Arguments: @z: A scalar or numpy array of any size Return: @s: sigmoid(z) ''' s = 1 / (1 + np.exp(-z)) return s Initializing parametersExercise: Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation. code: 1234567891011121314151617# Initializing parametersdef initialize_with_zeros(dim): ''' This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0 Argument: @dim: size of the w vector we want Returns: @w: initialized vector of shape (dim, 1) @b: initialized scalar (corresponds to the bias) ''' w = np.zeros((dim, 1)) b = 0 assert(w.shape == (dim, 1)) assert(isinstance(b, float) or isinstance(b, int)) return w, b Forward and Backward propagationNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Exercise: Implement a function propagation() that computes the cost function and its gradient. Hints(提示): Forward Propagation: You get x You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$ You calculate the cost function: $J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 - y^{(i)})log(1 - a^{(i)}) }$ Here are the two formulas you will be using: code: 123456789101112131415161718192021222324252627282930313233# Forward and Backword propagationdef propagate(w, b, X, Y): ''' Implement the cost function and its gradient for the propagation explained above Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of size (num_px * num_px * 3, number of examples) @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: @cost: negative log-likelihood cost for logistic regression @dw: gradient of the loss with respect to w, thus same shape as w @db: gradient of the loss with respect to b, thus same shape as b ''' m = X.shape[1] # nx A = sigmoid(np.add(np.dot(w.T, X), b)) # compute activation cost = -(np.dot(Y, np.log(A).T) + np.dot(1 - Y, np.log(1 - A).T)) / m # compute cost dw = np.dot(X, (A-Y).T) / m # compute dw db = np.sum(A - Y) / m # compute db assert(dw.shape == w.shape) assert(db.dtype == float) cost = np.squeeze(cost) # 把shape中为1的维度去掉 assert(cost.shape == ()) # 判断剩下的是否为空 grads = &#123;"dw": dw, "db": db&#125; return grads, cost Optimization You have initialized your parameters. You are also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent. Exercise: Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate. Code: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#Optimizationdef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): ''' This function optimizes w and b by running a gradient descent algorithm Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples) @num_iterations: number of iterations of the optimization loop @learning_rate: learning rate of the gradient descent update rule @print_cost: True to print the loss every 100 steps Returns: @params: dictionary containing the weights w and bias b @grads: dictionary containing the gradients of the weights and bias with respect to the cost function @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve Tips: You basically need to write down two steps and iterate through them: (1) Calculate the cost and the gradient for the current parameters. Use propagate() (2) Update the parameters using gradient descent rule for w and b ''' costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w, b, X, Y) # Retrieve derivatives from grads dw = grads["dw"] db = grads["db"] # Update rule w = w - learning_rate * dw b = b - learning_rate * db # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print("Cost after iteration %d: %f" %(i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs PredictExercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the predict() function. There is two steps to computing predictions: Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$ Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_Prediction. Code: 12345678910111213141516171819202122232425262728293031323334#Graded function: predictdef predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: @w: weights, a numpy array of size (num_px * num_px * 3, 1) @b: bias, a scalar @X: data of shape (num_px * num_px * 3, number of examples) Returns: @Y_prediction: a numpy array (vector) containing all prediction (0 / 1) for the examples in X ''' m = X.shape[1] # number of examples Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture A = sigmoid(np.add(np.dot(w.T, X), b)) # (1, m) for i in range(A.shape[1]): # Convert probabilities A[0, i] to actual predictions p[0, i] if A[0, i] &lt;= 0.5: Y_prediction[0, i] = 0 else: Y_prediction[0, i] = 1 assert(Y_prediction.shape == (1, m)) return Y_prediction **What you need remember:** You've implemented several functions that: initialize (w, b) Optimize the loss iteratively to learn parameters (w, b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w, b) to predict the labels for a given set of examples Merge all functions into a modelYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. Exercise: Implement the model function. Use the following notation: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() Code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Merge all functions into a modeldef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): ''' Builds the logistic regression model by calling the function you have implemented previously Arguments: @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train) @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train) @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test) @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test) @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize() @print_cost: Set to true to print the cost every 100 iterations Returns: @d: dictionary containing information about the model ''' # initailize parameters with zeros w,b = initialize_with_zeros(X_train.shape[0]) # num_px * num_px * 3, w: (dim, 1), b: a scalar # Gradient descent parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary "parameters" w = parameters["w"] b = parameters["b"] # Predict test/train set examples Y_prediction_test = predict(w, b, X_test) Y_prediction_train = predict(w, b, X_train) # Print train/test Errors print("train accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123;"costs" : costs, "Y_prediction_test" : Y_prediction_test, "Y_prediction_train" : Y_prediction_train, "w" : w, "b" : b, "learning_rate" : learning_rate, "num_iterations" : num_iterations &#125; return d Run the following cell to train your model: 1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =2000, learning_rate = 0.005, print_cost = True) The results are as follows: 12345678910111213141516171819202122Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 % Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week! Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Let's also plot the cost function and the gradients: 1234567# Plot learning curve (with costs)costs = np.squeeze(d['costs'])plt.plot(costs)plt.ylabel('cost')plt.xlabel('iterations (per hundreds)')plt.title("Learning rate =" + str(d["learning_rate"]))plt.show() Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. Further analysisCongratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α. Choice of learning rateReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate. Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. Code: 1234567891011121314151617learning_rates = [0.01, 0.001, 0.0001]models = &#123;&#125;for i in learning_rates: print ("learning rate is: " + str(i)) models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False) print ('\n' + "-------------------------------------------------------" + '\n')for i in learning_rates: plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))plt.ylabel('cost')plt.xlabel('iterations')legend = plt.legend(loc='upper center', shadow= True)frame = legend.get_frame()frame.set_facecolor('0.90')plt.show() Result: 1234567891011121314151617learning rate is: 0.01train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001train accuracy: 68.42105263157895 %test accuracy: 36.0 %------------------------------------------------------- Interpretation: Different learning rates give different costs and thus different predictions results. If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) Source Code]]></content>
      <categories>
        <category>Coursera深度学习笔记</category>
      </categories>
      <tags>
        <tag>Neural Networks</tag>
        <tag>Deep Learning</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo教程：使用Hexo写博客]]></title>
    <url>%2F2019%2F07%2F31%2Fhello-hexo-markdown%2F</url>
    <content type="text"><![CDATA[尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。 新建文档假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可： $ hexo new “hello hexo markdown” 上述命令的结果是在 ./hexo/source/_posts 路径下新建了一个 hello-hexo-markdown.md 文件。 然后，我们就可以打开编辑器尽情地写作了。 文档格式我们使用文本编辑器打开刚刚新建的 hello-hexo-markdown.md 文件，会发现其中已经存在内容： 12345---title: hello hexo markdowndate: 2019-07-31 20:44:47tags:--- 这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 --- 分隔的区域。其中， title的值是当前文档名，也是将来在网页中显示的文章标题 date值是我们新建文档时的当时地区时间 tags值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下： 12345678---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown--- 上面的配置参数为这篇文档贴上了 hello、hexo、mardown 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。 除了以上这些，还有很多预先定义的参数Front_mtter，我们这里选取一个常用且较为典型的配置参数categories讲解一下。 文章分类categories 是用来给文章分类的，它跟 tags 不同的是其具有顺序性和层次性。 例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，categories 有点儿像新建文件夹对文档进行分门别类的归置。 categories 的用法同 tags 一样，只不过斗个 categories 值是分先后顺序的。 引用资源写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。 这时，有两种解决办法： 使用绝对路径引用资源，在 Web 世界中就是资源的 URL 使用相对路径引用资源 文章资料文件夹如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。 使用文本编辑器打开站点根目录下的 _ config.yml 文件，将 post_asset_folder 值设置为 true。 1post_asset_folder: true 上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 hexo new &lt;title&gt; 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。 相对路径引用的标签插件通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题： 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 比如说：当你打开文章资源文件夹功能后，你把一个 example.jpg 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 ![](/example.jpg) ，它将 不会 出现在首页上。（但是它会在文章中按你期待的方式工作） ！！！注意： 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 ./资源名称，不用在引用路径中添加同名文件夹目录层级。 正确的引用图片方式是使用下列的标签插件而不是 markdown ： 1&#123;% asset_img example.jpg This is an example image %&#125; 通过这种方式，图片将会同时出现在文章和主页以及归档页中。 文章摘要有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。 所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。 设置文章摘要，我们只需在想显示为摘要的内容之后添 &lt;!-- more --&gt; 即可。像下面这样： 1234567891011---title: hello hexo markdowndate: 2016-11-16 18:11:25tags:- hello- hexo- markdown---我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧&lt;!-- more --&gt;紧接着文章摘要的正文内容 这样，&lt;!-- more --&gt; 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。 注意！文章摘要在文章详情页是正文中最前面的内容。 生成文件清除缓存文件为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令： 1$ hexo clean 上述命令会清除在本地站点文件下的缓存文件(db.json)和已有的静态文件(public). 生成静态文件写好MarkDown文档之后，我们就可以使用以下命令生成静态文件： 1$ hexo generate 然后我们就可以启动 Hexo 服务器，使用浏览器打开 http://localhost:4000 查看效果了。 示范下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
