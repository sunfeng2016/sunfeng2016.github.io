<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Coursera深度学习笔记,">





  <link rel="alternate" href="/atom.xml" title="SunFeng's Blog" type="application/atom+xml">






<meta name="description" content="Logistic Regression with a Neural Network mindset">
<meta name="keywords" content="Coursera深度学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset">
<meta property="og:url" content="http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/index.html">
<meta property="og:site_name" content="SunFeng&#39;s Blog">
<meta property="og:description" content="Logistic Regression with a Neural Network mindset">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure1.png">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/Figure2.png">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure3.png">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure4.png">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure5.png">
<meta property="og:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure6.png">
<meta property="og:updated_time" content="2019-08-08T03:28:35.916Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset">
<meta name="twitter:description" content="Logistic Regression with a Neural Network mindset">
<meta name="twitter:image" content="http://sunfeng.online/2019/08/01/课程一(Neural%20Networks%20and%20Deep%20Learning)，第二周（Basics%20of%20Neural%20Network%20programming）——%20Programming%20assignment%202、Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/figure1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/">





  <title>课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset | SunFeng's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SunFeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">学习，敲码，孤独终老！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="SunFeng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/sun.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SunFeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-01T16:06:21+08:00">
                2019-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coursera深度学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Coursera深度学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3,094
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  19
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a><strong>Logistic Regression with a Neural Network mindset</strong></h3><a id="more"></a>
<p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li>
</ul>
<p><strong>You will learn to:</strong></p>
<ul>
<li><p>Build the general architecture of a learning algorithm, including:</p>
<ul>
<li>Initializing parameters</li>
<li>Calculating the cost function and its gradient</li>
<li>Using an optimization algorithm (gradient descent)</li>
</ul>
</li>
<li><p>Gather all three functions above into a main function, in the right order.</p>
</li>
</ul>
<hr>
<h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p>
<ul>
<li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python.</li>
<li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li>
<li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python.</li>
<li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li>
</ul>
<p>code ————-&gt;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Overview-of-the-Problem-set"><a href="#Overview-of-the-Problem-set" class="headerlink" title="Overview of the Problem set"></a>Overview of the Problem set</h3><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p>
<blockquote>
<ul>
<li>a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0)</li>
<li>a test set of m_test images labeled as cat or non-cat</li>
<li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)</li>
</ul>
</blockquote>
<p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p>
<p>Let’s get more familiar with the dataset. Load the data by running the following code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure>
<p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p>
<p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p>
<p><strong>code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data preprocessing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_Preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load dataset and preprocess dataset</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @train_set_x: your train set features</span></span><br><span class="line"><span class="string">        @train_set_y: your train set labels</span></span><br><span class="line"><span class="string">        @test_set_x: your test set features</span></span><br><span class="line"><span class="string">        @test_set_y: your test set labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line">    <span class="comment"># load dataset from dataset files</span></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    <span class="comment"># vectorize the features of each examples</span></span><br><span class="line">    train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">    test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line">    <span class="comment"># normalize the features vector</span></span><br><span class="line">    <span class="keyword">return</span> train_set_x, train_set_y, test_set_x, test_set_y, classes</span><br></pre></td></tr></table></figure>
<font color="#0099ff">**What you need remember:**</font>

<font color="#0099ff">Common steps for pre-processing a new dataset are:</font>

<ul>
<li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li>
<li>Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1)</li>
<li>“Standardize” the data</li>
</ul>
<hr>
<h3 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h3><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p>
<p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure1.png" alt="Figure1"></p>
<p><strong>Mathematical expression of the algorithm:</strong></p>
<p>For one examples x(i):</p>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/Figure2.png" alt="Figure2"></p>
<p>The cost is the computed by summing over all training examples:</p>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure3.png" alt="Figure3"></p>
<p><strong>Key steps:</strong> In this exercise, you will carry out the following steps:</p>
<blockquote>
<ul>
<li>Initialize the parameters of the model</li>
<li>Learn the parameters for the model by minimizing the cost</li>
<li>Use the learned parameters to make predictions (on the test set)</li>
<li>Analyze the results and conclude</li>
</ul>
</blockquote>
<hr>
<h3 id="Building-the-parts-of-our-algorithm"><a href="#Building-the-parts-of-our-algorithm" class="headerlink" title="Building the parts of our algorithm"></a>Building the parts of our algorithm</h3><p>The main steps for building a Neural Network are:</p>
<ol>
<li>Define the model structure (such as number of input features)</li>
<li>Initialize the model’s parameters</li>
<li>Loop:<ul>
<li><strong>Calculate current loss (forward propagation)</strong></li>
<li><strong>Calculate current gradient (backward propagation)</strong></li>
<li><strong>Update parameters (gradient descent)</strong></li>
</ul>
</li>
</ol>
<p>You often build 1-3 separately and integrate them into one function we call model().</p>
<h4 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h4><p><strong>Exercise:</strong> using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute </p>
<script type="math/tex; mode=display">
sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}</script><p>to make predictions. Use np.exp().</p>
<p><strong>code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @z: A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @s: sigmoid(z)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<h4 id="Initializing-parameters"><a href="#Initializing-parameters" class="headerlink" title="Initializing parameters"></a>Initializing parameters</h4><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation.</p>
<p><strong>code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initializing parameters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @dim: size of the w vector we want</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @w: initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">        @b: initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p>
<p><strong>Exercise:</strong> Implement a function propagation() that computes the cost function and its gradient.</p>
<p><strong>Hints(提示):</strong></p>
<font color="#0099ff">Forward Propagation:</font>

<ul>
<li><p>You get x</p>
</li>
<li><p>You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$</p>
</li>
<li><p>You calculate the cost function: </p>
<p>$J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 -  y^{(i)})log(1 - a^{(i)}) }$</p>
</li>
</ul>
<p>Here are the two formulas you will be using:</p>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure4.png" alt="Figure4"></p>
<p><strong>code: </strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward and Backword propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the</span></span><br><span class="line"><span class="string">    propagation explained above</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @cost: negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">        @dw: gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">        @db: gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># nx</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b))  <span class="comment"># compute activation</span></span><br><span class="line">    cost = -(np.dot(Y, np.log(A).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A).T)) / m <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m <span class="comment"># compute dw</span></span><br><span class="line">    db = np.sum(A - Y) / m      <span class="comment"># compute db</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 把shape中为1的维度去掉</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    <span class="comment"># 判断剩下的是否为空</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, </span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul>
<li>You have initialized your parameters.</li>
<li>You are also able to compute a cost function and its gradient.</li>
<li>Now, you want to update the parameters using gradient descent.</li>
</ul>
<p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Optimization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @num_iterations: number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        @learning_rate: learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        @print_cost: True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">        @grads: dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">        @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">        You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        (1) Calculate the cost and the gradient for the current parameters. Use propagate()</span></span><br><span class="line"><span class="string">        (2) Update the parameters using gradient descent rule for w and b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update rule</span></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %d: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure>
<h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p>
<ol>
<li>Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$</li>
<li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_Prediction</code>. </li>
</ol>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Graded function: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned </span></span><br><span class="line"><span class="string">    logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @Y_prediction: a numpy array (vector) containing all prediction</span></span><br><span class="line"><span class="string">                       (0 / 1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m =  X.shape[<span class="number">1</span>]     <span class="comment"># number of examples</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b)) <span class="comment"># (1, m)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0, i] to actual predictions p[0, i]</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure>
<font color="#0099ff">**What you need remember:** You've implemented several functions that:</font>

<ul>
<li>initialize (w, b)</li>
<li>Optimize the loss iteratively to learn parameters (w, b):<ul>
<li>computing the cost and its gradient</li>
<li>updating the parameters using gradient descent</li>
</ul>
</li>
<li>Use the learned (w, b) to predict the labels for a given set of examples</li>
</ul>
<hr>
<h3 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h3><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p>
<p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p>
<blockquote>
<ul>
<li>Y_prediction for your predictions on the test set</li>
<li>Y_prediction_train for your predictions on the train set</li>
<li>w, costs, grads for the outputs of optimize()</li>
</ul>
</blockquote>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all functions into a model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function</span></span><br><span class="line"><span class="string">    you have implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">        @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">        @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">        @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test)</span></span><br><span class="line"><span class="string">        @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">        @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">        @print_cost: Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @d: dictionary containing information about the model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initailize parameters with zeros</span></span><br><span class="line">    w,b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])   <span class="comment"># num_px * num_px * 3, w: (dim, 1), b: a scalar</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples </span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span> : costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span> : Y_prediction_test,</span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">"w"</span> : w,</span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span> : num_iterations</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<p>Run the following cell to train your model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =<span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>The results are as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.376007</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.331463</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.303273</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.279880</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.260042</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.242941</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.228004</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.214820</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.203078</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.192544</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.183033</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.174399</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.166521</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.159305</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.152667</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.146542</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure>
<p><strong>Comment:</strong> Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p>
<p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. </p>
<font color="#0099ff">Let's also plot the cost function and the gradients:</font>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure5.png" alt="Figure5"></p>
<p><strong>Interpretation</strong>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p>
<hr>
<h3 id="Further-analysis"><a href="#Further-analysis" class="headerlink" title="Further analysis"></a>Further analysis</h3><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α.</p>
<h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p>
<p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow= <span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure6.png" alt="Figure6"></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>Different learning rates give different costs and thus different predictions results.</li>
<li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li>
<li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li>
<li>In deep learning, we usually recommend that you:<ul>
<li>Choose the learning rate that better minimizes the cost function.</li>
<li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li>
</ul>
</li>
</ul>
<hr>
<p><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment2_logistics_regression" target="_blank" rel="noopener">Source Code</a></p>

      
    </div>
    
    
    

    <div>
      
      <div>
    
    <div style="text-align:center;color:#ccc;font-size:14px;">
        -------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------
    </div>
    
</div>
      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera深度学习笔记/" rel="tag"># Coursera深度学习笔记</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/31/hello-hexo-markdown/" rel="next" title="Hexo教程：使用Hexo写博客">
                <i class="fa fa-chevron-left"></i> Hexo教程：使用Hexo写博客
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/02/如何上传本地代码到github/" rel="prev" title="如何上传本地代码到github">
                如何上传本地代码到github <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/sun.jpg" alt="SunFeng">
            
              <p class="site-author-name" itemprop="name">SunFeng</p>
              <p class="site-description motion-element" itemprop="description">南京航空航天大学 | 计算机科学与技术学院 | 物联网工程专业</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/sunfeng2016" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:SunFeng@nuaa.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-with-a-Neural-Network-mindset"><span class="nav-number">1.</span> <span class="nav-text">Logistic Regression with a Neural Network mindset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Packages"><span class="nav-number">2.</span> <span class="nav-text">Packages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Overview-of-the-Problem-set"><span class="nav-number">3.</span> <span class="nav-text">Overview of the Problem set</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#General-Architecture-of-the-learning-algorithm"><span class="nav-number">4.</span> <span class="nav-text">General Architecture of the learning algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-the-parts-of-our-algorithm"><span class="nav-number">5.</span> <span class="nav-text">Building the parts of our algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Helper-functions"><span class="nav-number">5.1.</span> <span class="nav-text">Helper functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Initializing-parameters"><span class="nav-number">5.2.</span> <span class="nav-text">Initializing parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-and-Backward-propagation"><span class="nav-number">5.3.</span> <span class="nav-text">Forward and Backward propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimization"><span class="nav-number">5.4.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Predict"><span class="nav-number">5.5.</span> <span class="nav-text">Predict</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Merge-all-functions-into-a-model"><span class="nav-number">6.</span> <span class="nav-text">Merge all functions into a model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Further-analysis"><span class="nav-number">7.</span> <span class="nav-text">Further analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choice-of-learning-rate"><span class="nav-number">7.1.</span> <span class="nav-text">Choice of learning rate</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SunFeng</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">30.8k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共30.8k字</span>
</div>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
