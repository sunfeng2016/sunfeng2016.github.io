<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Coursera深度学习笔记,">





  <link rel="alternate" href="/atom.xml" title="SunFeng's Blog" type="application/atom+xml">






<meta name="description" content="Building your Deep Neural Network: Step by Step">
<meta name="keywords" content="Coursera深度学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step">
<meta property="og:url" content="http://sunfeng.online/2019/08/07/课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step/index.html">
<meta property="og:site_name" content="SunFeng&#39;s Blog">
<meta property="og:description" content="Building your Deep Neural Network: Step by Step">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127165838722-72534730.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200241753-1787058979.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200327409-197353303.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200424847-714067061.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127213245862-62967896.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127233429440-720852258.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235155972-308393091.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235506737-1259548081.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128142511269-248723414.png">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128164243597-30974158.png">
<meta property="og:updated_time" content="2019-08-08T03:27:53.124Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step">
<meta name="twitter:description" content="Building your Deep Neural Network: Step by Step">
<meta name="twitter:image" content="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127165838722-72534730.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://sunfeng.online/2019/08/07/课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step/">





  <title>课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step | SunFeng's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SunFeng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">学习，敲码，孤独终老！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sunfeng.online/2019/08/07/课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="SunFeng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/sun.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SunFeng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-07T15:58:04+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Coursera深度学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Coursera深度学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4,647
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  29
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h3><a id="more"></a>
<p>Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a 2-layer Neural Network (with a signal hidden layer). This week, you will build a deep neural network, with as many layers as you want!</p>
<ul>
<li>In this notebook, you will implement all the functions required to build a deep neural network.</li>
<li>In the next assignment, you will use these functions to build a deep neural network for image classification.</li>
</ul>
<p><strong>After this assignment you will be able to:</strong></p>
<ul>
<li>Use non-linear units like ReLU to improve your model</li>
<li>Build a deeper neural network (with more than 1 hidden layer)</li>
<li>Implement an easy-to-use neural network class</li>
</ul>
<p><strong>Notation:</strong></p>
<ul>
<li>Superscript [I] denotes a quantity associated with the $1^{th}$ layer.<ul>
<li>Example: $a^{[L]}$is the $L^{th}$ layer activation. $W^{[L]}$ and  $b^{[L]}$ are the $L^{th}$  layer parameters.</li>
</ul>
</li>
<li>Superscript (i) denotes a quantity associated with the $i^{th}$ example.<ul>
<li>Example: $x^{(i)}$ is the $i^{th}$ training example.</li>
</ul>
</li>
<li>Lowerscript i denotes the $i^{th}$ entry of a vector.<ul>
<li>Example: $a_i^{[I]}$ denotes the $i^{th}$ entry of the $I^{th}$ layer’s activations.</li>
</ul>
</li>
</ul>
<p><strong>Let’s get started!</strong></p>
<hr>
<h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p>
<ul>
<li><a href="https://numpy.org/" target="_blank" rel="noopener">numpy</a> is the main packages for scientific computing with Python.</li>
<li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li>
<li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/dnn_utils_v2.py" target="_blank" rel="noopener">dnn_utils</a> provides some necessary functions for this notebook.</li>
<li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/testCases_v2.py" target="_blank" rel="noopener">testCases</a> provides some test cases to assess the correctness of your functions.</li>
<li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li>
</ul>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Outline-of-the-Assignment"><a href="#Outline-of-the-Assignment" class="headerlink" title="Outline of the Assignment"></a>Outline of the Assignment</h3><p>To build your neural network, you will be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of the assignment, you will:</p>
<ul>
<li>Initialize the parameters for a two-layer network and for an L-layer neural network.</li>
<li>Implement the forward propagation module (shown in purple  in the figure below).<ul>
<li>Complete the LINEAR part of a layer’s forward propagation step (resulting in $Z^{l}$).</li>
<li>We give you the ACTIVATION function (relu / sigmoid).</li>
<li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li>
<li>Stack the [LINEAR-&gt;RELU] forward function L-1 times (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li>
</ul>
</li>
<li>Compute the loss.</li>
<li>Implement the backward propagation module (denoted in red in the figure below).<ul>
<li>Complete the LINEAR part of a layer’s backward propagation step.</li>
<li>We give you the gradient of the ACTIVATION function (relu_backward / sigmoid_backward).</li>
<li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li>
<li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li>
</ul>
</li>
<li>Finally update the parameters.</li>
</ul>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127165838722-72534730.png" alt="img"></p>
<p><strong>Note</strong> that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backward propagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps. </p>
<hr>
<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>You will write two helper functions that will initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one will generalize this initialization process to L layers.</p>
<h4 id="2-layer-Neural-Network"><a href="#2-layer-Neural-Network" class="headerlink" title="2-layer Neural Network"></a>2-layer Neural Network</h4><p><strong>Exercise:</strong> Create and initialize the parameters of the 2-layer neural network.</p>
<p><strong>Instructions:</strong> </p>
<ul>
<li>The model’s structure is: <em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID​.</em></li>
<li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) *0.01</code> with the correct shape.</li>
<li>Use zero initialize for the biases. Use <code>np.zeros(shape)</code>.</li>
</ul>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 2-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize parameters for a two-layer network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @n_x: size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01624345</span> <span class="number">-0.00611756</span> <span class="number">-0.00528172</span>]</span><br><span class="line"> [<span class="number">-0.01072969</span>  <span class="number">0.00865408</span> <span class="number">-0.02301539</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[ <span class="number">0.01744812</span> <span class="number">-0.00761207</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h4><p>The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the <code>initialize_parameters_deep</code>, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units of layer l.</p>
<p>Thus for example if the size of our input X is (12288, 209) (with m = 209 examples) then:</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200241753-1787058979.png" alt="img"></p>
<p>Remember that when we compute $WX + b$ in Python, it carries out broadcasting. For example, if:</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200327409-197353303.png" alt="img"></p>
<p>The $WX + b$ will be:</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200424847-714067061.png" alt="img"></p>
<p><strong>Exercise:</strong> Implement initialization for an L-layer Neural Network.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>The model’s structure is <em>[LINEAR -&gt; RELU]</em> $\times$ <em>(L-1) -&gt; LINEAR -&gt; SIGMOID.</em> It has <em>L-1</em> layers using a ReLU activation function followed by an output layer with a sigmoid activation function.</li>
<li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) * 0.01</code>.</li>
<li>We will store $n^{[l]}$, the number of units in different layers, in a variables <code>layer_dims</code>. For example, the <code>layer_dims</code> for the “Planar Data classification model” from last week would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. Thus means <code>W1</code>‘s shape was (4,2), <code>b1</code> was (4,1), <code>W2</code> was (1,4) and <code>b2</code> was (1,1). Now you will generalize this to LL layers!</li>
<li>Here is the implementation for L = 1 (one layer neural network). It should inspire you to implement the general case (L-layer neural network).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> L == <span class="number">1</span>:</span><br><span class="line">      parameters[<span class="string">"W"</span> + str(L)] = np.random.randn(layer_dims[<span class="number">1</span>], layer_dims[<span class="number">0</span>]) * <span class="number">0.01</span></span><br><span class="line">      parameters[<span class="string">"b"</span> + str(L)] = np.zeros((layer_dims[<span class="number">1</span>], <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. L-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initailize parameters for an L-layer neural network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @layer_dims: python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters "W1", "b1",...</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (layer_dims[1], 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)     <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># Random Initialization</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01788628</span>  <span class="number">0.0043651</span>   <span class="number">0.00096497</span> <span class="number">-0.01863493</span> <span class="number">-0.00277388</span>]</span><br><span class="line"> [<span class="number">-0.00354759</span> <span class="number">-0.00082741</span> <span class="number">-0.00627001</span> <span class="number">-0.00043818</span> <span class="number">-0.00477218</span>]</span><br><span class="line"> [<span class="number">-0.01313865</span>  <span class="number">0.00884622</span>  <span class="number">0.00881318</span>  <span class="number">0.01709573</span>  <span class="number">0.00050034</span>]</span><br><span class="line"> [<span class="number">-0.00404677</span> <span class="number">-0.0054536</span>  <span class="number">-0.01546477</span>  <span class="number">0.00982367</span> <span class="number">-0.01101068</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.01185047</span> <span class="number">-0.0020565</span>   <span class="number">0.01486148</span>  <span class="number">0.00236716</span>]</span><br><span class="line"> [<span class="number">-0.01023785</span> <span class="number">-0.00712993</span>  <span class="number">0.00625245</span> <span class="number">-0.00160513</span>]</span><br><span class="line"> [<span class="number">-0.00768836</span> <span class="number">-0.00230031</span>  <span class="number">0.00745056</span>  <span class="number">0.01976111</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Forward-propagation-module"><a href="#Forward-propagation-module" class="headerlink" title="Forward propagation module"></a>Forward propagation module</h3><h4 id="Linear-Forward"><a href="#Linear-Forward" class="headerlink" title="Linear Forward"></a>Linear Forward</h4><p>Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:</p>
<ul>
<li>LINEAR</li>
<li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li>
<li>[LINEAR -&gt; RELU] $\times$ (L-1) -&gt; LINEAR -&gt;SIGMOID (whole model)</li>
</ul>
<p>The linear forward module (vectorized over all the examples) computes the following equations:</p>
<script type="math/tex; mode=display">
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}</script><p>where $A^{[0]} = X$.</p>
<p><strong>Exercise:</strong> Build the linear part of forward propagation.</p>
<p><strong>Reminder:</strong> The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. You may also find <code>np.dot()</code> useful. If your dimensions don’t match, printing <code>W.shape</code> may help.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @z: the input of activation function, also called pre-activation parameter</span></span><br><span class="line"><span class="string">        @cache: a python dictionary containing "A", "W", and "b"; stored for computing the backword pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A, W, b = linear_forward_test_case()</span><br><span class="line"></span><br><span class="line">Z, linear_cache = linear_forward(A, W, b)</span><br><span class="line">print(<span class="string">"Z = "</span> + str(Z))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = [[ <span class="number">3.26295337</span> <span class="number">-1.23429987</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="Linear-Activation-Forward"><a href="#Linear-Activation-Forward" class="headerlink" title="Linear-Activation Forward"></a>Linear-Activation Forward</h4><p>In this notebook, you will use two activation functions:</p>
<ul>
<li><p><strong>Sigmoid:</strong> $\sigma(Z) = \sigma(WA + b) = \frac{1}{1 + e^{-(WA + b)}}$. We have provided you with the <code>sigmoid</code> function. This function returns two items: the activation value <code>&quot;a&quot;</code> and a <code>&quot;cache&quot;</code> that contains <code>&quot;Z&quot;</code> (it’s what we feed in to the corresponding backward function). To use it you could just call:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = sigmoid(Z)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>ReLU:</strong> The mathematical formula for ReLu is $A = ReLU(Z) = max(0, Z)$. We have provided you with the <code>relu</code> function. This function returns <strong>two</strong> items: the activation value “<code>A</code>“ and a “<code>cache</code>“ that contains “<code>Z</code>“ (it’s what we will feed in to the corresponding backward function). To use it you could just call:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = relu(Z)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.</p>
<p><strong>Exercise:</strong> Implement the forward propagation of the <em>LINEAR -&gt; ACTIVATION</em> layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$ where the activation <code>&quot;g&quot;</code> can be <code>sigmoid()</code> or <code>relu()</code>. Use <code>linear_forward()</code> and the correct activation function.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A_prev: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weight matrix: numpy array of shape (size pf current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector: numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">        @activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        @A: the output of the activation function, also called the post-activation value</span></span><br><span class="line"><span class="string">        @cache: a python tuple containing "linear_cache" and "activation_cache"; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A_prev, W, b = linear_activation_forward_test_case()</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A))</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>)</span><br><span class="line">print(<span class="string">"With ReLU: A = "</span> + str(A))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">With sigmoid: A = [[ <span class="number">0.96890023</span>  <span class="number">0.11013289</span>]]</span><br><span class="line">With ReLU: A = [[ <span class="number">3.43896131</span>  <span class="number">0.</span>        ]]</span><br></pre></td></tr></table></figure>
<p><strong>Note:</strong> In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.</p>
<h4 id="L-Layer-Model"><a href="#L-Layer-Model" class="headerlink" title="L-Layer Model"></a>L-Layer Model</h4><p>For even convenience when implementing the L-layer Neural Network, you will need a function that replicates the previous one <code>linear_activation_forward</code> with ReLU L-1 times, then follows that with one <code>linear_activation_forward</code>with SIGMOID.</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127213245862-62967896.png" alt="img"></p>
<p><strong>Exercise:</strong> Implement the forward propagation of the above model. </p>
<p><strong>Instruction:</strong> In the code below, the variable AL will denote </p>
<p>$A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]}A^{[L-1]} + b^{[L]})$.</p>
<p><strong>Tips:</strong> </p>
<ul>
<li>Use the functions you had previously written.</li>
<li>Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times</li>
<li>Don’t forget to keep track of the caches in the “caches” list. To add a new value <code>c</code> to a <code>list</code>, you can use <code>list.append(c)</code>.</li>
</ul>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L-layer Model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1) -&gt; [LINEAR-&gt;SIGMOID] computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @AL -- last post-activation value</span></span><br><span class="line"><span class="string">        @caches -- list of caches containing:</span></span><br><span class="line"><span class="string">            every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">            the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU] * (L-1).</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line"></span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], activation = <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; SIGMOID]</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, parameters = L_model_forward_test_case_2hidden()</span><br><span class="line">AL, caches = L_model_forward(X, parameters)</span><br><span class="line">print(<span class="string">"AL = "</span> + str(AL))</span><br><span class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AL = [[ <span class="number">0.03921668</span>  <span class="number">0.70498921</span>  <span class="number">0.19734387</span>  <span class="number">0.04728177</span>]]</span><br><span class="line">Length of caches list = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>Great! Now you have a full forward propagation that takes the input X and outputs a row vector AL containing your predictions. It also records all intermediate values in “caches”. Using AL, you can computes the cost of your predictions.</p>
<hr>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.</p>
<p><strong>Exercise:</strong> Compute the cross-entropy cost J, using the following formula:</p>
<script type="math/tex; mode=display">
J = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}</script><p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function (the cross-entropy cost J)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape of (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y</span></span><br><span class="line">    cost = - (np.dot(Y, np.log(AL).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL).T)) / m</span><br><span class="line">    cost = np.squeeze(cost) <span class="comment"># To make sure your cost's shape is waht we expect</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y, AL = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = <span class="number">0.414931599615397</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Backward-propagation-module"><a href="#Backward-propagation-module" class="headerlink" title="Backward propagation module"></a>Backward propagation module</h3><p>Just like with forward propagation, you will implement helper functions for backward propagation. Remember that backward propagation is used to calculate the gradient of the loss function with respect to the parameters.</p>
<p><strong>Reminder:</strong></p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127233429440-720852258.png" alt="img"></p>
<p><em>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</em></p>
<p>Now, similar to forward propagation, you are going to build the backward propagation in three steps:</p>
<ul>
<li>LINEAR backward</li>
<li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li>
<li>[LINEAR -&gt; RELU] ×× (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li>
</ul>
<h4 id="Linear-backward"><a href="#Linear-backward" class="headerlink" title="Linear backward"></a>Linear backward</h4><p>For layer l, the linear part is: $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$ (followed by an activation). </p>
<p>Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial J}{\partial Z^{[l]}}$ . You want to get ($dW^{[l]}$, $db^{[l]}$, $dA^{[l]}$).</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235155972-308393091.png" alt="img"></p>
<p>The three output ($dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$) are computed using the input $dZ^{[l]}$. Here are the formulas you need:</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235506737-1259548081.png" alt="img"></p>
<p><strong>Exercise：</strong> Use the 3 formulas above to implement <code>linear_backward().</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA -- Gradient of the cost with respect to the activation (of the prevous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of the cost with respect to the W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of the cost with respect to the b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.sum(dZ, axis = <span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up some test inputs</span></span><br><span class="line">dZ, linear_cache = linear_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dA_prev = [[ <span class="number">0.51822968</span> <span class="number">-0.19517421</span>]</span><br><span class="line"> [<span class="number">-0.40506361</span>  <span class="number">0.15255393</span>]</span><br><span class="line"> [ <span class="number">2.37496825</span> <span class="number">-0.89445391</span>]]</span><br><span class="line">dW = [[<span class="number">-0.10076895</span>  <span class="number">1.40685096</span>  <span class="number">1.64992505</span>]]</span><br><span class="line">db = [[ <span class="number">0.50629448</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="Linear-Activation-backward"><a href="#Linear-Activation-backward" class="headerlink" title="Linear-Activation backward"></a>Linear-Activation backward</h4><p>Next, you will create a function that merges the two helper functions: <code>linear_backward</code> and the backward step for the activation <code>linear_activation_backward</code>.</p>
<p>To help you implement <code>linear_activation_backward</code>, we provided two backward functions:</p>
<ul>
<li><p><strong>sigmoid_backward</strong>: Implements the backward propagation for SIGMOID unit. You can call it as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = sigmoid_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>relu_backward</strong>: Implements the backward propagation for RELU unit. You can call it as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = relu_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>If g(.) is the activation function, <code>sigmoid_backward</code> and <code>relu_backward</code> compute:  $dZ^{[l]} = dA^{[l]} * g^{\prime}(Z^{[l]})$ </p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dA -- post-activation gradient for current layer 1</span></span><br><span class="line"><span class="string">        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">        activation -- the activation function to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA_prev -- Gradient of cost with respect to the activation (of the previous layer l - 1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        </span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dA, linear_activation_cache = linear_activation_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sigmoid:</span><br><span class="line">dA_prev = [[ <span class="number">0.11017994</span>  <span class="number">0.01105339</span>]</span><br><span class="line"> [ <span class="number">0.09466817</span>  <span class="number">0.00949723</span>]</span><br><span class="line"> [<span class="number">-0.05743092</span> <span class="number">-0.00576154</span>]]</span><br><span class="line">dW = [[ <span class="number">0.10266786</span>  <span class="number">0.09778551</span> <span class="number">-0.01968084</span>]]</span><br><span class="line">db = [[<span class="number">-0.05729622</span>]]</span><br><span class="line"></span><br><span class="line">relu:</span><br><span class="line">dA_prev = [[ <span class="number">0.44090989</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.37883606</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.2298228</span>   <span class="number">0.</span>        ]]</span><br><span class="line">dW = [[ <span class="number">0.44513824</span>  <span class="number">0.37371418</span> <span class="number">-0.10478989</span>]]</span><br><span class="line">db = [[<span class="number">-0.20837892</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="L-Model-Backward"><a href="#L-Model-Backward" class="headerlink" title="L-Model Backward"></a>L-Model Backward</h4><p>Now you will implement the backward function for the whole network. Recall that when you implemented the <code>L_model_forward</code> function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the <code>L_model_backward</code> function, you will iterate through all the hidden layers backward, starting from layer L. On each step, you will use the cached values for layer l to backward propagate through layer l. Figure 5 below shows the backward pass.</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128142511269-248723414.png" alt="img"></p>
<p><strong>Initializing backward propagation:</strong>  To implement backward propagate through this network, we know that output is, $A^{[L]} = \sigma(Z^{[L]}) $ . Your code thus need to compute $dAL = \frac{\partial J}{\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don’t need in-depth knowledge of):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br></pre></td></tr></table></figure>
<p>You can then use this post-activation gradient <code>dAL</code> to keep going backward. As seen in Figure 5, you can now feed in <code>dAL</code> into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a <code>for</code> loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula: grads[“dW” + str(l)] = $dW^{[l]}$</p>
<p>For example, for l=3 this would store $dW^{[l]}$ in <code>grads[&quot;dW3&quot;]</code>.</p>
<p><strong>Exercise:</strong> Implement backpropagation for the <em>[LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em> model.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. L-Model Backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backword propagaiton for the [LINEAR-&gt;RELU] * (L - 1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">        caches -- list of caches containing: every cache of linear_activation_forward() with "relu" </span></span><br><span class="line"><span class="string">                  (it's caches[1], for l in range(L - 1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                  the cache of linear_activation_foreward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">                 grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment"># the number of examples</span></span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class="line">    <span class="comment"># Inputs: AL, Y, caches</span></span><br><span class="line">    <span class="comment"># Outputs: grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line"></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients</span></span><br><span class="line">        <span class="comment"># Inputs: grads["dA" + str(l + 2)], caches</span></span><br><span class="line">        <span class="comment"># outputs: grads["dA" + str(l + 1)], grads["dW" + str(l + 1)], grads["db" + str(l + 1)]</span></span><br><span class="line"></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AL, Y_assess, caches = L_model_backward_test_case()</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches)</span><br><span class="line">print_grads(grads)</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ <span class="number">0.41010002</span>  <span class="number">0.07807203</span>  <span class="number">0.13798444</span>  <span class="number">0.10502167</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.05283652</span>  <span class="number">0.01005865</span>  <span class="number">0.01777766</span>  <span class="number">0.0135308</span> ]]</span><br><span class="line">db1 = [[<span class="number">-0.22007063</span>]</span><br><span class="line"> [ <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.02835349</span>]]</span><br><span class="line">dA1 = [[ <span class="number">0.12913162</span> <span class="number">-0.44014127</span>]</span><br><span class="line"> [<span class="number">-0.14175655</span>  <span class="number">0.48317296</span>]</span><br><span class="line"> [ <span class="number">0.01663708</span> <span class="number">-0.05670698</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="Update-Parameters"><a href="#Update-Parameters" class="headerlink" title="Update Parameters"></a>Update Parameters</h4><p>In this section you will update the parameters of the model, using gradient descent:</p>
<p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128164243597-30974158.png" alt="img"></p>
<p>where $\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.</p>
<p><strong>Exercise:</strong> Implement <code>update_parameters()</code> to update your parameters using gradient descent.</p>
<p><strong>Instructions:</strong> Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for l = 1, 2, … L.</p>
<p><strong>Code:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Update Parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent updatte rule</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">                      parameters["W" + str(l)] = ...</span></span><br><span class="line"><span class="string">                      parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = parameters[<span class="string">"W"</span> + str(l)] - learning_rate * grads[<span class="string">"dW"</span> + str(l)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = parameters[<span class="string">"b"</span> + str(l)] - learning_rate * grads[<span class="string">"db"</span> + str(l)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>Test:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Result:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[<span class="number">-0.59562069</span> <span class="number">-0.09991781</span> <span class="number">-2.14584584</span>  <span class="number">1.82662008</span>]</span><br><span class="line"> [<span class="number">-1.76569676</span> <span class="number">-0.80627147</span>  <span class="number">0.51115557</span> <span class="number">-1.18258802</span>]</span><br><span class="line"> [<span class="number">-1.0535704</span>  <span class="number">-0.86128581</span>  <span class="number">0.68284052</span>  <span class="number">2.20374577</span>]]</span><br><span class="line">b1 = [[<span class="number">-0.04659241</span>]</span><br><span class="line"> [<span class="number">-1.28888275</span>]</span><br><span class="line"> [ <span class="number">0.53405496</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.55569196</span>  <span class="number">0.0354055</span>   <span class="number">1.32964895</span>]]</span><br><span class="line">b2 = [[<span class="number">-0.84610769</span>]]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Congrats on implementing all the functions required for building a deep neural network!</p>
<p>We know it was a long assignment but going forward it will only get better. The next part of the assignment is easier.</p>
<p>In the next assignment you will put all these together to build two models:</p>
<ul>
<li>A two-layer neural network</li>
<li>An L-layer neural network</li>
</ul>
<p>You will in fact use these models to classify cat vs non-cat images!</p>

      
    </div>
    
    
    

    <div>
      
      <div>
    
    <div style="text-align:center;color:#ccc;font-size:14px;">
        -------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------
    </div>
    
</div>
      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera深度学习笔记/" rel="tag"># Coursera深度学习笔记</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/" rel="next" title="《机器学习》西瓜书学习笔记（二）-- 模型评估与选择">
                <i class="fa fa-chevron-left"></i> 《机器学习》西瓜书学习笔记（二）-- 模型评估与选择
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/08/课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application/" rel="prev" title="课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application">
                课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/sun.jpg" alt="SunFeng">
            
              <p class="site-author-name" itemprop="name">SunFeng</p>
              <p class="site-description motion-element" itemprop="description">南京航空航天大学 | 计算机科学与技术学院 | 物联网工程专业</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/sunfeng2016" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:SunFeng@nuaa.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-your-Deep-Neural-Network-Step-by-Step"><span class="nav-number">1.</span> <span class="nav-text">Building your Deep Neural Network: Step by Step</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Packages"><span class="nav-number">2.</span> <span class="nav-text">Packages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Outline-of-the-Assignment"><span class="nav-number">3.</span> <span class="nav-text">Outline of the Assignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Initialization"><span class="nav-number">4.</span> <span class="nav-text">Initialization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-layer-Neural-Network"><span class="nav-number">4.1.</span> <span class="nav-text">2-layer Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-layer-Neural-Network"><span class="nav-number">4.2.</span> <span class="nav-text">L-layer Neural Network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-propagation-module"><span class="nav-number">5.</span> <span class="nav-text">Forward propagation module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Forward"><span class="nav-number">5.1.</span> <span class="nav-text">Linear Forward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Activation-Forward"><span class="nav-number">5.2.</span> <span class="nav-text">Linear-Activation Forward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-Layer-Model"><span class="nav-number">5.3.</span> <span class="nav-text">L-Layer Model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-function"><span class="nav-number">6.</span> <span class="nav-text">Cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backward-propagation-module"><span class="nav-number">7.</span> <span class="nav-text">Backward propagation module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-backward"><span class="nav-number">7.1.</span> <span class="nav-text">Linear backward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Activation-backward"><span class="nav-number">7.2.</span> <span class="nav-text">Linear-Activation backward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-Model-Backward"><span class="nav-number">7.3.</span> <span class="nav-text">L-Model Backward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Update-Parameters"><span class="nav-number">7.4.</span> <span class="nav-text">Update Parameters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusion"><span class="nav-number">8.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SunFeng</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">26.2k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共26.2k字</span>
</div>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
