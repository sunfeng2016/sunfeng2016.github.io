<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunFeng&#39;s Blog</title>
  
  <subtitle>学习，敲码，孤独终老！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunfeng.online/"/>
  <updated>2019-10-14T10:24:12.852Z</updated>
  <id>http://sunfeng.online/</id>
  
  <author>
    <name>SunFeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Python Machine Learning》学习笔记 chapter-2</title>
    <link href="http://sunfeng.online/2019/10/14/%E3%80%8APython%20Machine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20chapter-2/"/>
    <id>http://sunfeng.online/2019/10/14/《Python Machine Learning》学习笔记 chapter-2/</id>
    <published>2019-10-14T10:08:02.000Z</published>
    <updated>2019-10-14T10:24:12.852Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">《Python Machine Learning》学习笔记 Chapter 2: Training Simple Machine Learning Algorithms for Classification</font></strong></p><a id="more"></a><font color="#0099ff">The topics that we will cover in this chapter are as follows:</font><ul><li>Building an intuition for machine learning algorithms</li><li>Using pandas, Numpy, and matplotlib to read in, process, and visualize data</li><li>Implementing linear classification algorithm in Python</li></ul><h3 id="Artificial-neurons-a-brief-glimpse-into-the-early-history-of-machine-learning"><a href="#Artificial-neurons-a-brief-glimpse-into-the-early-history-of-machine-learning" class="headerlink" title="Artificial neurons - a brief glimpse into the early history of machine learning"></a>Artificial neurons - a brief glimpse into the early history of machine learning</h3><h4 id="The-formal-definition-of-an-artificial-neuron"><a href="#The-formal-definition-of-an-artificial-neuron" class="headerlink" title="The formal definition of an artificial neuron"></a>The formal definition of an artificial neuron</h4><h4 id="The-perceptron-learning-rule"><a href="#The-perceptron-learning-rule" class="headerlink" title="The perceptron learning rule"></a>The perceptron learning rule</h4><h3 id="Implementing-a-perceptron-learning-algorithm-in-Python"><a href="#Implementing-a-perceptron-learning-algorithm-in-Python" class="headerlink" title="Implementing a perceptron learning algorithm in Python"></a>Implementing a perceptron learning algorithm in Python</h3><h4 id="An-object-oriented-perceptron-API"><a href="#An-object-oriented-perceptron-API" class="headerlink" title="An object-oriented perceptron API"></a>An object-oriented perceptron API</h4><h4 id="Training-a-perceptron-model-on-the-Iris-dataset"><a href="#Training-a-perceptron-model-on-the-Iris-dataset" class="headerlink" title="Training a perceptron model on the Iris dataset"></a>Training a perceptron model on the Iris dataset</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;《Python Machine Learning》学习笔记 Chapter 2: Training Simple Machine Learning Algorithms for Classification&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="《Python Machine Learning》学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8APython-Machine-Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="http://sunfeng.online/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://sunfeng.online/tags/Machine-Learning/"/>
    
      <category term="Perceptron" scheme="http://sunfeng.online/tags/Perceptron/"/>
    
  </entry>
  
  <entry>
    <title>论文常见单词积累（三）</title>
    <link href="http://sunfeng.online/2019/10/14/%E8%AE%BA%E6%96%87%E5%B8%B8%E8%A7%81%E5%8D%95%E8%AF%8D%E7%A7%AF%E7%B4%AF%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://sunfeng.online/2019/10/14/论文常见单词积累（三）/</id>
    <published>2019-10-14T01:45:14.000Z</published>
    <updated>2019-10-14T09:08:21.632Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">论文常见单词积累（三）</font></strong></p><h3 id="2019-10-14"><a href="#2019-10-14" class="headerlink" title="2019/10/14"></a>2019/10/14</h3><p>simultaneous [ˌsɪmlˈteɪniəs] adj. 同时的，联立的，同时发生的 n. 同时译员</p><p>proportional [prəˈpɔːʃənl] adj. 比例的，成比例的，相称的，均衡的 n. 比例项</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;论文常见单词积累（三）&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;2019-10-14&quot;&gt;&lt;a href=&quot;#2019-10-14&quot; class=&quot;headerlink&quot; title=&quot;2019/10/14&quot;&gt;&lt;/
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://sunfeng.online/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="paper" scheme="http://sunfeng.online/tags/paper/"/>
    
      <category term="word" scheme="http://sunfeng.online/tags/word/"/>
    
  </entry>
  
  <entry>
    <title>论文常见单词积累 (二)</title>
    <link href="http://sunfeng.online/2019/10/02/%E8%AE%BA%E6%96%87%E5%B8%B8%E8%A7%81%E5%8D%95%E8%AF%8D%E7%A7%AF%E7%B4%AF%20(%E4%BA%8C)/"/>
    <id>http://sunfeng.online/2019/10/02/论文常见单词积累 (二)/</id>
    <published>2019-10-02T05:15:38.000Z</published>
    <updated>2019-10-13T14:49:28.605Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">论文常见单词积累 (二)</font></strong></p><a id="more"></a><h3 id="2019-10-02"><a href="#2019-10-02" class="headerlink" title="2019/10/02"></a>2019/10/02</h3><p>scarce [skeəs] adj. 缺乏的，不足的；稀有的 adv. 仅仅，几乎不，几乎没有</p><p>scarcity [ˈskeəsəti] n. 不足，缺乏</p><p>inverse [ˌɪnˈvɜːs] n. 相反，倒转；反面；倒数 adj. 相反的，倒转的</p><p>pervasiveness [pərˈveɪsɪvnəs] n. 无处不在的，广泛性，普遍性</p><p>rigorous [ˈrɪɡərəs] adj. 严厉的，严格的，严密的，严酷的</p><p>systematic [ˌsɪstəˈmætɪk] adj. 系统的，体系的，有系统的，分类的，一贯的</p><p>treatment [ˈtriːtmənt] n. 治疗，疗法，处理，对待</p><p>thereof [ˌðeərˈɒv] adv. 它的，由此，在其中，关于…，将它</p><p>circumvent [ˌsɜːkəmˈvent] v. 包围，智取，绕行，规避</p><p>stem [stem] n. 干，茎  v. 阻止，起源于某事物</p><p>adversarial [ˌædvəˈseəriəl] adj. 对抗的，对手的，敌手的</p><p>generic [dʒəˈnerɪk] adj. 类的，一般的，非双标的</p><p>via [ˈvaɪə，ˈviːə]  prep. 经由，通过，凭借，借助于</p><p>benchmark [ˈbentʃmɑːk] n. 基准，标准检查程序 </p><p>degenerate [dɪ’dʒen(ə)rət] v. 使退化，恶化，堕落； adj. 退化的，堕落的</p><hr><h3 id="2019-10-04"><a href="#2019-10-04" class="headerlink" title="2019/10/04"></a>2019/10/04</h3><p>random variable 随机变量</p><p>posterior probability 后验概率</p><p>prior probability 先验概率</p><p>maximum likehood estimation 极大似然估计</p><p>Linear Guassian Model 线性高斯模型</p><p>underlying [ˌʌndəˈlaɪɪŋ] adj. 潜在，根本的，在下面的，优先的 </p><p>volume [ˈvɒljuːm] n. 量，大量 adj. 大量的 v. 把……收集成卷</p><p>prohibitive [prəˈhɪbətɪv] adj. 禁止的，禁止性的，(价格，费用)过高的</p><p>guarantee [ˌɡærənˈtiː] n. 保证，担保，保证人 vt. 保证，担保</p><p>guaranteed [ˌɡærənˈtiːd] adj. 肯定的，保证的</p><p>hinder [ˈhɪndə(r)] v. 成为……的阻碍，阻碍，打扰 adj. 后面的</p><p>conduct [kənˈdʌkt] v. 组织，实施，代领，引导 n. 行为举止，管理</p><p>empirical [ɪmˈpɪrɪkl] adj. 经验主义的，完全根据经验的，实证的</p><p>intuitive [ɪnˈtjuːɪtɪv] adj. 直觉的，凭直觉获知的</p><p>factor [ˈfæktə(r)] n. 因素，要素，代理人</p><p>divergence [daɪˈvɜːdʒəns] n. 差异，分歧，分散，发散</p><p>crucial [ˈkruːʃl] adj. 重要的，决定性的，定局的，决断的</p><p>derive [dɪˈraɪv] v. 源于，得自，获得</p><p>tractable [ˈtræktəbl] adj. 易于管教的，易驾驭，易处理的，驯良的</p><p>reveal [rɪˈviːl] vt. 显示，透露，揭露 n. 揭露，暴露</p><p>margin [ˈmɑːdʒɪn] n. 边缘，利润 v. 加边于</p><p>marginal [ˈmɑːdʒɪnl] adj. 微不足道的，不重要的，边缘的，临界的 </p><p>rethink [ˈriːθɪŋk] vt. 重新考虑，再想</p><p>conventional [kənˈvenʃənl] adj. 符合习俗的，传统的，常见的，惯例的</p><p>convention [kənˈvenʃn] n. 大会，惯例，约定，协定，习俗</p><p>comparison [kəmˈpærɪsn] n. 比较，对比，比喻，比较关系</p><hr><h3 id="2019-10-05"><a href="#2019-10-05" class="headerlink" title="2019/10/05"></a>2019/10/05</h3><p>hypothesis [haɪˈpɒθəsɪs] n. 假设 (复数 hypotheses)</p><p>notion [ˈnəʊʃn] n. 概念，见解，打算</p><p>notation [nəʊˈteɪʃn] n. 符号，乐谱，注释，记号法</p><p>define [dɪˈfaɪn] v. 定义，使明确，规定</p><p>definition [ˌdefɪˈnɪʃn] n. 定义</p><p>conceal [kənˈsiːl] v. 隐藏，隐瞒</p><p>specify [ˈspesɪfaɪ] v. 指定，详细说明，列举，把……列入说明</p><p>vice versa [vaɪs ‘vɜ:sə] 反之亦然</p><p>divergent [daɪˈvɜːdʒənt] adj. 相异的，分歧的，求异的，发散的</p><p>divergence [daɪˈvɜːdʒəns] n. 差异，分歧，分散，发散</p><p>convergence [kənˈvɜːdʒəns] n. 收敛，会聚，集合</p><p>quantify [ˈkwɒntɪfaɪ] v. 量化，为……定量，确定数量</p><p>quantity [ˈkwɒntəti] n. 量，数量，大量，总量</p><p>quantifiable [ˈkwɒntɪfaɪəbl; ˌkwɒntɪˈfaɪəbl] adj. 可以计量的</p><p>discard [dɪˈskɑːd] vt. 抛弃，放弃，丢弃</p><p>feasibility [ˌfiːzəˈbɪləti] n. 可行性，可能性</p><p>reliability [rɪˌlaɪəˈbɪləti] n. 可靠性</p><p>component [kəmˈpəʊnənt] n. 组成部分，成分</p><p>resort [rɪˈzɔːt] n. 凭借，手段 v. 求助，诉诸，常去</p><p>contrast [ˈkɒntrɑːst] n. 明显对比，对比，对照，反差 v. 对比，对照</p><p>mitigate [ˈmɪtɪɡeɪt] vt. 使缓和，使减轻 </p><p>regularity [ˌreɡjuˈlærəti] n. 规则性，整齐，正规，匀称</p><p>regularize [ˈreɡjələraɪz] vt. 调整，使有秩序</p><p>regularization [,rɛɡjʊlərɪ’zeʃən] n. 规则化，标准化，合法化</p><p>implicit [ɪmˈplɪsɪt] adj. 含蓄的，暗示的，盲从的</p><p>eliminate [ɪˈlɪmɪneɪt] v. 消除，排除</p><p>proper [ˈprɒpə(r)] adj. 适当的，本身的，特有的，正派的 adv. 完全地</p><p>derive [dɪˈraɪv] v. 起源，源于、获得</p><p>instantiate [ɪn’stænʃɪeɪt] v. 例示，举例说明</p><p>mechanism [ˈmekənɪzəm] n. 机制，原理，途径，进程</p><p>augment [ɔːɡˈment] v. 增大，增加</p><p>incorporate [ɪnˈkɔːpəreɪt] v. 包含，吸收，合并 adj. 合并的，一体化的</p><p>dummy [ˈdʌmi] adj. 假的，仿真的 n. 人体模型，仿制品</p><p>theory [ˈθɪəri] n. 理论，原理，学说，推测</p><p>theoretical [ˌθɪəˈretɪkl] adj. 理论的，理论上的，假设的，推理的</p><p>invariant [ɪnˈveəriənt] adj. 不变的 n. 不变量</p><p>variant [ˈveəriənt] n. 变体，转化 adj. 不同的，多样的</p><hr><h3 id="2019-10-06"><a href="#2019-10-06" class="headerlink" title="2019/10/06"></a>2019/10/06</h3><p>density [ˈdensəti] n. 密度</p><p>significant [sɪɡˈnɪfɪkənt] adj. 重大的，有效的，有意义的 n. 象征，有意义的事物</p><p>significance [sɪɡˈnɪfɪkəns] n. 意义，重要性，意思</p><p>sufficient [səˈfɪʃnt] adj. 足够的，充分的</p><p>moderate [ˈmɒdərət] adj. 稳健的，温和的，适度的，中等的 v. 节制，减轻</p><p>alter [ˈɔːltə(r)] v. 改变，修改，更改</p><p>perturbation [ˌpɜːtəˈbeɪʃn] n. 摄动，不安，扰乱</p><p>reveal [rɪˈviːl] vt. 显示，透露，揭露，泄露 n. 揭露，暴露，门侧</p><p>differentiation [ˌdɪfəˌrenʃiˈeɪʃn] n. 变异，分化，区别</p><p>derivative [dɪˈrɪvətɪv] n. 派生物，导数</p><hr><h3 id="2019-10-07"><a href="#2019-10-07" class="headerlink" title="2019/10/07"></a>2019/10/07</h3><p>align [əˈlaɪn] v. 使结盟，使成一行，匹配</p><p>identical [aɪˈdentɪkl] adj. 同一的，完全相同的 n. 完全相同的事物</p><p>scenario [səˈnɑːriəʊ] n. 方案，情节，剧本，设想</p><p>subsume  [səbˈsjuːm] v. 把……归入，把……包括在内</p><p>vulnerable [ˈvʌlnərəbl] adj. 易受攻击的，易受伤害的，有弱点的</p><p>mismatch [ˌmɪsˈmætʃ] v. 使配错 n. 错配，搭配不当，不匹配</p><p>match [mætʃ] n. 火柴，比赛，匹配 v. 相配，相称，相似</p><p>simultaneous [ˌsɪmlˈteɪniəs] adj. 同时的，联立的，同时发生的</p><p>alleviate [əˈliːvieɪt] vt. 减轻，缓和</p><p>exceed [ɪkˈsiːd] v. 超过，胜过，超过其它</p><p>state-of-the-art adj. 最先进的，已经发展的，达到最高水准的</p><p>inspire [ɪnˈspaɪə(r)] v. 鼓舞，激发，启示，产生</p><p>intuit [ɪnˈtjuːɪt] v. 由直觉知道，凭直觉知道</p><p>intuitive [ɪnˈtjuːɪtɪv] adj. 直觉的，凭直觉获知的</p><p>necessary [ˈnesəsəri] adj. 必要的，必需的，必然的 n. 必需品</p><hr><h3 id="2019-10-08"><a href="#2019-10-08" class="headerlink" title="2019/10/08"></a>2019/10/08</h3><p>augment [ɔːɡˈment]  v. 增大，加大 n. 增加，增大</p><p>stochastic [stɒ’kæstɪk] adj. 随机的，猜测的</p><p>stochastic gradient descent 随机梯度下降</p><p>arbitrary [ˈɑːbɪtrəri; ˈɑːbɪtri] adj. 任意的，武断的，专制的</p><p>validate [ˈvælɪdeɪt] v. 证实，验证，确认</p><p>independently identically distribution (i.i.d) 独立同分布</p><p>drow…from… 从……中得到</p><p>indicator [ˈɪndɪkeɪtə(r)] n. 指标，标志，迹象，指示器</p><p>hypothsis [haɪˈpɒθəsɪs] n. 假设</p><p>exhibit  [ɪɡˈzɪbɪt] v. 展览，展示 n. 展览品，证据</p><p>simplicity [sɪmˈplɪsəti] n. 朴素，简易，愚蠢，天真</p><p>simple [ˈsɪmpl] adj. 简单的，单纯的，天真的 n. 笨蛋，愚蠢的行为</p><p>sufficient [səˈfɪʃnt] adj. 足够的，充分的</p><p>incentive [ɪnˈsentɪv]  n. 动机，刺激 adj. 激励的，刺激的</p><p>versatile [ˈvɜːsətaɪl] adj. 多才多艺的，通用的，万能的</p><p>leverage [ˈliːvərɪdʒ] n. 手段，影响力 v. 利用</p><p>gap [ɡæp] n. 间隙，缺口，差距，分歧 v. 裂开</p><p>invariant [ɪnˈveəriənt] adj. 不变的</p><p>disentangle [ˌdɪsɪnˈtæŋɡl] v. 解决，松开，解开，使解脱</p><p>explanatory [ɪkˈsplænətri] adj. 解释的，说明的</p><p>proper [ˈprɒpə(r)] adj. 适当的，本身的，特有的 adv. 完全地</p><p>evolution [ˌiːvəˈluːʃn; ˌevəˈluːʃn] n. 演变，进化论，进展</p><hr><h3 id="2019-10-09"><a href="#2019-10-09" class="headerlink" title="2019/10/09"></a>2019/10/09</h3><p>associated [əˈsoʊsieɪtɪd,əˈsoʊʃieɪtɪd] adj. 联合的，关联的</p><p>eliminate [ɪˈlɪmɪneɪt] v. 消除，排除</p><p>indicate [ˈɪndɪkeɪt] v. 表明，指出，预示，象征</p><hr><h3 id="2019-10-11"><a href="#2019-10-11" class="headerlink" title="2019/10/11"></a>2019/10/11</h3><p>dimensionality [dɪ,menʃə’nælətɪ]  n. 维度</p><p>dimensionality reduction 降维</p><p>compression [kəmˈpreʃn] n. 压缩，浓缩，压榨，压迫</p><p>data compression 数据压缩</p><p>terminology [ˌtɜːmɪˈnɒlədʒi]  n. 术语，用辞</p><p>algebra [ˈældʒɪbrə] n. 代数学</p><p>linear algebra 线性代数</p><p>redundant [rɪˈdʌndənt]  adj. 多余的，过剩的，冗长的，累赘的</p><h3 id="2019-10-13"><a href="#2019-10-13" class="headerlink" title="2019/10/13"></a>2019/10/13</h3><p>preceding [prɪ’siːdɪŋ] adj.  在前的，前述的</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;论文常见单词积累 (二)&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://sunfeng.online/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="paper" scheme="http://sunfeng.online/tags/paper/"/>
    
      <category term="word" scheme="http://sunfeng.online/tags/word/"/>
    
  </entry>
  
  <entry>
    <title>迁移学习简介</title>
    <link href="http://sunfeng.online/2019/09/27/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>http://sunfeng.online/2019/09/27/迁移学习简介/</id>
    <published>2019-09-27T07:55:42.000Z</published>
    <updated>2019-09-28T07:16:18.733Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">迁移学习简介</font></strong></p><a id="more"></a><h2 id="迁移学习简介"><a href="#迁移学习简介" class="headerlink" title="迁移学习简介"></a>迁移学习简介</h2><h3 id="定义与概念"><a href="#定义与概念" class="headerlink" title="定义与概念"></a>定义与概念</h3><h4 id="什么是迁移学习？"><a href="#什么是迁移学习？" class="headerlink" title="什么是迁移学习？"></a>什么是迁移学习？</h4><ul><li><p><strong>心理学角度</strong>：人们利用之前的经验和知识进行推理和学习的能力</p></li><li><p><strong>机器学习角度：</strong> 一个系统将别的相关领域中的知识应用到本应用中的学习模式</p><p><strong>举例：</strong> C++ $\rightarrow$ Java；骑自行车 $\rightarrow$ 骑摩托车</p><p><strong>关键词：</strong>举一反三</p></li></ul><hr><h4 id="迁移学习需要解决的问题"><a href="#迁移学习需要解决的问题" class="headerlink" title="迁移学习需要解决的问题"></a>迁移学习需要解决的问题</h4><ul><li>给定一个研究领域和任务，如何利用相似领域进行知识的迁移，从而达成目标？</li></ul><hr><h4 id="为什么要进行迁移学习？"><a href="#为什么要进行迁移学习？" class="headerlink" title="为什么要进行迁移学习？"></a>为什么要进行迁移学习？</h4><ul><li>数据的标签很难获取</li><li>从头建立模型是复杂和耗时的</li></ul><p><img src="/2019/09/27/迁移学习简介/figure1.PNG" alt="figure1"></p><hr><h3 id="迁移学习-VS-传统机器学习"><a href="#迁移学习-VS-传统机器学习" class="headerlink" title="迁移学习 VS 传统机器学习"></a>迁移学习 VS 传统机器学习</h3><p><img src="/2019/09/27/迁移学习简介/figure3.PNG" alt="figure3"></p><p><img src="/2019/09/27/迁移学习简介/figure2.PNG" alt="figure2"></p><hr><h3 id="与其他概念的对比"><a href="#与其他概念的对比" class="headerlink" title="与其他概念的对比"></a>与其他概念的对比</h3><ul><li><strong>终身学习</strong> (Life-long learning): 连续不断地在一个域上学习</li><li><strong>多任务学习</strong> (Multi-task learning): 两个任务同时完成</li><li><strong>域适配</strong> (Domain adaptation): 迁移学习的子类</li><li><strong>增量学习</strong> (Incremental learning): 一个域上的不断学习</li><li><strong>自我学习</strong> (Self-taught learning): 从自身数据中学习</li><li><strong>协方差漂移</strong> (Covariance shift): 迁移学习的子类</li><li>……</li></ul><p>迁移学习与其他已有概念相比，着重强调学习任务之间的<strong>相关性</strong>，并利用这种相关性完成知识之间的迁移。</p><hr><h4 id="迁移学习的形式化概念"><a href="#迁移学习的形式化概念" class="headerlink" title="迁移学习的形式化概念"></a>迁移学习的形式化概念</h4><h5 id="迁移学习常用概念"><a href="#迁移学习常用概念" class="headerlink" title="迁移学习常用概念"></a>迁移学习常用概念</h5><ul><li><strong>Domain (域)</strong>：由数据特征和特征分布组成，是学习的主体<ul><li>Source domain (源域)：已有知识的域</li><li>Target domain (目标域)：要进行学习的域</li></ul></li><li><strong>Task (任务)</strong> ：由目标函数和学习结果组成，是学习的结果</li></ul><h5 id="迁移学习的形式化定义"><a href="#迁移学习的形式化定义" class="headerlink" title="迁移学习的形式化定义"></a>迁移学习的形式化定义</h5><ul><li><strong>条件</strong>：给定一个源域 ${\cal D}_S$ 和源域上的学习任务 ${\cal T}_{S}$, 以及目标域 ${\cal D}_T$ 和目标域上的学习任务 ${\cal T}_T$</li><li><strong>目标</strong>：利用 ${\cal D}_S$ 和 ${\cal T}_S$ 学习在目标域上的预测函数 $f(\cdot)$</li><li><strong>限制条件</strong>：${\cal D}_S \ne {\cal D}_T$ 或 ${\cal T}_S \ne {\cal T}_T$</li></ul><hr><h4 id="迁移学习的应用领域"><a href="#迁移学习的应用领域" class="headerlink" title="迁移学习的应用领域"></a>迁移学习的应用领域</h4><p><img src="/2019/09/27/迁移学习简介/figure4.PNG" alt="figure4"></p><hr><h2 id="迁移学习的分类方法"><a href="#迁移学习的分类方法" class="headerlink" title="迁移学习的分类方法"></a>迁移学习的分类方法</h2><p><strong>常用的迁移学习分类方法</strong></p><ul><li>按迁移情景</li><li>按迁移方法</li><li>按特征空间</li></ul><h3 id="按迁移情景分类"><a href="#按迁移情景分类" class="headerlink" title="按迁移情景分类"></a>按迁移情景分类</h3><ul><li><strong>归纳式迁移学习</strong> (inductive transfer learning)：源域和目标域的学习任务不同</li><li><strong>直推式迁移学习</strong> (transductive transfer learning)：源域和目标域不同，学习任务相同</li><li><strong>无监督迁移学习</strong> (unsupervised transfer learning)：源域和目标域均没有标签</li></ul><p><img src="/2019/09/27/迁移学习简介/figure5.PNG" alt="figure5"></p><hr><h3 id="按特征空间分类"><a href="#按特征空间分类" class="headerlink" title="按特征空间分类"></a>按特征空间分类</h3><ul><li><strong>同构迁移学习</strong> (Homogeneous TL)：特征维度相同分布不同</li><li><strong>异构迁移学习</strong> (Heterogeneous TL)：特征维度不同</li></ul><p><img src="/2019/09/27/迁移学习简介/figure7.PNG" alt="figure7"></p><hr><h3 id="按迁移方法分类"><a href="#按迁移方法分类" class="headerlink" title="按迁移方法分类"></a>按迁移方法分类</h3><ul><li><strong>基于实例的迁移</strong> (instance based TL)：通过权重重用源域和目标域的样例进行迁移</li><li><strong>基于特征的迁移</strong> (feature based TL)：将源域和目标域的特征变换到相同空间</li><li><strong>基于模型的迁移</strong> (parameter based TL)：利用源域和目标域的参数共享模型</li><li><strong>基于关系的迁移</strong> (relation based TL)：利用源域中的逻辑网络关系进行迁移</li></ul><p><img src="/2019/09/27/迁移学习简介/figure6.PNG" alt="figure6"></p><hr><h4 id="基于实例的迁移学习方法"><a href="#基于实例的迁移学习方法" class="headerlink" title="基于实例的迁移学习方法"></a>基于实例的迁移学习方法</h4><ul><li><p><strong>假设</strong>：源域中的一些数据和目标域会共享很多共同的特征</p></li><li><p><strong>方法</strong>：对源域进行 instance reweighting, 筛选出与目标域数据相似度高的数据，然后进行训练学习</p></li><li><p><strong>代表工作</strong>：</p><ul><li>TrAdaBoost [Dal, ICML-07]</li><li>Kemel Mean Matching (KMM) [Smola, ICML-08]</li><li>Density ratio estimation [Suglyama, NIPS-07]</li></ul></li><li><p><strong>优点</strong>:</p><ul><li>方法较简单，实现容易</li></ul></li><li><p><strong>缺点</strong>:</p><ul><li>权重选择与相似度度量依赖经验</li><li>源域和目标域的数据分布往往不同</li></ul></li><li><p>图示：</p><p><img src="/2019/09/27/迁移学习简介/figure8.PNG" alt="figure8"></p></li></ul><hr><h4 id="基于特征的迁移学习方法"><a href="#基于特征的迁移学习方法" class="headerlink" title="基于特征的迁移学习方法"></a>基于特征的迁移学习方法</h4><ul><li><p><strong>假设：</strong>源域和目标域仅仅有一些交叉特征</p></li><li><p><strong>方法：</strong>通过特征变化，将两个域的数据变化到同一特征空间，然后进行传统的机器学习</p></li><li><p><strong>代表工作：</strong></p><ul><li>Transfer component analysis (TCA) [Pan, TKDE-11]</li><li>Spectral Feature Alignment (SFA) [Pan, WWW-10]</li><li>Geodesic flow kernel (GFK) [Duan, CVPR-12]</li><li>Transfer kernel learning (TKL) [Long, TKDE-15]</li></ul></li><li><p><strong>优点：</strong></p><ul><li>大多数方法采用</li><li>特征选择与变换可以取得好效果</li></ul></li><li><p><strong>缺点：</strong></p><ul><li>往往是一个优化问题，难求解</li><li>容易发生过适配</li></ul></li><li><p>图示</p><p><img src="/2019/09/27/迁移学习简介/figure9.PNG" alt="figure9"></p></li></ul><hr><h4 id="基于模型的迁移学习方法"><a href="#基于模型的迁移学习方法" class="headerlink" title="基于模型的迁移学习方法"></a>基于模型的迁移学习方法</h4><ul><li><strong>假设：</strong> 源域和目标域可以共享一些模型参数</li><li><strong>方法：</strong> 由源域学习到的模型运用到目标域上，再根据目标域学习新的模型</li><li><strong>代表工作：</strong><ul><li>TransEMDT [Zhao, IJCAI-11]</li><li>TRCNN [Oquab, CVPR-14]</li><li>TaskTrAdaBoost [Yao, CVPR-10]</li></ul></li><li><strong>优点：</strong><ul><li>模型间存在相似性，可以被利用</li></ul></li><li><strong>缺点：</strong><ul><li>模型参数不易收敛</li></ul></li></ul><hr><h4 id="基于关系的迁移学习方法"><a href="#基于关系的迁移学习方法" class="headerlink" title="基于关系的迁移学习方法"></a>基于关系的迁移学习方法</h4><ul><li><p><strong>假设：</strong>如果两个域是相似的，那么它们会共享某种相似关系</p></li><li><p><strong>方法：</strong>利用源域学习逻辑关系网络，再应用到目标域上</p></li><li><p><strong>代表工作：</strong></p><ul><li>Predicates mapping and revising [Mihalkova, AAAI-07]</li><li>Second-order Markov Logic [Davis, ICML-09]</li></ul></li><li><p>图示：</p><p><img src="/2019/09/27/迁移学习简介/figure10.PNG" alt="figure10"></p></li></ul><hr><h2 id="代表性研究工作"><a href="#代表性研究工作" class="headerlink" title="代表性研究工作"></a>代表性研究工作</h2><h3 id="迁移学习的热门研究领域"><a href="#迁移学习的热门研究领域" class="headerlink" title="迁移学习的热门研究领域"></a>迁移学习的热门研究领域</h3><ul><li><strong>域适配问题</strong> (domain adaptation)</li><li><strong>多源迁移学习</strong> (multi-source TL)</li><li><strong>深度迁移学习</strong> (deep TL)</li><li><strong>异构迁移学习</strong> (heterogeneous TL)</li></ul><p><img src="/2019/09/27/迁移学习简介/figure11.PNG" alt="figure11"></p><hr><h3 id="域适配问题-Domain-adaptation"><a href="#域适配问题-Domain-adaptation" class="headerlink" title="域适配问题 (Domain adaptation)"></a>域适配问题 (Domain adaptation)</h3><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><ul><li><p>有标签的源域和无标签的目标域共享相同的特征和模型，但是特征分布不同，如何利用源域标定目标域</p><p>${\cal D}_S \ne {\cal D}_T$ : $P_S(X) \ne P_T(X)$</p></li></ul><h4 id="域适配问题的迁移方法"><a href="#域适配问题的迁移方法" class="headerlink" title="域适配问题的迁移方法"></a>域适配问题的迁移方法</h4><ul><li><p><strong>基于特征的迁移方法：</strong></p><ul><li><p>Transfer component analysis [Pan, TKDE-11]</p></li><li><p>Geodesic flow kernel [Duan, CVPR-12]</p></li><li><p>Transfer kernel learning [Long, TKDE-15]</p></li><li><p>TransEMDT [Zhao, IJCAI-11]</p></li></ul></li></ul><ul><li><p><strong>基于实例的迁移方法：</strong></p><ul><li><p>Kernel mean matching [Huang, NIPS-06]</p></li><li><p>Covariate Shift Adaptation [Sugiyama, JMLR-07]</p></li></ul></li></ul><ul><li><p><strong>基于模型的迁移方法：</strong></p><ul><li>Adaptive SVM (ASVM) [Yang et al, ACM Multimedia-07]</li><li>Multiple Convex Combination (MCC) [Schweikert, NIPS-09]</li><li>Domain Adaptation Machine (DAM) [Duan, TNNLS-12]</li></ul></li></ul><h4 id="迁移成分分析-TCA-transfer-component-analysis-Pan-TKDE-11"><a href="#迁移成分分析-TCA-transfer-component-analysis-Pan-TKDE-11" class="headerlink" title="迁移成分分析 (TCA, transfer component analysis) [Pan, TKDE-11]"></a>迁移成分分析 (TCA, transfer component analysis) [Pan, TKDE-11]</h4><ul><li>将源域和目标域变换到相同空间，最小化它们的距离</li></ul><p><img src="/2019/09/27/迁移学习简介/figure12.PNG" alt="figure12"></p><ul><li><p>优化目标</p><script type="math/tex; mode=display">\begin {align}&\mathop \min_{\varphi} \ \ {\text {Dist}}(\varphi(\boldsymbol X_S),\varphi(\boldsymbol X_T))+\lambda\Omega(\varphi)\\&{\text {s.t.}} \ \ {\text {constraints on}} \ \varphi(\boldsymbol X_S) \ {\text {and}}\ \varphi(\boldsymbol X_S) \end {align}</script></li><li><p>Maximum mean discrepancy (MMD)</p><script type="math/tex; mode=display">{\text {Dist}}(P(\boldsymbol X_S),P(\boldsymbol X_T)) = \left\|\frac{1}{n_S}\sum_{i=1}^{n_S}\Phi(x_{S_i}) -\frac{1}{n_T}\sum_{i=1}^{n_T}\Phi(x_{T_i}) \right\|_{\cal H}</script></li><li><p>Original feature</p><p><img src="/2019/09/27/迁移学习简介/figure13.PNG" alt="figure13"></p></li><li><p>PCA</p><p><img src="/2019/09/27/迁移学习简介/figure14.PNG" alt="figure14"></p></li><li><p>TCA</p><p><img src="/2019/09/27/迁移学习简介/figure15.PNG" alt="figure15"></p></li></ul><h4 id="GFK-geodesic-flow-kernel-Duan-CVPR-12"><a href="#GFK-geodesic-flow-kernel-Duan-CVPR-12" class="headerlink" title="GFK (geodesic flow kernel) [Duan, CVPR-12]"></a>GFK (geodesic flow kernel) [Duan, CVPR-12]</h4><ul><li><p>利用流行学习，将数据映射到高维空间中，然后测量其距离，使得源域和目标域差异最大</p></li><li><p><strong>优化目标：</strong></p><script type="math/tex; mode=display">\begin {align}&\Phi(t) = P_SU_1\Gamma(t)-R_sU_2\Sigma(t)\\&P_S^TP_T = U_1\Gamma V^T,\ \ R_S^TP_T = -U_2\Sigma V^T\end {align}</script></li><li><p><strong>流形正则项：</strong></p><script type="math/tex; mode=display">{\cal R}({\cal S},{\cal T}) = \frac{1}{d^*}\sum_{i}^{d^*}\theta_t\left|KL({\cal S}_i||{\cal T}_i)+KL({\cal T}_i||{\cal S}_i) \right|</script><p><img src="/2019/09/27/迁移学习简介/figure16.PNG" alt="figure16"></p></li></ul><h4 id="Transfer-Kernel-Learning-TKL-Long-TKDE-15"><a href="#Transfer-Kernel-Learning-TKL-Long-TKDE-15" class="headerlink" title="Transfer Kernel Learning (TKL) [Long, TKDE-15]"></a>Transfer Kernel Learning (TKL) [Long, TKDE-15]</h4><ul><li><p>在再生核希尔伯特空间中学习一个领域不变核矩阵，从而实现源域和目标域的适配</p></li><li><p>优化目标：</p><script type="math/tex; mode=display">\begin {align}\mathop \min_{A}\left\|\overline{\boldsymbol K}_Z-{\boldsymbol K}_Z \right\|_F^2 &=  \left\|\overline{\boldsymbol \Phi }_Z\Lambda \overline{\boldsymbol \Phi }_Z^T -{\boldsymbol K}_Z \right\|_F^2\\\lambda_i \ge \zeta\lambda_{i+1},i &= 1,...,n-1\\\lambda_i \ge 0, i&=1,...,n\end {align}</script></li></ul><p><img src="/2019/09/27/迁移学习简介/figure17.PNG" alt="figure17"></p><h4 id="嵌入决策树算法-TransEMDT-Zhao-IJCAI-11"><a href="#嵌入决策树算法-TransEMDT-Zhao-IJCAI-11" class="headerlink" title="嵌入决策树算法(TransEMDT) [Zhao, IJCAI-11]"></a>嵌入决策树算法(TransEMDT) [Zhao, IJCAI-11]</h4><ul><li>首先通过聚类得到初始的目标域决策树模型，然后迭代更新决策树的参数直到收敛为止</li></ul><p><img src="/2019/09/27/迁移学习简介/figure18.PNG" alt="figure18"></p><p><img src="/2019/09/27/迁移学习简介/figure19.PNG" alt="figure19"></p><p><img src="/2019/09/27/迁移学习简介/figure20.PNG" alt="figure20"></p><h4 id="Kernel-mean-matching-Huang-NIPS-06"><a href="#Kernel-mean-matching-Huang-NIPS-06" class="headerlink" title="Kernel mean matching [Huang, NIPS-06]"></a>Kernel mean matching [Huang, NIPS-06]</h4><ul><li><p>在再生核希尔伯特空间中计算源域和目标域的协方差分布差异，然后用二次规划求解样本权重</p></li><li><p>优化目标：</p><script type="math/tex; mode=display">\begin {align}&\mathop\min_{\alpha}\sum_{i=1}^{n_{tr}}\beta_ig(\alpha|x_i^{tr})\\&-\sum_{i,j=1;y\in{\cal Y}}^{n_{tr}}\alpha_{iy}\beta_jk(x_i^{tr},y,x_j^{tr},y_j^{tr} )\\&+ \sum_{i,j=1;y,y\prime \in {\cal Y}}\alpha_{iy}\alpha_{jy\prime}k(x_i^{tr},y,x_j^{tr},y_\prime)\\&{\text where}\ \ g(\alpha|x_i^{tr}):= \log \sum_{y\in{\cal Y}}\exp\left(\sum_{j=1;y\prime \in {\cal Y}}^{n_{tr}}\alpha_{jy\prime}k(x_i^{tr},y,x_j^{tr},y^{\prime}) \right)\end {align}</script></li></ul><p><img src="/2019/09/27/迁移学习简介/figure21.PNG" alt="figure21"></p><h4 id="Covariate-Shift-Adaptation-Sugiyama-JMLR-07"><a href="#Covariate-Shift-Adaptation-Sugiyama-JMLR-07" class="headerlink" title="Covariate Shift Adaptation [Sugiyama, JMLR-07]"></a>Covariate Shift Adaptation [Sugiyama, JMLR-07]</h4><ul><li><p>采用自然估计法估计源域和目标域的密度比例，然后进行实例权重分配，最后迁移</p></li><li><p>优化目标：</p><script type="math/tex; mode=display">\mathop \max_{\{\alpha_l\}_{l=1}^b}\left[\sum_{j=1}^{n_{te}} {\log}\left(\sum_{l=1}^b\alpha_l \varphi_l(\boldsymbol x_j^{te}) \right) \right]\\{\text subject\ \ to\ }\sum_{i=1}^{n_{tr}}\sum_{l=1}^{b}\alpha_l\varphi_l(x_i^{tr}) = n_{tr}\ and\ \alpha_1, \alpha_2,...,\alpha_b \ge 0.</script></li></ul><p><img src="/2019/09/27/迁移学习简介/figure22.PNG" alt="figure22"></p><h4 id="Adaptive-SVM-ASVM-Yang-et-al-ACM-Multimedia-07"><a href="#Adaptive-SVM-ASVM-Yang-et-al-ACM-Multimedia-07" class="headerlink" title="Adaptive SVM (ASVM) [Yang et al, ACM Multimedia-07]"></a>Adaptive SVM (ASVM) [Yang et al, ACM Multimedia-07]</h4><ul><li><p>使用SVM模型，在适配和原模型之间学习“数据函数”，达到了优化模型迁移效果</p></li><li><p>优化目标：</p><script type="math/tex; mode=display">\mathop \min_w \frac{1}{2}\left\|w \right\|^2+c\sum_{i=1}^{N}\xi_i\\s.t \ \ \xi_i\ge0,y_i\sum_{M}t_kf_k^{n}(x_i)+y_iw^T\phi(x_i)\ge1-\xi_i</script></li></ul><h4 id="Multiple-Convex-Combination-MCC-Schweikert-NIPS-09"><a href="#Multiple-Convex-Combination-MCC-Schweikert-NIPS-09" class="headerlink" title="Multiple Convex Combination (MCC) [Schweikert, NIPS-09]"></a>Multiple Convex Combination (MCC) [Schweikert, NIPS-09]</h4><ul><li><p>对一些域适配的方法做集成学习</p></li><li><p>优化目标</p><script type="math/tex; mode=display">F(x) = \alpha f_T(x)+(1-\alpha)\frac{1}{|\cal S|}\sum_{S\in{\cal S}}f_S(x)</script></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>通常假设源域和目标域的数据有着相同的条件分布，或者在 高维空间里，有着相同的条件分布 </li><li>这个假设是有一定局限性的，无法衡量源域和目标域之间相 似性，可能发生负迁移</li></ul><hr><h3 id="多源迁移学习"><a href="#多源迁移学习" class="headerlink" title="多源迁移学习"></a>多源迁移学习</h3><h4 id="问题定义-1"><a href="#问题定义-1" class="headerlink" title="问题定义"></a>问题定义</h4><ul><li>多个源域和目标域，如何进行有效的域筛选，从而进行迁移？</li></ul><p><img src="/2019/09/27/迁移学习简介/figure23.PNG" alt="figure23"></p><h4 id="多源迁移学习的方法"><a href="#多源迁移学习的方法" class="headerlink" title="多源迁移学习的方法"></a>多源迁移学习的方法</h4><ul><li>TrAdaBoost[Dai, ICML-07]</li><li>MsTL-MvAdaboost[Xu, ICONIP-12]</li><li>Consensus regularization [Luo, CIKM-08] </li><li>Transitive transfer learning [Tan, KDD-15] </li><li>Distant domain TL [Tan, AAAI-17]</li></ul><h4 id="TrAdaBoost-Dai-ICML-07"><a href="#TrAdaBoost-Dai-ICML-07" class="headerlink" title="TrAdaBoost[Dai, ICML-07]"></a>TrAdaBoost[Dai, ICML-07]</h4><ul><li>利用Boost的技术过滤掉多个源域中与目标域不相似的样本 ，然后进行实例迁移学习</li></ul><p><img src="/2019/09/27/迁移学习简介/figure24.PNG" alt="figure24"></p><p><img src="/2019/09/27/迁移学习简介/figure25.PNG" alt="figure25"></p><h4 id="MsTL-MvAdaboost-Xu-ICONIP-12"><a href="#MsTL-MvAdaboost-Xu-ICONIP-12" class="headerlink" title="MsTL-MvAdaboost [Xu, ICONIP-12]"></a>MsTL-MvAdaboost [Xu, ICONIP-12]</h4><ul><li>不仅考虑源域和目标域的样本相似度情况，同时，以多视图学习的目标来进行统一的迁移 </li></ul><h4 id="Consensus-regularization-Luo-CIKM-08"><a href="#Consensus-regularization-Luo-CIKM-08" class="headerlink" title="Consensus regularization [Luo, CIKM-08]"></a>Consensus regularization [Luo, CIKM-08]</h4><ul><li><p>同时在源域和伪标注的目标域上训练分类器，利用一致性约束进行知识的迁移</p></li><li><p>优化目标：</p><script type="math/tex; mode=display">\begin {align}\max P(h^l|{\cal D_s^l}) &= \max \frac{P(D^l_s|h^l)P(h^l)}\\{P({\cal D_s^l})} \\&= \max P(D^l_s|h^l)P(h^l) \\&= \max P(h^l)\cdot \prod_{i=1}^{n^l}P(y_i^l|x_i;h^l)\\&= \max (\log P(h^l)+\sum_{i=1}^{n^l}\log P(y_i^l|x_i;h^l))\end {align}</script></li></ul><h4 id="Transitive-transfer-learning-Tan-KDD-15"><a href="#Transitive-transfer-learning-Tan-KDD-15" class="headerlink" title="Transitive transfer learning [Tan, KDD-15]"></a>Transitive transfer learning [Tan, KDD-15]</h4><ul><li>在两个相似度不高的域中，利用从第三方中学习到的相似度关系，完成知识的传递迁移</li></ul><p><img src="/2019/09/27/迁移学习简介/figure26.PNG" alt="figure26"></p><h4 id="Distant-domain-TL-Tan-AAAI-17"><a href="#Distant-domain-TL-Tan-AAAI-17" class="headerlink" title="Distant domain TL [Tan, AAAI-17]"></a>Distant domain TL [Tan, AAAI-17]</h4><ul><li><p>在相似度极低的两个域进行迁移时，用autoencoder自动从 多个中间辅助域中选择知识，完成迁移</p><p><img src="/2019/09/27/迁移学习简介/figure27.PNG" alt="figure27"></p></li></ul><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul><li>多源迁移学习可以有效利用存在的多个可用域，综合起来进 行迁移，达到较好的效果 </li><li>如何衡量多个域之间的相关性还是一个问题 </li><li>对多个域的利用方法也存在一定挑战性</li></ul><hr><h3 id="深度迁移学习"><a href="#深度迁移学习" class="headerlink" title="深度迁移学习"></a>深度迁移学习</h3><h4 id="问题定义-2"><a href="#问题定义-2" class="headerlink" title="问题定义"></a>问题定义</h4><ul><li>利用深度神经网络的结构进行迁移学习</li><li>神经网络学习非线性的特征表示</li><li>层次性</li><li>使得数据具有不可解释性</li><li>表明在数据中具有某些不可变成分，可以用来迁移</li></ul><h4 id="代表方法"><a href="#代表方法" class="headerlink" title="代表方法"></a>代表方法</h4><ul><li>Joint CNN [Tzeng, ICCV-15] </li><li>SHL-MDNN [Huang, ICASSP-13] </li><li>Deep Adaptation Network (DAN) [Long, ICML-15] </li><li>Joint Adaptation Networks [Long, CVPR-13]</li></ul><h4 id="Joint-CNN-Tzeng-ICCV-15"><a href="#Joint-CNN-Tzeng-ICCV-15" class="headerlink" title="Joint CNN [Tzeng, ICCV-15]"></a>Joint CNN [Tzeng, ICCV-15]</h4><ul><li>针对有稀疏标记的目标域数据，用CNN同时优化域之间的 距离和迁移学习任务的损失<script type="math/tex; mode=display">\begin {align}{\cal L}(x_S,y_S,x_T,y_T,\theta_D;\theta_{repr},\theta_C)\\ &= {\cal L}_C(x_S,y_S,x_T,y_T,\theta_D;\theta_{repr},\theta_C)\\&+ \lambda{\cal L}_{cont}(x_T,y_T,\theta_D;\theta_{repr})\\&+ v{\cal L}_{soft}(x_T,y_T,\theta_D;\theta_{repr},\theta_C)\end {align}</script><img src="/2019/09/27/迁移学习简介/figure28.PNG" alt="figure28"></li></ul><h4 id="SHL-MDNN-Huang-ICASSP-13"><a href="#SHL-MDNN-Huang-ICASSP-13" class="headerlink" title="SHL-MDNN [Huang, ICASSP-13]"></a>SHL-MDNN [Huang, ICASSP-13]</h4><ul><li>在不同的学习网络之间共享隐藏层，通过不同的softmax层 控制学习任务的不同</li></ul><p><img src="/2019/09/27/迁移学习简介/figure29.PNG" alt="figure29"></p><h4 id="Deep-Adaptation-Network-DAN-Long-ICML15"><a href="#Deep-Adaptation-Network-DAN-Long-ICML15" class="headerlink" title="Deep Adaptation Network (DAN) [Long, ICML15]"></a>Deep Adaptation Network (DAN) [Long, ICML15]</h4><ul><li><p>将CNN中与学习任务相关的隐藏层映射到再生核希尔伯特 空间中，通过多核优化的方法最小化不同域之间的距离</p><p><img src="/2019/09/27/迁移学习简介/figure30.PNG" alt="figure30"></p></li></ul><h4 id="Joint-Adaptation-Networks-Long-CVPR-15"><a href="#Joint-Adaptation-Networks-Long-CVPR-15" class="headerlink" title="Joint Adaptation Networks [Long, CVPR-15]"></a>Joint Adaptation Networks [Long, CVPR-15]</h4><ul><li>提出一种新的联合分布距离度量关系，利用这种关系泛化深 度模型的迁移学习能力，从而适配不同领域的数据分布。基 于AlexNet和GoogLeNet重新优化了网络结构</li></ul><p><img src="/2019/09/27/迁移学习简介/figure31.PNG" alt="figure31"></p><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><ul><li>迁移学习大增强了模型的泛化能力</li><li>深度学习可以深度表征域中的知识结构</li><li>深度学习+迁移学习还可大有作为</li></ul><p><img src="/2019/09/27/迁移学习简介/figure32.PNG" alt="figure32"></p><h2 id="问题与展望"><a href="#问题与展望" class="headerlink" title="问题与展望"></a>问题与展望</h2><h3 id="迁移学习存在的问题"><a href="#迁移学习存在的问题" class="headerlink" title="迁移学习存在的问题"></a>迁移学习存在的问题</h3><ul><li><strong>负迁移：</strong>无法判断域之间的相关性，导致负迁移</li><li>缺乏理论支撑：尚未有统一的迁移学习理论</li><li>相似度衡量：域之间的相似度通常依赖经验进行衡量，缺乏统一有效的相似度衡量方法</li></ul><h3 id="已有基础"><a href="#已有基础" class="headerlink" title="已有基础"></a>已有基础</h3><ul><li>负迁移：利用自编码器实现相关度较低的两个域之间的迁移</li><li>理论支撑：利用物理学定律为迁移找到理论保证</li><li>相似度衡量：提出迁移度量学习，寻找行为之间相关性最高的域进行迁移</li></ul><hr><h2 id="迁移学习资源"><a href="#迁移学习资源" class="headerlink" title="迁移学习资源"></a>迁移学习资源</h2><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><ul><li>A survey on transfer learning [Pan and Yang, TKDE-10] </li><li>A survey of transfer learning [Weiss, Big data-15] </li></ul><h3 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a>开源项目</h3><ul><li><a href="http://www.cse.ust.hk/TL/" target="_blank" rel="noopener">http://www.cse.ust.hk/TL/</a></li></ul><h3 id="研究学者"><a href="#研究学者" class="headerlink" title="研究学者"></a>研究学者</h3><ul><li>QiangYang @ HKUST: <a href="http://www.cs.ust.hk/~qyang/" target="_blank" rel="noopener">http://www.cs.ust.hk/~qyang/</a> </li><li>SinnoJialinPan @ NTU: <a href="http://www.ntu.edu.sg/home/sinnopan/" target="_blank" rel="noopener">http://www.ntu.edu.sg/home/sinnopan/</a> </li><li>FuzhenZhuang @ ICT CAS: <a href="http://www.intsci.ac.cn/users/zhuangfuzhen/" target="_blank" rel="noopener">http://www.intsci.ac.cn/users/zhuangfuzhen/</a> </li><li>MingshengLong @ THU: <a href="http://ise.thss.tsinghua.edu.cn/~mlong/" target="_blank" rel="noopener">http://ise.thss.tsinghua.edu.cn/~mlong/</a> </li><li>LixinDuan@ Amazon: <a href="http://www.lxduan.info/" target="_blank" rel="noopener">http://www.lxduan.info/</a> </li></ul><h3 id="会议、期刊"><a href="#会议、期刊" class="headerlink" title="会议、期刊"></a>会议、期刊</h3><ul><li><strong>人工智能、机器学习：</strong>AAAI, ICML, ICJAI, NIPS, TNNLS, TIST, CVPR</li><li><strong>数据挖掘：</strong>TKDE，SIGKDD，ACL，WWW, SIGIR</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;迁移学习简介&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="迁移学习" scheme="http://sunfeng.online/categories/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="transfer learning" scheme="http://sunfeng.online/tags/transfer-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文常见单词积累 (一)</title>
    <link href="http://sunfeng.online/2019/09/20/%E8%AE%BA%E6%96%87%E5%B8%B8%E8%A7%81%E5%8D%95%E8%AF%8D%E7%A7%AF%E7%B4%AF%20(%E4%B8%80)/"/>
    <id>http://sunfeng.online/2019/09/20/论文常见单词积累 (一)/</id>
    <published>2019-09-20T06:21:32.000Z</published>
    <updated>2019-09-25T13:54:42.654Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">论文常见单词积累 (一)</font></strong></p><a id="more"></a><h3 id="2019-09-18"><a href="#2019-09-18" class="headerlink" title="2019/09/18"></a>2019/09/18</h3><p>representative [reprɪ’zentətɪv] adj. 典型的，有代表性的</p><p>representativeness n. 代表性，典型性</p><p>informative [ɪn’fɔːmətɪv] adj. 提供有用信息的，教育性的</p><p>informativeness n. 信息量，忠实度</p><p>collaborative [kə’læbəretɪv] adj. 合作的，协作的</p><p>distinctiveness [di’stiŋktivnis] n. 特殊性，区别性，辨别性</p><p>distinct [dɪ’stɪŋ(k)t]  adj. 明显的，独特的，清楚的，有区别的</p><p>distinguish [dɪ’stɪŋgwɪʃ] vt. 区分，辨别，使杰出</p><p>distinguished [dɪ’stɪŋgwɪʃt] adj. 卓越的，著名的，高贵的</p><p>distinguishing [dɪ’stɪŋgwɪʃɪŋ] adj. 有区别的</p><p>distinguishable [dɪ’stɪŋgwɪʃəbl] adj. 可区分的</p><p>segment [‘segm(ə)nt] n. 段，节；v. 分割</p><p>segmentation [,seɡmən’teɪʃən] n. 分割，割断，细胞分裂</p><p>critical [‘krɪtɪk(ə)l]  adj. 关键的，决定性的，批评的，临界的</p><p>alleviate [ə’liːvɪeɪt] vt. 减轻，缓和</p><p>specifically [spɪ’sɪfɪkəlɪ] adv. 特别地，明确地</p><p>specific [spɪ’sɪfɪk] adj. 特殊的，特定的，明确的，详细的 n. 特性，细节</p><p>compact [kəm’pækt] adj. 紧凑的，坚实的，简洁的</p><p>consistent [kən’sɪst(ə)nt] adj, 一致的，始终如一的</p><p>batch [bætʃ] n. 一批 vt. 分批处理</p><p>strategy [ˈstrætədʒɪ] n. 战略，策略</p><p>extract [ˈekstrækt; ɪkˈstrækt] v. 提取，选取</p><p>ensemble [ɒn’sɒmb(ə)l]  n. 全体，总效果 adv. 同时</p><p>available [ə’veɪləb(ə)l] adj. 可获得的，可购得的，可找到的，有空的</p><p>comparable [‘kɒmp(ə)rəb(ə)l] adj. 类似的，可比较的，同等的，相当的</p><p>feasible [‘fiːzɪb(ə)l] adj. 可行的，可实施的，可实行的</p><p>excessive  [ɪkˈsesɪv] adj. 过多的，极度的，过分的</p><p>geometric [,dʒɪə’metrɪk] adj. 几何学的，</p><p>morphology [mɔː’fɒlədʒɪ] adj. 形态学，形态论</p><p>refinement [rɪ’faɪnm(ə)nt] n. 改进，改善，有教养，文雅</p><p>scheme [skiːm] n. 计划，组合，体制</p><p>optimal [‘ɒptɪm(ə)l] adj. 最佳的，最理想的</p><p>optimally [‘ɒptɪm(ə)lɪ] adv. 最佳，最适宜</p><p>optimize [‘ɒptɪmaɪz] v. 最优化，优化</p><p>optimization [,ɒptɪmaɪ’zeɪʃən] n. 最佳化，最优化</p><p>circular [‘sɜːkjʊlə]  adj. 循环的，圆形的 </p><p>regular [ˈreɡjələr] adj. 定期的，有规律的，合格的</p><p>regularize [‘regjʊləraɪz] vt. 调整，使有秩序，是合法化, 正则化</p><p>regularization [,rɛɡjʊlərɪ’zeʃən] n. 规则化，调整，合法化</p><p>underlying [ʌndə’laɪɪŋ] adj. 潜在的；根本的；在下面的；优先的</p><p>potential [pəˈtenʃl] adj. 潜在的，可能的 </p><p>hierarchical [ˌhaɪəˈrɑːkɪ.kəl] adj. 分层的，分等级体系的</p><hr><h3 id="2019-9-19"><a href="#2019-9-19" class="headerlink" title="2019/9/19"></a>2019/9/19</h3><p>precise [prɪ’saɪs] adj. 精确的，明确的，严格的</p><p>precisely [prɪ’saɪsIɪ] adv. 精确地，恰恰</p><p>precision [prɪ’sɪʒ(ə)n] n. 精度，精密度，精确</p><p>situation [sɪtjʊ’eɪʃ(ə)n] n. 情况，形式，处境，位置</p><p>interactive [ɪntər’æktɪv] adj. 交互式的，相互作用的</p><p>interact [ɪntər’ækt]  v.相互影响，相互作用</p><p>criterion [kraɪ’tɪərɪən] n.标准，准则，规范 (单数)</p><p>criteria [kraɪ’tɪərɪə] n. 标准，条件 (复数)</p><p>committee [kə’mɪtɪ] n. 委员会</p><p>overall [‘əʊvərɔːl] adj. 全部的，全体的，一切在内的 adv. 全部地，总得来说</p><p>aggregate [‘ægrɪgət; (for v.) ˈægrɪgeɪt] n. 合计，集合体；v. 集合，聚集，合计 </p><p>represent [reprɪ’zent] v. 代表，表现</p><p>representation [,reprɪzen’teɪʃ(ə)n] n. 代表，表现</p><p>representative [reprɪ’zentətɪv] adj. 典型的，有代表性的 n. 代表，典型</p><p>representativeness n. 代表性，典型性</p><p>neglect [nɪ’glekt] v. 忽略，忽视 n. 忽略，忽视，未被重视</p><p>exploit [ˈeksplɔɪt;ɪkˈsplɔɪt] vt. 开发，开拓，剥削</p><p>abundance [ə’bʌnd(ə)ns] n. 充裕，丰富</p><p>abundant [ə’bʌnd(ə)nt] adj. 丰富的，充裕的</p><p>solely [‘səʊllɪ] adv. 单独地，仅仅</p><p>outperform [aʊtpə’fɔːm] vt. 胜过，做得比……好</p><p>outperformance [,autpə’fɔ:məns] n. 优胜，业绩出色</p><p>material [mə’tɪərɪəl] n. 材料，用具，人才，素材; adj. 物质的，身体需要的</p><p>schematic [skiː’mætɪk; skɪ-] adj. 图解的，概要的 n. 原理图</p><p>diagram [‘daɪəgræm] n. 图表，图解</p><p>sensitive [‘sensɪtɪv] adj. 敏感的，感觉的，灵敏的 n. 敏感的人</p><p>interval [‘ɪntəv(ə)l] n. 间隔，间距</p><p>connectivity [kɒnek’tɪvɪtɪ] n. 连通 (性)，联结 (度)</p><p>appropriate [əˈprəʊprɪət;(for v.)əˈprəʊprɪeɪt] adj. 适当的，合适的 vt. 占用</p><p>appropriately [əˈprəʊpriətli] adv. 适当地，合适地</p><p>discriminate [dɪ’skrɪmɪneɪt] vt. 歧视，区别，辨别 vi. 区别</p><p>discrimination [dɪ,skrɪmɪ’neɪʃ(ə)n] n. 歧视，区别，辨别，识别力</p><hr><h3 id="2019-09-20"><a href="#2019-09-20" class="headerlink" title="2019/09/20"></a>2019/09/20</h3><p>intuitive [ɪn’tjuːɪtɪv] adj. 直觉的，凭直觉的</p><p>intuitively [ɪn’tjʊɪtɪvli] adv. 直观地，直觉地</p><p>intuition [ɪntjʊ’ɪʃ(ə)n] n. 直觉，直觉力，直觉的知识</p><p>foreground [‘fɔːgraʊnd] n. 前景，最显著的位置</p><p>background [‘bækgraʊnd] n. 背景，隐蔽的位置；v. 作……的背景 adj. 背景的</p><p>general [‘dʒen(ə)r(ə)l] adj. 一般的，普通的，综合的，大体的 n. 一般；将军</p><p>generally [‘dʒen(ə)rəlɪ] adv. 通常，普遍地，一般地</p><p>generalize [ˈdʒɛnrəˌlaɪz] v. 一般化，推广</p><p>generality [dʒenə’rælɪtɪ] n. 普遍性，大部分，概论</p><p>mathematics [mæθ(ə)’mætɪks] n. 数学，数学运算</p><p>mathematical [mæθ(ə)’mætɪk(ə)l] adj. 数学的，数学上的，精确的</p><p>mathematically adv. 算术地，数学上地</p><p>formulate [‘fɔːmjʊleɪt] v. 规划，用公式表示，明确地表达</p><p>component [kəm’pəʊnənt]  n. 组成部分，成分，组件，元件 adj. 组成的</p><p>identify [aɪ’dentɪfaɪ] v. 确定，鉴定，识别，辨认出</p><p>identity [aɪˈdentəti] n. 身份，特性，一致</p><p>identification [aɪ,dentɪfɪ’keɪʃ(ə)n] n. 鉴定，识别，认同，身份证明</p><p>convex ‘kɒnveks] adj. 凸面的，凸出的 n. 凸面，凸出部分</p><p>alternative [ɔːl’tɜːnətɪv; ɒl-] adj. 供选择的，选择性的，交替的 n. 二中择一</p><p>quadratic  [kwɒ’drætɪk] adj. 二次的 n. 二次方程式</p><p>constraint [kən’streɪnt] n. 约束</p><p>converge [kən’vɜːdʒ]  v. 使汇聚 v. 聚集，靠拢，收敛</p><hr><h3 id="2019-9-21"><a href="#2019-9-21" class="headerlink" title="2019/9/21"></a>2019/9/21</h3><p>posterior [pɒ’stɪərɪə] adj. 其次的，较后的 n. 后部</p><p>probability [prɒbə’bɪlɪtɪ] n. 可能性，几率</p><p>multiple [‘mʌltɪpl] adj. 多重的，多样的，许多的 </p><p>multiply [‘mʌltɪplaɪ] v. 乘，繁殖，增加 </p><p>robust [rə(ʊ)’bʌst] adj. 强健的，健康的，粗野的</p><p>contrast [‘kɒntrɑːst] n. 明显的差异，对比，对照，反差 v. 对比，对照</p><p>retain [rɪ’teɪn] vt. 保持，雇，记住</p><p>reserve [rɪ’zɜːv] v. 预订，储备，拥有 n. 储备量</p><p>comparison [kəm’pærɪs(ə)n] n. 比较，对照，比喻，比较关系</p><hr><h3 id="2019-9-24"><a href="#2019-9-24" class="headerlink" title="2019/9/24"></a>2019/9/24</h3><p>prerequisite [priː’rekwɪzɪt] n.先决条件 adj. 首要必备的</p><p>intensity [ɪn’tensɪtɪ] n. 强度，强烈，亮度，紧张</p><p>density [‘densɪtɪ] n. 密度</p><p>orientation [,ɔːrɪən’teɪʃ(ə)n; ,ɒr-] n. 方向，定向，适应，情况介绍</p><p>interval [‘ɪntəv(ə)l] n. 间隔，间距，幕间休息</p><hr><h3 id="2019-9-25"><a href="#2019-9-25" class="headerlink" title="2019/9/25"></a>2019/9/25</h3><p>generate [ˈdʒenəreɪt] v. 使形成，发生，生殖</p><p>generation [ˌdʒenəˈreɪʃn] n. 一代，产生，一代人，生殖</p><p>generator [ˈdʒenəreɪtə(r)] n. 发电机，发生器，生产者</p><p>generative  [ˈdʒenərətɪv] adj. 生殖的，生产的，有生殖力的，有生产力的</p><p>general [ˈdʒenrəl] adj. 一般的，普通的，综合的，大体的；n. 一般</p><p>generality [ˌdʒenəˈræləti] n. 概论，普遍性，大部分</p><p>adversarial [ˌædvəˈseəriəl] adj. 对抗的，对手的，敌手的</p><p>tackle [ˈtækl] v. 应付，应对，处理</p><p>auxiliary [ɔːɡˈzɪliəri] adj. 辅助的，副的，附加的 </p><p>scenario [səˈnɑːriəʊ] n. 方案，情节，剧本，设想</p><p>primary [ˈpraɪməri] adj. 主要的，初级的，基本的 n. 原色</p><p>respective [rɪˈspektɪv] adj. 分别的，各自的</p><p>respectively [rɪˈspektɪvlɪ] adv. 分别地</p><p>discriminate [dɪˈskrɪmɪneɪt] vt. 歧视，区别，辨别 vi. 区别，辨别</p><p>discrimination n. 歧视，区别，辨别，识别力</p><p>discriminator n. 判别器，鉴别器，辨识者</p><p>discriminable adj. 可辨别的，可分别的</p><p>intensify [ɪnˈtensɪfaɪ] vi. 增强，强化，变激烈</p><p>intensity [ɪnˈtensəti] n. 强度，强烈，亮度，紧张</p><p>intensive [ɪnˈtensɪv] adj. 加强的，集中的，透彻的</p><p>converge [kənˈvɜːdʒ] v. 聚集，靠拢，收敛</p><p>explicit [ɪkˈsplɪsɪt] adj. 明确的，清楚的，直率的，详述的</p><p>approximation  [əˌprɒksɪˈmeɪʃn] n. 近似法，接近，近似值</p><p>intractable [ɪnˈtræktəbl] adj. 棘手的，难治的，倔强的，不听话的</p><p>contrast [ˈkɒntrɑːst] n. 对比，对照 v. 对比，对照</p><p>mode [məʊd] n. 模式，方式，风格，时尚</p><p>collapse [kəˈlæps] v. 倒塌，倒下，昏倒 n. 崩溃，倒塌</p><p>entropy [ˈentrəpi] n. 熵</p><p>margin [ˈmɑːdʒɪn] n. 边缘，利润 v. 加边于</p><p>omitted [əʊ’mɪtɪd] adj. 省略了的，省去的 v. 遗漏，省略</p><p>ascending [ə’sendɪŋ] adj. 上升的，增长的 v. 上升</p><p>descending [dɪ’sendɪŋ] v. 下降，下倾 adj. 递降的</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;论文常见单词积累 (一)&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://sunfeng.online/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="paper" scheme="http://sunfeng.online/tags/paper/"/>
    
      <category term="word" scheme="http://sunfeng.online/tags/word/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的数学基础</title>
    <link href="http://sunfeng.online/2019/09/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <id>http://sunfeng.online/2019/09/17/机器学习的数学基础/</id>
    <published>2019-09-17T13:34:53.000Z</published>
    <updated>2019-09-24T13:10:11.678Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">机器学习的数学基础</font></strong></p><a id="more"></a><hr><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h4 id="机器学习的特点"><a href="#机器学习的特点" class="headerlink" title="机器学习的特点"></a>机器学习的特点</h4><ul><li>以计算机为工具和平台</li><li>以数据为研究对象</li><li>以学习方法为中心</li></ul><h4 id="机器学习涉及的学科"><a href="#机器学习涉及的学科" class="headerlink" title="机器学习涉及的学科"></a>机器学习涉及的学科</h4><ul><li>机器学习是一门交叉学科</li><li>机器学习涉及的领域：<ul><li>概率论</li><li>线性代数</li><li>数值计算</li><li>信息论</li><li>最优化理论</li><li>计算机科学</li></ul></li></ul><hr><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><h4 id="标量-scalar"><a href="#标量-scalar" class="headerlink" title="标量 (scalar)"></a>标量 (scalar)</h4><p>一个标量就是一个单独的数，一般用小写的变量名称表示。</p><hr><h4 id="向量-vector"><a href="#向量-vector" class="headerlink" title="向量 (vector)"></a>向量 (vector)</h4><p>一个向量就是一列数，这些数是有序排列的。通过次序中的索引，我们可以确定每个单独的数。常会赋予向量粗体的小写名称，当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱：</p><script type="math/tex; mode=display">\boldsymbol x = \begin{bmatrix}x_1\\x_2\\\vdots \\x_n\end{bmatrix}</script><p>我们可以把向量看作空间中的点，每个元素是不同坐标轴上的坐标。</p><hr><h4 id="矩阵-matrix"><a href="#矩阵-matrix" class="headerlink" title="矩阵 (matrix)"></a>矩阵 (matrix)</h4><p>矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常赋予矩阵粗体的大写变量名称，比如 $\boldsymbol A$. 如果一个实数矩阵高度为 $m$, 宽度为 $n$, 那么我们说 $A\in R^{m \times n}$.</p><script type="math/tex; mode=display">\boldsymbol A = \begin {bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\a_{31} & a_{32} & \cdots & a_{3n} \\\vdots & \vdots & \cdots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix}</script><p><strong>矩阵在机器学习中非常重要！</strong></p><p>实际上，如果我们现在有 $N$ 个用户的数据，每条数据含有 $M$ 个特征，那其实它对应的就是一个 $N \times M$ 的矩阵；在比如，一张图由 $16\times 16$ 个像素点组成，那么这就是一个 $16 \times 16$ 的矩阵了。</p><hr><h4 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量 (tensor)"></a>张量 (tensor)</h4><p><strong>张量本质上是一个数据容器</strong></p><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将<strong>标量视为零阶张量，向量视为一阶张量，那么矩阵就是二阶张量。</strong></p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图片用张量表示出来，就是下方的这张表格：</p><p><img src="/2019/09/17/机器学习的数学基础/image1.PNG" alt="image1"></p><p>其中，表的横轴表示图片的宽度值，这里只截取0~319；表的纵轴表示图片的高度值，这里只截取0~4；表格中的每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况 (即 R = 1.0, G = 1.0, B = 1.0)</p><p>当然，我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别为：图片在数据集中的编号、图片高度、宽度以及色彩数据。</p><font color="#0099ff">张量在深度学习中是一个很重要的概念，因为它是深度学习框架 TensorFlow 中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的</font><hr><h4 id="范数-norm"><a href="#范数-norm" class="headerlink" title="范数 (norm)"></a>范数 (norm)</h4><p>有时我们需要衡量一个向量的大小，在机器学习中，我们常用被称为范数的函数衡量向量大小，$L_p$范数如下：</p><script type="math/tex; mode=display">\left\|x \right\|_p = \left(\sum_i \left|x_i \right|^p \right)^{\frac{1}{p}}</script><p>所以：</p><p>$L_1$ 范数 $\left|x \right|$: 为 $x$ 向量各个元素绝对值之和</p><p>$L_2$ 范数 $\left|x \right|_2$: 为 $x$ 向量各个元素平方和的开方</p><p><strong>说明：</strong></p><p>在机器学习中，$L_1$ 范数和 $L_2$ 范数很常见，主要用在损失函数中起到一个限制模型复杂度的作用，至于为什么要限制模型复杂度，这里又涉及到机器学习中常见的过拟合问题。</p><hr><h4 id="特征分解-eigendecomposition"><a href="#特征分解-eigendecomposition" class="headerlink" title="特征分解 (eigendecomposition)"></a>特征分解 (eigendecomposition)</h4><p>许多数学对象可以通过将它们分解成多个组成部分，特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组<strong>特征向量</strong>和<strong>特征值</strong>。</p><p><strong><font color="#0099ff">方阵 A 的特征向量是指与 A 相乘后相当于对该向量进行缩放的非零向量 v:</font></strong></p><script type="math/tex; mode=display">\boldsymbol A \boldsymbol v = \lambda \boldsymbol v</script><p>标量 $\lambda$ 被称为这个特征向量 $\boldsymbol v $ 对应的特征值</p><p>假设矩阵 $\boldsymbol A$ 有 $n$ 个线性无关的特征向量 $\{\boldsymbol v^{(1)},\boldsymbol v^{(2)},…,\boldsymbol v^{(n)} \}$, 对应着特征值 $\{\lambda_{1}, \lambda_{2},…,\lambda_{n} \}$. 我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$\boldsymbol V = \left[\boldsymbol v^{(1)},\boldsymbol v^{(2)},…,\boldsymbol v^{(n)}  \right]$.  类似地，我们也可以将特征值连接成一个向量 $\boldsymbol \lambda = [\lambda_{1}, \lambda_{2},…,\lambda_{n}]^T$. 因此 $\boldsymbol A$ 的特征分解可记作：</p><script type="math/tex; mode=display">\boldsymbol A = \boldsymbol V diag(\boldsymbol \lambda)\boldsymbol V^{-1}</script><p>其中，$diag(\boldsymbol \lambda)$ 表示向量 $\boldsymbol \lambda$ 在 $n\times n$ 方阵的对角线上。</p><hr><h4 id="奇异值分解-Singular-Value-Decomposition-SVD"><a href="#奇异值分解-Singular-Value-Decomposition-SVD" class="headerlink" title="奇异值分解 (Singular Value Decomposition, SVD)"></a>奇异值分解 (Singular Value Decomposition, SVD)</h4><p>矩阵的特征分解是由前提条件的，那就是只有对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵。</p><p>因此，可将矩阵的特征分解进行推广，得到一种叫作<strong>矩阵奇异值分解</strong>的方法。</p><p>其具体做法是将一个普通矩阵分解为奇异向量和奇异值，比如将矩阵 $\boldsymbol A$ 分解成三个矩阵的乘积：</p><script type="math/tex; mode=display">\boldsymbol A = \boldsymbol U \boldsymbol D \boldsymbol V^T</script><p>假设 <strong>$\boldsymbol A$ 是一个 $m \times n $ 矩阵</strong>，那么 <strong>$\boldsymbol U$ 是一个 $m \times m$ 矩阵，$\boldsymbol D$ 是一个 $m \times n$ 矩阵，$\boldsymbol V$ 是一个 $n \times n$ 矩阵</strong> </p><p>这些矩阵每一个都拥有特殊的结构，其中 $\boldsymbol U$ 和 $\boldsymbol V $ 都是正交矩阵，$\boldsymbol D$ 是对角矩阵。</p><p>对角矩阵 $\boldsymbol D$ 对角线上的元素被称为矩阵 $\boldsymbol A$ 的<strong>奇异值</strong></p><p>正交矩阵 $\boldsymbol U$ 的列向量被称为<strong>左奇异向量</strong></p><p>正交矩阵 $\boldsymbol V$ 的列向量被称为<strong>右奇异向量</strong></p><p>SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。</p><hr><h4 id="几种常用的距离"><a href="#几种常用的距离" class="headerlink" title="几种常用的距离"></a>几种常用的距离</h4><p>在机器学习中，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。</p><p>设有两个 $n$ 维向量 $\boldsymbol A =\left[x_{11},x_{12},…,x_{1n} \right]$ 和 $\boldsymbol B =\left[x_{21},x_{22},…,x_{2n} \right]$, 则一些常用的距离公式定义如下：</p><h5 id="曼哈顿距离-Manhattan-distance"><a href="#曼哈顿距离-Manhattan-distance" class="headerlink" title="曼哈顿距离 (Manhattan distance)"></a>曼哈顿距离 (Manhattan distance)</h5><p>曼哈顿距离也称为城市街区距离，数学定义如下：</p><script type="math/tex; mode=display">d_{12} = \sum_{k=1}^{n}\left|x_{1k}-x_{2k} \right|</script><p>曼哈顿距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">dist = sum(abs(vector1 - vector2))</span><br></pre></td></tr></table></figure><hr><h5 id="欧式距离-Euclidean-distance"><a href="#欧式距离-Euclidean-distance" class="headerlink" title="欧式距离 (Euclidean distance)"></a>欧式距离 (Euclidean distance)</h5><p>欧式距离是比较常用的距离，数学定义如下：</p><script type="math/tex; mode=display">d_{12} = \sqrt{\sum_{k=1}^{n}(x_{1k}-x_{2k})^2}</script><p>欧式距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">dist = sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure><hr><h5 id="闵可夫斯基距离-Minkowski-distance"><a href="#闵可夫斯基距离-Minkowski-distance" class="headerlink" title="闵可夫斯基距离 (Minkowski distance)"></a>闵可夫斯基距离 (Minkowski distance)</h5><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</p><script type="math/tex; mode=display">d_{12} = \sqrt[p]{\sum_{k=1}^{n}(x_{1k}-x_{2k})^p }</script><p>当 $p = 1$ 时，就是曼哈顿距离</p><p>当 $p = 2$ 时，就是欧式距离</p><hr><h5 id="切比雪夫距离-Chebyshev-distance"><a href="#切比雪夫距离-Chebyshev-distance" class="headerlink" title="切比雪夫距离 (Chebyshev distance)"></a>切比雪夫距离 (Chebyshev distance)</h5><p>切比雪夫距离就是 $L_{\infty}$, 即无穷范数，数学表达式如下：</p><script type="math/tex; mode=display">d_{12} = max(\left|x_{1k}-x_{2k} \right|)</script><p>切比雪夫距离的 Python 实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span>*</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">dist = sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure><hr><h5 id="夹角余弦-Angle-cosine"><a href="#夹角余弦-Angle-cosine" class="headerlink" title="夹角余弦 (Angle cosine)"></a>夹角余弦 (Angle cosine)</h5><p>夹角余弦的取值范围为 [-1,1], 可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1.</p><p>机器学习中用这一概念来衡量样本向量之间的差异，用其数学表达式如下：</p><script type="math/tex; mode=display">\cos\theta = \frac{\boldsymbol A \boldsymbol B}{|\boldsymbol A| |\boldsymbol B|} = \frac{\sum_{k=1}^{n}x_{1k}x_{2k}}{\sqrt{\sum_{k=1}^{n}x_{1k}^2}\sqrt{\sum_{k=1}^{n}x_{2k}^2}}</script><p>夹角余弦的 Python 实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">dist = dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure><hr><h5 id="汉明距离-Hamming-distance"><a href="#汉明距离-Hamming-distance" class="headerlink" title="汉明距离 (Hamming distance)"></a>汉明距离 (Hamming distance)</h5><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><p>例如：字符串 <code>&#39;1111&#39;</code>  与 <code>&#39;1001&#39;</code> 之间的汉明距离为2.</p><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的 Python 实现:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>][<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><hr><h5 id="杰卡德相似系数-Jaccard-similarity-coefficient"><a href="#杰卡德相似系数-Jaccard-similarity-coefficient" class="headerlink" title="杰卡德相似系数 (Jaccard similarity coefficient)"></a>杰卡德相似系数 (Jaccard similarity coefficient)</h5><p>两个集合 $A$ 和 $B$ 的交集元素在 $A$ 和 $B$ 的并集中所占的比例称为两个集合的杰卡德相似系数，用符号 $J(A,B)$ 表示，表达式为：</p><script type="math/tex; mode=display">J(A,B) = \frac{\left|A\cap B\right|}{\left|A\cup B\right|}</script><p>杰卡德相似系数是衡量两个集合相似度的一种指标，一般可以将其用在衡量样本的相似度上</p><hr><h5 id="杰卡德距离-Jaccard-distance"><a href="#杰卡德距离-Jaccard-distance" class="headerlink" title="杰卡德距离 (Jaccard distance)"></a>杰卡德距离 (Jaccard distance)</h5><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：</p><script type="math/tex; mode=display">J_e = 1-J(A,B) = \frac{\left|A\cup B\right| - \left|A\cap B\right|}{\left|A\cup B\right|}</script><p>杰卡德距离的 Python 实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matv = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>][<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">d = dist.pdist(matV,<span class="string">'jaccard'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h3><h4 id="为什么使用概率？-Probability"><a href="#为什么使用概率？-Probability" class="headerlink" title="为什么使用概率？(Probability)"></a>为什么使用概率？(Probability)</h4><p>概率论用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。</p><p>在人工智能领域，我们主要以两种方式来使用概率。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式；其次，我们可以用概率和统计从理论上分析我们提出的 AI 系统的行为。</p><p>计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常，因为机器学习大部分时候处理的都是不确定量或随机量。</p><hr><h4 id="随机变量-Random-Variable"><a href="#随机变量-Random-Variable" class="headerlink" title="随机变量 (Random Variable)"></a>随机变量 (Random Variable)</h4><p>随机变量是可以随机第取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如：$x_1$ 和 $x_2$ 都是随机变量 x 可能的取值。</p><p>对于向量值变量，我们会将随机变量写成 $\boldsymbol X$, 它的一个值为 $x$. 就其本身而言，一个随机变量只是对可能状态的描述，它必须伴随着一个概率分布来指定每个状态的可能性。</p><p>随机变量可以是离散的或者连续的。</p><hr><h4 id="概率分布-Probability-distribution"><a href="#概率分布-Probability-distribution" class="headerlink" title="概率分布 (Probability distribution)"></a>概率分布 (Probability distribution)</h4><p>给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。</p><p>从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。</p><hr><h4 id="条件概率-Conditional-Probability"><a href="#条件概率-Conditional-Probability" class="headerlink" title="条件概率 (Conditional Probability)"></a>条件概率 (Conditional Probability)</h4><p>很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫做条件概率。</p><p>我们将给定 $\boldsymbol X = x$ 时 $\boldsymbol Y = y$ 发生的概率记为 $P(\boldsymbol Y = y |   \boldsymbol X = x)$ , 这个概率可以通过下面的公式来计算：</p><script type="math/tex; mode=display">P(\boldsymbol Y = y\ | \  \boldsymbol X = x) = \frac{P(\boldsymbol Y = y\ , \  \boldsymbol X = x)}{P( \boldsymbol X = x)}</script><hr><h4 id="贝叶斯公式-Bayesian-formula"><a href="#贝叶斯公式-Bayesian-formula" class="headerlink" title="贝叶斯公式 (Bayesian formula)"></a>贝叶斯公式 (Bayesian formula)</h4><p><strong>先验概率：</strong> </p><p>假设某种病在人群中的发病率是0.001，即在1000人中大概会有1个人得病，则：<strong>P(患病) = 0.1%</strong>；即：在没有做检验之前，我们预计的患病率为<strong>P(患病) = 0.1%</strong>，这个就叫做<strong>先验概率</strong></p><p><strong>后验概率：</strong></p><p>再假设现在有一种该病的检测方法，其检测的准确率为 <strong>95%</strong>; 即：如果真的得了这种病，该检测法有 <strong>95%</strong> 的概率会检测出阳性，但也有 <strong>5%</strong> 的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有 <strong>95%</strong> 的概率检测出阴性，但也有 <strong>5%</strong> 的概率检测为阳性。用条件概率表示即为：<strong>P(显示阳性|患病) = 95%</strong></p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率 <strong>P(患病|显示阳性)</strong>，这个其实就称为<strong>后验概率</strong></p><p><strong>贝叶斯公式：</strong></p><p>而这个交贝叶斯的人其实就是为我们提供了一种可以 <strong>利用先验概率计算后验概率</strong>的方法，我们将其称为<strong>贝叶斯公式</strong></p><p>这里先了解<strong>条件概率公式：</strong></p><script type="math/tex; mode=display">P(B|A) = \frac{P(AB)}{P(A)}, P(A|B) =\frac{P(AB)}{P(B)}</script><p>有条件概率可以得到<strong>乘法公式：</strong></p><script type="math/tex; mode=display">P(AB) = P(B|A)P(A) = P(A|B)P(B)</script><p>将条件概率公式和乘法公式结合可以得到：</p><script type="math/tex; mode=display">P(B|A) = \frac{P(A|B)\cdot P(B)}{P(A)}</script><p>再由<strong>全概率公式</strong>：</p><script type="math/tex; mode=display">P(A) = \sum_{i=1}^{N}P(A|B_i)\cdot P(B_i)</script><p>代入可以得到<strong>贝叶斯公式：</strong></p><script type="math/tex; mode=display">P(B_i|A) = \frac{P(A|B_i)\cdot P(B_i)}{\sum_{i=1}^{N}P(A|B_i)\cdot P(B_i)}</script><p>在这个例子里就是：</p><script type="math/tex; mode=display">\begin{align}P(患病|显示阳性) &= \frac{P(显示阳性|患病)P(患病)}{P(显示阳性)}\\&= \frac{P(显示阳性|患病)P(患病)}{}\end{align}</script><h4 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h4><h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><h4 id="常见的函数分布"><a href="#常见的函数分布" class="headerlink" title="常见的函数分布"></a>常见的函数分布</h4><h5 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h5><h5 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h5><h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h5><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h5><h5 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h5><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h5><h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><h4 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h4><h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><h4 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h4><h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><h4 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h4><h4 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h4><h4 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h4><hr><h3 id="数值计算"><a href="#数值计算" class="headerlink" title="数值计算"></a>数值计算</h3><h4 id="上溢和下溢"><a href="#上溢和下溢" class="headerlink" title="上溢和下溢"></a>上溢和下溢</h4><h4 id="计算复杂性和-NP-问题"><a href="#计算复杂性和-NP-问题" class="headerlink" title="计算复杂性和 NP 问题"></a>计算复杂性和 NP 问题</h4><h5 id="算法复杂性"><a href="#算法复杂性" class="headerlink" title="算法复杂性"></a>算法复杂性</h5><h5 id="确定性和非确定性"><a href="#确定性和非确定性" class="headerlink" title="确定性和非确定性"></a>确定性和非确定性</h5><h5 id="NP问题"><a href="#NP问题" class="headerlink" title="NP问题"></a>NP问题</h5><h4 id="数值计算-1"><a href="#数值计算-1" class="headerlink" title="数值计算"></a>数值计算</h4><hr><h3 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h3><h4 id="最优化理论"><a href="#最优化理论" class="headerlink" title="最优化理论"></a>最优化理论</h4><h4 id="最优化问题和数学描述"><a href="#最优化问题和数学描述" class="headerlink" title="最优化问题和数学描述"></a>最优化问题和数学描述</h4><h4 id="凸集和凸集分离定理"><a href="#凸集和凸集分离定理" class="headerlink" title="凸集和凸集分离定理"></a>凸集和凸集分离定理</h4><h5 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h5><h5 id="超平面和空间"><a href="#超平面和空间" class="headerlink" title="超平面和空间"></a>超平面和空间</h5><h5 id="凸集分离定理"><a href="#凸集分离定理" class="headerlink" title="凸集分离定理"></a>凸集分离定理</h5><h5 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h5><h4 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h4><h5 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h5><h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><h4 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h4><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><h5 id="牛顿法介绍"><a href="#牛顿法介绍" class="headerlink" title="牛顿法介绍"></a>牛顿法介绍</h5><h5 id="牛顿法推导"><a href="#牛顿法推导" class="headerlink" title="牛顿法推导"></a>牛顿法推导</h5><h5 id="牛顿法过程"><a href="#牛顿法过程" class="headerlink" title="牛顿法过程"></a>牛顿法过程</h5><h4 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h4><h5 id="引入-1"><a href="#引入-1" class="headerlink" title="引入"></a>引入</h5><h5 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h5><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><h5 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h5><h5 id="拟牛顿法推导"><a href="#拟牛顿法推导" class="headerlink" title="拟牛顿法推导"></a>拟牛顿法推导</h5><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;机器学习的数学基础&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数学方法整理" scheme="http://sunfeng.online/categories/%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86/"/>
    
    
      <category term="机器学习" scheme="http://sunfeng.online/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《改善神经网络》课程笔记（1）-- 深度学习的实践层面</title>
    <link href="http://sunfeng.online/2019/09/15/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E5%B1%82%E9%9D%A2/"/>
    <id>http://sunfeng.online/2019/09/15/吴恩达《改善神经网络》课程笔记（1）-- 深度学习的实践层面/</id>
    <published>2019-09-15T05:53:24.000Z</published>
    <updated>2019-09-20T09:12:54.480Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《改善神经网络》课程笔记（1） 深度学习的实践层面</font></strong></p><a id="more"></a><h3 id="训练，验证，测试集-Train-Dev-Test-sets"><a href="#训练，验证，测试集-Train-Dev-Test-sets" class="headerlink" title="训练，验证，测试集 (Train / Dev / Test sets)"></a>训练，验证，测试集 (Train / Dev / Test sets)</h3><p>应用深度学习是一个典型的迭代过程</p><p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p><ul><li><strong>训练集</strong> (training set)：用训练集对算法或模型进行<strong>训练</strong>过程</li><li><strong>验证集</strong> (development set)：利用验证集进行<strong>交叉验证</strong>，选出最好的模型</li><li><strong>测试集</strong> (test set)：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong></li></ul><p>在<strong>小数据量</strong>的时代，如100,1000,10000，的数据量大小，可以将数据集按照以下比例进行划分：</p><ul><li>无验证集的情况：70%/30%</li><li>有验证集的情况：60%/20%/20%</li></ul><p>如今在<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p><p><strong>验证集的目的</strong>是验证不同算法哪种更有效果，所以验证集只要足够大到能够验证大约2~10种算法哪种更好，而不需要使用20%的数据作为验证集，如百万数据中抽取1万的数据作为验证集就可以了。</p><p><strong>测试集的目的</strong>是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中1000条数据足以评估单个模型的效果。</p><ul><li>100万数据量：98%/1%/1%</li><li>超百万数据量：99.5%/0.25%/0.25%</li></ul><p><strong><font size="4,font" color="#FF00">建议：</font></strong></p><p>建议<strong>确保验证集和测试集的数据来自同一分布</strong></p><p>如果不需要<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集</p><hr><h3 id="偏差，方差-Bias-Variance"><a href="#偏差，方差-Bias-Variance" class="headerlink" title="偏差，方差 (Bias / Variance)"></a>偏差，方差 (Bias / Variance)</h3><p><strong>偏差-方差分解</strong> (bias-variance decomposition) 是解释学习算法泛化性能的一种重要工具</p><p><strong>泛化误差可以分解为偏差、方差和噪声之和：</strong></p><ul><li><strong>偏差：</strong> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了 <strong>学习算法本身的拟合能力</strong></li><li><strong>方差：</strong> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong></li><li><strong>噪声：</strong>  表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了 <strong>学习问题本身的难度</strong></li></ul><p><strong>偏差-方差分解说明：</strong></p><ul><li><strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的，给定学习人往无，为了取得好的泛化性能，则需要是偏差较小，既能够充分拟合数据；并且使方差较小，即使得数据扰动产生的影响小</li></ul><p>在 <strong>欠拟合 (under-fitting)</strong> d的情况下，出现 <strong>高偏差 (high bias)</strong> 的情况，即不能很好地对数据进行分类。</p><p>当模型设置太复杂是，训练集中的一些噪声没有被排除，使得模型出现 <strong>过拟合 (over-fitting)</strong> 的情况，在验证集上表现出 <strong>高方差 (high variance)</strong> 的现象</p><p><strong>当训练一个模型以后，如果：</strong></p><ul><li>训练集的错误率较小，而验证集的错误率较大，说明模型存在较大方差，可能出现了过拟合</li><li>训练集和验证集错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合</li><li>训练集错误率较大，且验证集错误率远较训练集大，说明方差和偏差都较大，模型很差</li><li>训练集和验证集的错误率都较小，且两者相差也较小，说明方差和偏差都较小，这个模型效果比较好</li></ul><p><strong>应对方法：</strong></p><ul><li>存在高偏差：<ul><li>扩大网络规模，如添加隐藏层或隐藏层单元数目</li><li>寻找合适的网络架构，使用更大的神经网络结构</li><li>花费更长时间训练</li></ul></li><li>存在高方差：<ul><li>获取更多的数据</li><li>正则化 (regularization)</li><li>寻找更合适的网络结构</li></ul></li></ul><h3 id="机器学习基础-Basic-Recipe-for-Machine-Learning"><a href="#机器学习基础-Basic-Recipe-for-Machine-Learning" class="headerlink" title="机器学习基础 (Basic Recipe for Machine Learning)"></a>机器学习基础 (Basic Recipe for Machine Learning)</h3><h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 (Regularization)"></a>正则化 (Regularization)</h3><h3 id="为什么正则化有利于过拟合-Why-regularization-reduces-over-fitting"><a href="#为什么正则化有利于过拟合-Why-regularization-reduces-over-fitting" class="headerlink" title="为什么正则化有利于过拟合 (Why regularization reduces over-fitting ?)"></a>为什么正则化有利于过拟合 (Why regularization reduces over-fitting ?)</h3><h3 id="Dropout-正则化"><a href="#Dropout-正则化" class="headerlink" title="Dropout 正则化"></a>Dropout 正则化</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《改善神经网络》课程笔记（1） 深度学习的实践层面&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="改善神经网络" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（4）-- 深层神经网络</title>
    <link href="http://sunfeng.online/2019/09/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89--%20%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://sunfeng.online/2019/09/10/吴恩达《神经网络与深度学习》课程笔记（4）-- 深层神经网络/</id>
    <published>2019-09-10T12:25:32.000Z</published>
    <updated>2019-09-12T11:02:54.741Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（4） 深层神经网络</font></strong></p><a id="more"></a><h3 id="深层神经网络-Deep-L-layer-neural-network"><a href="#深层神经网络-Deep-L-layer-neural-network" class="headerlink" title="深层神经网络 (Deep L-layer neural network)"></a>深层神经网络 (Deep L-layer neural network)</h3><p><strong>神经网络的层数定义：</strong></p><ul><li><strong><font color="#ff000">从左到右，由0开始定义</font></strong></li><li>即输出层为第0层，输入层左边第一层隐藏层为第1层</li><li><strong><font color="#0099ff">在算神经网络层数时，我们不断输入层，只算隐藏层和输出层</font></strong></li></ul><p><strong>深度学习的符号定义：</strong></p><p><img src="/2019/09/10/吴恩达《神经网络与深度学习》课程笔记（4）-- 深层神经网络/figure1.PNG" alt="figure1"></p><p>上图是一个四层的神经网络，有三个隐藏层，其中第一层有5个神经元，第二层有5个，第三层有3个</p><ul><li><strong>层数</strong>：用 L 表示层数，上图 L = 4</li><li><strong>索引：</strong><ul><li>输入层的索引为 “0”，且 $n^{[0]} = 3$</li><li>第一个隐藏层 $n^{[1]} = 5$, 表示有 5 个隐藏层神经元</li><li>同理：$n^{[2]} = 5$, $n^{[3]} = 3$, $n^{[4]} = n^{[L]} = 1$</li></ul></li><li><strong>激活结果：</strong> $a^{[l]}$ 表示第 $l$ 层激活后结果</li><li><strong>激活函数：</strong> $g^{[l]}$ 表示第 $l$ 层激活函数</li></ul><hr><h3 id="前向传播和反向传播-Forward-and-backward-propagation"><a href="#前向传播和反向传播-Forward-and-backward-propagation" class="headerlink" title="前向传播和反向传播 (Forward and backward propagation)"></a>前向传播和反向传播 (Forward and backward propagation)</h3><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p><strong>输入：</strong> $a^{[l-1]}$</p><p><strong>输出：</strong> $a^{[l]}$, cache($z^{l}$)</p><p><strong>公式：</strong> </p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}\cdot a^{[l-1]} + b^{[l]}\\a^{[l]} = g^{[l]}(Z^{[l]})</script><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p><strong>输入：</strong> $da^{[l]}$</p><p><strong>输出：</strong> $da^{[l-1]}$, $dW^{[l]}$, db^{[l]}</p><p><strong>公式：</strong></p><script type="math/tex; mode=display">\begin {align}dZ^{[l]} &= da^{[l]}*g^{[l] \prime}(Z^{[l]})\\dW^{[l]} &= dZ^{[l]}\cdot a^{[l-1]}\\db^{[l]} &= dZ^{[l]}\\da^{[l-1]} &= W^{[l]T}\cdot dZ^{[l]}\end {align}</script><hr><h3 id="搭建神经网络模块-Building-Neural-Network-Modules"><a href="#搭建神经网络模块-Building-Neural-Network-Modules" class="headerlink" title="搭建神经网络模块 (Building Neural Network Modules)"></a>搭建神经网络模块 (Building Neural Network Modules)</h3><p><img src="/2019/09/10/吴恩达《神经网络与深度学习》课程笔记（4）-- 深层神经网络/figure2.PNG" alt="figure2"></p><p>神经网络的一步训练 (一个梯度下降循环)，包含了从 $a^{[0]}$ (即 $X$) 经过一系列正向传播计算得到 $\hat y$ ( 即 $a^{[l]}$ ). 然后计算 $da^{[l]}$, 开始实现反向传播，用 <strong>链式法则</strong>得到所有的导数项, $W$ 和 $b$ 也会在每一层被更新。</p><font color="#0099ff">在代码实现时，可以将正向传播过程计算出来的z值缓存下来，待到反向传播计算时使用</font><p><img src="/2019/09/10/吴恩达《神经网络与深度学习》课程笔记（4）-- 深层神经网络/figure3.PNG" alt="figure3"></p><hr><h3 id="矩阵的维数-Matrix-dimensions"><a href="#矩阵的维数-Matrix-dimensions" class="headerlink" title="矩阵的维数 (Matrix dimensions)"></a>矩阵的维数 (Matrix dimensions)</h3><ul><li><p>$W^{[l]}: (n^{[l]},n^{[l-1]})$</p></li><li><p>$b^{[l]}: (n^{[l]},1)$</p></li><li><p>$dW^{[l]}: (n^{[l]},n^{[l-1]})$</p></li><li><p>$db^{[l]}: (n^{l},1)$</p></li><li><p>对于 $Z$, $a$ 向量化之前: $Z^{[l]},a^{[l]}: (n^{[l]},1)$</p></li><li><p>而在向量化之后，则有: $Z^{[l]},A^{[l]}: (n^{[l]},m)$</p></li></ul><hr><h3 id="为什么使用深层表示？-Why-deep-representations"><a href="#为什么使用深层表示？-Why-deep-representations" class="headerlink" title="为什么使用深层表示？ (Why deep representations?)"></a>为什么使用深层表示？ (Why deep representations?)</h3><p>对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。</p><p>同样的，对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。</p><p>通过例子可以看到，随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</p><hr><h3 id="参数-VS-超参数-Parameters-VS-Hyperparameters"><a href="#参数-VS-超参数-Parameters-VS-Hyperparameters" class="headerlink" title="参数 VS 超参数 (Parameters VS Hyperparameters)"></a>参数 VS 超参数 (Parameters VS Hyperparameters)</h3><p><strong>参数</strong>即是我们在过程中想要模型学习到的信息（<strong>模型自己能计算出来的</strong>），例如 $W^{[l]}$，$b^{[l]}$。而<strong>超参数（hyper parameters）</strong>即为控制参数的输出值的一些网络信息（<strong>需要人经验判断</strong>）。超参数的改变会导致最终得到的参数 $W^{[l]}$，$b^{[l]}$ 的改变。</p><p>典型的超参数有：</p><ul><li>学习速率：α</li><li>迭代次数：N</li><li>隐藏层的层数：L</li><li>每一层的神经元个数：n[1]n[1]，n[2]n[2]，…</li><li>激活函数 g(z) 的选择</li></ul><p>当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（4） 深层神经网络&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络</title>
    <link href="http://sunfeng.online/2019/09/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89--%20%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://sunfeng.online/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/</id>
    <published>2019-09-07T08:53:01.000Z</published>
    <updated>2019-09-10T09:10:34.180Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（3） 浅层神经网络</font></strong></p><a id="more"></a><h3 id="神经网络的表示-Neural-Network-Representation"><a href="#神经网络的表示-Neural-Network-Representation" class="headerlink" title="神经网络的表示 (Neural Network Representation)"></a>神经网络的表示 (Neural Network Representation)</h3><p><strong>输入层 (Input Layer):</strong></p><ul><li>竖向堆叠起来的输入特征被称为神经网络的输入层</li></ul><p><strong>隐藏层 (Hidden Layer):</strong></p><ul><li>“隐藏”的含义是：在训练集中，这些中间节点的真正数值是无法看到的</li></ul><p><strong>输出层 (Output Layer):</strong></p><ul><li>负责输出预测值</li></ul><p><img src="/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/figure1.PNG" alt="figure1"></p><p>如图是一个 <strong>双层神经网络</strong>，也称作<strong>单隐层神经网络</strong> (a single hidden layer neural network). 当我们计算网络层数时，通常不考虑输入层，因此，图中隐藏层是第一层，输出层是第二层。</p><p><strong>一些约定俗成的符号表示：</strong></p><ul><li>输入层的激活值为 $a^{[0]}$</li><li>同样，隐藏层也会产生一些激活值，记作 $a^{[1]}$ , 其中隐藏层的第一个单元或节点就记作 $a^{[1]}_1$, 输出层同理</li><li>另外，隐藏层和输出层都是带有参数 $w$ 和 $b$ 的，它们都使用上标 <code>[1]</code> 来表示是和第一个隐藏层有关，或者上标 <code>[2]</code> 来表示和输出层有关</li></ul><hr><h3 id="计算神经网络的输出-Computing-a-Neural-Network’s-output"><a href="#计算神经网络的输出-Computing-a-Neural-Network’s-output" class="headerlink" title="计算神经网络的输出 (Computing a Neural Network’s output)"></a>计算神经网络的输出 (Computing a Neural Network’s output)</h3><p><img src="/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/figure2.PNG" alt="figure2"></p><ul><li><p>实际上，神经网络只不过将逻辑回归的计算步骤重复很多次</p></li><li><p>对于隐藏层的第一个节点，有:</p><script type="math/tex; mode=display">\begin {align}z_1^{[1]} &= (W_1^{[1]})^Tx+b_1^{[1]}\\a_1^{[1]} &= \sigma(z_1^{[1]})\end {align}</script></li><li><p>我们可以类推得到，对于第一个隐藏层有下列公式:</p><script type="math/tex; mode=display">\begin {align}z^{[1]} &= (W^{[1]})^Ta^{[0]}+b^{[1]}\\a^{[1]} &= \sigma(z^{[1]})\end {align}</script><p>其中，$a^{[0]}$ 代表输入层的输入 $x$</p></li><li><p>同理，对于输出层，有：</p><script type="math/tex; mode=display">\begin {align}z^{[2]} &= (W^{[2]})^Ta^{[1]}+b^{[2]}\\a^{[2]} &= \sigma(z^{[2]})\end {align}</script></li></ul><p><strong>总结：</strong> 通过上述四个公式，给出一个单独的输入特征向量，运用四行代码计算出一个神经网络的输出</p><p><strong>注意：</strong> 层与层之间的矩阵大小：</p><ul><li>输入层和隐藏层：<ul><li>$(W^{[1]})^T$ 的 shape 为 <code>(4,3)</code>: 前面的4是隐藏层神经单元的个数，后面的3是输入层神经单元的个数</li><li>$b^{[1]}$ 的 shape 为 <code>(4,1)</code>: 和隐藏层神经单元的个数相同</li></ul></li><li>隐藏层和输出层：<ul><li>$(W^{[2]})^T$ 的 shape 为 <code>(1,1)</code>: 前面的1是输出层神经单元的个数，后面的4是隐藏层神经单元的个数</li><li>$b^{[2]}$ 的 shape 为 <code>(1,1)</code>: 和输出层的神经元个数相同</li></ul></li></ul><hr><h3 id="多样本向量化-Vectorizing-across-multiple-examples"><a href="#多样本向量化-Vectorizing-across-multiple-examples" class="headerlink" title="多样本向量化 (Vectorizing across multiple examples)"></a>多样本向量化 (Vectorizing across multiple examples)</h3><p><strong>矩阵 $X$ </strong></p><ul><li>将 $m$ 个含有 n 维特征的训练样本组合成一个 $n \times m$ 的矩阵 $X$:<script type="math/tex; mode=display">X = \begin {bmatrix}\vdots & \vdots & \vdots & \vdots\\x^{(1)} & x^{(2)} &\cdots & x^{(m)} \\\vdots & \vdots & \vdots & \vdots\end {bmatrix}</script></li></ul><p><strong>矩阵 $Z$</strong></p><script type="math/tex; mode=display">Z^{[1]} = \begin {bmatrix}\vdots & \vdots & \vdots & \vdots\\z^{[1](1)} & z^{[1](2)} &\cdots & z^{[1](m)} \\\vdots & \vdots & \vdots & \vdots\end {bmatrix}</script><p><strong>矩阵 $A$</strong></p><script type="math/tex; mode=display">A^{[1]} = \begin {bmatrix}\vdots & \vdots & \vdots & \vdots\\\alpha^{[1](1)} & \alpha^{[1](2)} &\cdots & \alpha^{[1](m)} \\\vdots & \vdots & \vdots & \vdots\end {bmatrix}</script><p><strong>单个样本公式 $\Rightarrow$ 向量化公式</strong></p><script type="math/tex; mode=display">\left. \begin{array}{c}z^{[1](i)} = w^{[1](i)}x^{i}+b^{[1]} \\ \alpha^{[1](i)} = \sigma(z^{[1](i)}) \\ z^{[2](i)} = w^{[2](i)}\alpha^{[1](i)}+b^{[2]} \\ \alpha^{[2](i)} = \sigma(z^{[2](i)})\end{array}\right\}\Rightarrow\left\{\begin{array}{c}Z^{[1]} = W^{[1]}X+b^{[1]}\\A^{[1]} = \sigma(Z^{[1]})\\Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\A^{[2]} = \sigma(Z^{[2]})\\\end{array}\right.</script><hr><h3 id="向量化实现的解释-Justification-for-vectorized-implementation"><a href="#向量化实现的解释-Justification-for-vectorized-implementation" class="headerlink" title="向量化实现的解释 (Justification for vectorized implementation)"></a>向量化实现的解释 (Justification for vectorized implementation)</h3><p><strong>$Z^{[1]} = W^{[1]}X+b^{[1]}$ 的直观解释：</strong></p><ul><li>此处先忽略 $b^{[1]}$, 因为用 <strong>Python</strong> 的广播机制，很容易将 $b^{[1]}$ 加进来</li><li>用图形直观的表示矩阵相乘：</li></ul><p><img src="/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/figure3.PNG" alt="figure3"></p><ul><li>从上图可以看出，当加入更多的样本时，只需向矩阵 $X$ 中加入更多列</li></ul><hr><h3 id="激活函数-Activation-function"><a href="#激活函数-Activation-function" class="headerlink" title="激活函数 (Activation function)"></a>激活函数 (Activation function)</h3><font color="#0099ff">有一个问题是神经网络的隐藏层和输出单元用什么激活函数。之前我们都是选用 sigmoid 函数，但有时其他函数效果会好很多。</font><p><strong>常见的激活函数：</strong></p><ul><li><p><strong>Sigmoid 函数</strong></p><script type="math/tex; mode=display">a = sigmoid(z) = \frac{1}{1+e^{-z}}</script><p>在二分类问题中，对于输出层，因为 $y$ 的值是 0 或 1，所以常用 Sigmoid 函数</p></li><li><p><strong>tanh 函数 (双曲正切函数)</strong></p><script type="math/tex; mode=display">a = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}</script><p>tanh 函数效果几乎总是优于 Sigmoid 函数，因为其函数值域在 -1 和 1之间，其均值更接近零均值，有类似数据中心化的效果</p><p><strong><font color="#ff000">Sigmoid 函数和 tanh 函数两者的共同缺点：</font></strong>在 $z$ 特别大或者特别小时，梯度会变得很小，最后会接近0，导致降低梯度下降的速度。</p></li><li><p><strong>ReLU 函数 (修正线性单元)</strong></p><script type="math/tex; mode=display">a = max(0,z)</script><p>当 z &gt; 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z &lt; 0 时，梯度一直为 0，但是实际的运用中，该缺陷的影响不是很大</p></li><li><p><strong>Leaky ReLU (带泄漏的 ReLU)</strong></p><script type="math/tex; mode=display">a = max(0.01z,z)</script><p>Leaky Relu 保证在 z &lt; 0 的时候，梯度仍然不为 0。理论上来说，Leaky ReLU 有 ReLU 的所有优点，但在实际操作中没有证明总是好于 ReLU，因此不常用。</p></li></ul><p><img src="/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/figure4.PNG" alt="figure4"></p><p><strong>总结：</strong></p><ul><li><strong>sigmoid</strong> 激活函数：除了输出层是一个二分类问题基本不会用它</li><li><strong>tanh</strong> 激活函数：<strong>tanh</strong> 函数是非常优秀的，几乎适合所有场合</li><li><strong>ReLu</strong> 激活函数：最常用的默认函数，如果不确定用哪个函数，就用 <strong>ReLu</strong> 函数或者 <strong>Leaky ReLu</strong></li></ul><hr><h3 id="为什么需要非线性激活函数？-Why-need-a-nonlinear-activation-function"><a href="#为什么需要非线性激活函数？-Why-need-a-nonlinear-activation-function" class="headerlink" title="为什么需要非线性激活函数？(Why need a nonlinear activation function?)"></a>为什么需要非线性激活函数？(Why need a nonlinear activation function?)</h3><p><strong><font color="#0099ff">使用线性激活函数和不使用激活函数，直接使用 Logistic 回归没有区别，那么无论神经网络有多少层，输出都是线性组合，与没有隐藏层效果相当，就成了最原始的感知器了</font></strong></p><hr><h3 id="激活函数的导数-Derivatives-of-activation-functions"><a href="#激活函数的导数-Derivatives-of-activation-functions" class="headerlink" title="激活函数的导数 (Derivatives of activation functions)"></a>激活函数的导数 (Derivatives of activation functions)</h3><ul><li><p><strong>Sigmoid</strong> 函数：</p><script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}\\g^{\prime}(z) = \frac{dg(z)}{dz} = \frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}}) = g(z)(1-g(z))</script></li><li><p><strong>tanh</strong> 函数：</p><script type="math/tex; mode=display">g(z) = tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}} \\g^{\prime}(z) =\frac{dg(z)}{dz} = 1-(tanh(z))^2 = 1-(g(z))^2</script></li></ul><hr><h3 id="神经网络的梯度下降-Gradient-descent-for-neural-networks"><a href="#神经网络的梯度下降-Gradient-descent-for-neural-networks" class="headerlink" title="神经网络的梯度下降 (Gradient descent for neural  networks)"></a>神经网络的梯度下降 (Gradient descent for neural  networks)</h3><h4 id="正向梯度下降"><a href="#正向梯度下降" class="headerlink" title="正向梯度下降"></a>正向梯度下降</h4><script type="math/tex; mode=display">\begin{align}Z^{[1]} &= (W^{[1]})^TX+b^{[1]}\\A^{[1]} &= g^{[1]}(Z^{[1]})\\Z^{[2]} &= (W^{[2]})^TA^{[1]}+b^{[2]}\\A^{[2]} &= g^{[2]}(Z^{[2]})\\\end {align}</script><h4 id="反向梯度下降"><a href="#反向梯度下降" class="headerlink" title="反向梯度下降"></a>反向梯度下降</h4><p>神经网络反向梯度下降公式 (左) 和其代码向量化 (右):</p><p><img src="/2019/09/07/吴恩达《神经网络与深度学习》课程笔记（3）-- 浅层神经网络/figure5.PNG" alt="figure5"></p><hr><h3 id="随机初始化-Random-Initialization"><a href="#随机初始化-Random-Initialization" class="headerlink" title="随机初始化 (Random Initialization)"></a>随机初始化 (Random Initialization)</h3><font color="#0099ff">如果在初始时将两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。</font><p>在初始化的时候，W 参数要进行随机初始化，不可以设置为 0。而 b 因为不存在对称性的问题，可以设置为 0。</p><p><strong>以 2 个输入，2 个隐藏层神经元为例：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(<span class="number">2</span>,<span class="number">2</span>)*<span class="number">0.01</span></span><br><span class="line">b = np.zeros((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>这里将 W 的值乘以 0.01（或者其他的常数值）的原因是为了使得权重 W 初始化为较小的值，这是因为使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 Z=WX+b 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。而如果 W 设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p><p>ReLU 和 Leaky ReLU 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（3） 浅层神经网络&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础</title>
    <link href="http://sunfeng.online/2019/09/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89--%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://sunfeng.online/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/</id>
    <published>2019-09-05T09:41:52.000Z</published>
    <updated>2019-09-07T08:41:02.501Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（2） 神经网络基础</font></strong></p><a id="more"></a><hr><h3 id="二分类-Binary-Classification"><a href="#二分类-Binary-Classification" class="headerlink" title="二分类 (Binary Classification)"></a>二分类 (Binary Classification)</h3><p><strong>神经网络的训练过程：</strong></p><ul><li>先有一个叫做前向暂停 <strong>(forward pause)</strong> 或前向传播 <strong>(forward propagation)</strong> 的步骤</li><li>接着有一个叫做反向暂停 <strong>(backward pause)</strong> 或反向传播 <strong>(backward propagation)</strong> 的步骤</li></ul><p><strong>一个二分类的例子：</strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure1.PNG" alt="figure1"></p><p><strong>输入：</strong> 一张图片</p><p><strong>输出：</strong> 1 (识别这张图片含有猫)、0 (识别这张图片不含猫)</p><p><strong>符号定义：</strong></p><ul><li>$x$: 表示一个 $n_x$ 维数据，为输入数据，维度为 $(n_x,1)$</li><li>$y$: 表示输出结果，取值为 $(0,1)$</li><li>$(x^{(i)},y^{(i)})$: 表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据</li><li>$X = [x^{(1)},x^{(2)},…,x^{(m)}]$: 表示所有训练数据集的输入值，放在一个 $n_x \times m$ 的矩阵中，$m$ 表示样本数目</li><li>$Y = [y^{(1)},y^{(2)},…,y^{(m)}]$: 对应所有训练数据集的输出值，维度为 $1\times m$</li></ul><hr><h3 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归 (Logistic Regression)"></a>逻辑回归 (Logistic Regression)</h3><p><strong><font color="#0099ff">Logistic 回归是一个用于二分类的算法</font></strong></p><p><strong>Logistic 回归中使用的参数如下：</strong></p><ul><li><p>输入的特征向量：$x \in R^{n_x}$, 其中 $n_x$ 是特征数量</p></li><li><p>用于训练的标签：$y \in \{0,1\}$</p></li><li><p>权重：$w\in R^{n_x}$</p></li><li><p>偏置：$b\in R$</p></li><li><p>输出：$\hat y = \sigma(w^Tx+b)$</p></li><li><p>激活函数：Sigmoid 函数</p><script type="math/tex; mode=display">s = \sigma(w^Tx+b) = \sigma(z) = \frac{1}{1+e^{-z}}</script><p>为将 $w^Tx+b$ 约束在 [0,1]间，引入 Sigmoid函数</p></li></ul><p><strong><font color="#0099ff">Logistic 回归可以看作是一个非常小的神经网络，下图是一个典型的例子：</font></strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure2.PNG" alt="figure2"></p><hr><h3 id="逻辑回归的代价函数-Logistic-Regression-Cost-Function"><a href="#逻辑回归的代价函数-Logistic-Regression-Cost-Function" class="headerlink" title="逻辑回归的代价函数 (Logistic Regression Cost Function)"></a>逻辑回归的代价函数 (Logistic Regression Cost Function)</h3><p><strong>损失函数 (loss function)</strong></p><ul><li><font color="#0099">用于衡量预测结果与真实值之间的误差</font></li><li><p>最简单的损失函数定义方式为平方差损失：</p><script type="math/tex; mode=display">L(\hat y,y) = \frac{1}{2}(\hat y-y)^2</script><font color="#0099">但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法找不到全局最优值</font></li><li><p>一般使用交叉熵损失:</p><script type="math/tex; mode=display">L(\hat y,y) = -y\log(\hat y)-(1-y)\log(1-\hat y)</script></li><li><font color="#0099">损失函数是在单个训练样本中定义的，它衡量了算法在单个训练样本上的表现</font></li></ul><p><strong>代价函数 (Cost function)</strong></p><ul><li><font color="#0099">代价函数用于衡量算法在全部样本上的表现</font></li><li><p>代价函数是对 $m$ 个样本的损失函数求和然后除以 $m$:</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})= \frac{1}{m}\left(-y^{(i)}\log(\hat y^{(i)})-(1-y^{(i)})\log(1-\hat y^{(i)})\right)</script></li></ul><hr><h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 (Gradient Descent)"></a>梯度下降 (Gradient Descent)</h3><p><strong>梯度 (gradient)</strong></p><ul><li>梯度是一个向量，其方向是函数增长最快的方向，其大小表示函数的最大增长速度</li><li>按梯度的方向走，函数增长的最快</li><li>按梯度的负方向走，函数降低的最快</li></ul><p><strong>模型训练目标</strong></p><ul><li>模型的训练目标即是寻找合适的 $w$ 和 $b$ 以最小化代价函数值</li><li>简单起见，先假设 $w$ 与 $b$ 都是一维实数，那么可以得到函数 $J$ 关于 $w$ 和 $b$ 的图如下：</li></ul><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure3.PNG" alt="figure3"></p><ul><li><font color="#0099">成本函数 J 是一个凸函数，与非凸函数的区别在于其不含有多个局部最低点，选择这样的代价函数就保证了无论怎样初始化模型参数值，都能寻找到合适的最优解</font></li></ul><p><strong>参数更新：</strong></p><ul><li><p>参数 $w$ 的更新公式：</p><script type="math/tex; mode=display">w := w-\alpha\frac{\partial J(w,b)}{\partial w}</script></li><li><p>参数 $b$ 的更新公式：</p><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial J(w,b)}{\partial b}</script><p>其中， $\alpha$ 为学习率，控制步长</p></li></ul><hr><h3 id="逻辑回归中的梯度下降-Logistic-Regression-Gradient-Descent"><a href="#逻辑回归中的梯度下降-Logistic-Regression-Gradient-Descent" class="headerlink" title="逻辑回归中的梯度下降 (Logistic Regression Gradient Descent)"></a>逻辑回归中的梯度下降 (Logistic Regression Gradient Descent)</h3><p><strong>问题假设：</strong></p><ul><li>假设样本只有两个特征 $x_1$ 和 $x_2$</li></ul><p><strong>参数回顾：</strong></p><ul><li>权值：$w_1$ 和 $w_2$</li><li>偏置：$b$</li><li>线性回归输出：$z = w_1x_1+w_2x_2+b$</li><li>逻辑回归输出：$\hat y  = a = \sigma(z)$, 其中 $z = w_1x_1+w_2x_2+b$, $\sigma(z) = \frac{1}{1-e^{-z}}$</li><li>损失函数：$L(\hat y^{(i)},y^{(i)}) = -y^{(i)}\log(\hat y^{(i)})-(1-y^{(i)})\log(1-\hat y^{(i)})$</li><li>代价函数：$J(w,b) = \frac{1}{m}\sum_{i =1}^{m}L(\hat y^{(i)},y^{(i)})$</li></ul><p><strong>单个样本的梯度下降：</strong></p><ul><li><p>根据上述假设，输入参数有5个：$x_1$, $x_2$, $w_1$, $w_2$ 和 $b$, 可以推导出如下的计算图：</p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure4.PNG" alt="figure4"></p></li><li><p>其中 $L(a,y) = -y\log(a)-(1-y)\log(1-a)$</p></li><li><p>首先反向求出 $L$ 对于 $a$ 的导数:</p><script type="math/tex; mode=display">da = \frac{d L}{d a} = -y/a+(1-y)/(1-a)</script></li><li><p>然后继续求出 $L$ 对于 $z$ 的导数：</p><script type="math/tex; mode=display">\begin{align}dz = \frac{d L}{d z}&= \frac{d L}{d a} \cdot\frac{d a}{d z}\\&= \left( -y/a+(1-y)/(1-a) \right)\cdot\left(a(1-a) \right)\\&= a- y\end {align}</script></li><li><p>最终求出 $L$ 对于参数 $w$ 和 $b$ 的导数：</p><script type="math/tex; mode=display">\begin {align}dw_1 = \frac{\partial L}{\partial w_1} &= \frac{dL}{dz}\cdot\frac{dz}{dw_1} \\&= (a-y) \cdot x_1 \\dw_2 = \frac{\partial L}{\partial w_2} &= \frac{dL}{dz}\cdot\frac{dz}{dw_2} \\&= (a-y) \cdot x_2 \\db = \frac{\partial L}{\partial w_1} &= \frac{dL}{dz}\cdot\frac{dz}{db} \\&= (a-y) \end {align}</script></li><li><p>根据如下公式进行参数更新：</p><script type="math/tex; mode=display">\begin {align}w_1 &:= w_1-\alpha dw_1 \\w_2 &:= w_2-\alpha dw_2 \\b &:= b - \alpha db\end {align}</script></li></ul><p><strong>$m$个样本的梯度下降：</strong></p><ul><li><p>接下来，我们需要将对于单个样本的损失函数扩展到整个训练集的代价函数：</p><script type="math/tex; mode=display">J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)})\\a^{(i)} = \hat y^{(i)} = \sigma(z^{(i)})= \sigma(w^Tx^{(i)}+b)</script></li><li><p>我们可以对于某个权重参数 $w_1$, 其导数计算为:</p><script type="math/tex; mode=display">\frac{\partial J(w,b)}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\frac{\partial L(a^{(i)},y^{(i)})}{\partial w_1}</script></li></ul><p><strong>完成的逻辑回归中某次训练流程如下，这里仅假设特征向量的维度为2：</strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure5.PNG" alt="figure5"></p><ul><li>然后，对 $w_1$, $w_2$ 和 $b$ 进行迭代</li><li><font color="#ff00">上述过程在计算时有一个缺点：需要编写两个 for 循环</font><ul><li>第一个 for 循环用于遍历所有的样本</li><li>第二个 for 循环用于遍历所有的特征</li></ul></li></ul><hr><h3 id="向量化-Vectorization"><a href="#向量化-Vectorization" class="headerlink" title="向量化 (Vectorization)"></a>向量化 (Vectorization)</h3><ul><li><p>在逻辑回归中，需要计算:</p><script type="math/tex; mode=display">z = w^Tx+b</script></li><li><p>如果是非向量化的循环方式，代码可能如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_x):</span><br><span class="line">    z += w[i] * x[i]</span><br><span class="line">z += b</span><br></pre></td></tr></table></figure></li><li><p>而如果是项量化的操作，代码则会简介很多，并带来近百倍性能的提升(并行指令):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></li><li><p>不用显示 for 循环，实现逻辑回归的梯度下降一次迭代 (对应之前蓝色代码的for循环部分，这里公式和 Numpy 的代码混杂，注意分辨)：</p><script type="math/tex; mode=display">\begin {align}Z &= w^TX+b=np.dot(w.T,x)+b\\A &= \sigma(Z)\\dZ &= A-Y\\dw &= \frac{1}{m}XdZ^T\\db &= \frac{1}{m}np.sum(dZ)\\w &:= w-\sigma dw\\b &:= b -\sigma db\end {align}</script></li></ul><h3 id="Python-中的广播机制-Broadcasting-in-Python"><a href="#Python-中的广播机制-Broadcasting-in-Python" class="headerlink" title="Python 中的广播机制 (Broadcasting in Python)"></a>Python 中的广播机制 (Broadcasting in Python)</h3><p><strong>numpy 广播机制</strong>：</p><ul><li><strong><font color="#0099ff">如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的，广播会在缺失维度和轴长度为1的维度上进行</font></strong></li></ul><p><strong>三种常见的广播形式：</strong></p><ul><li>矩阵 $A_{m,n}$ 和矩阵 $B_{1,n}$ 进行四则运算:<ul><li>后缘维度轴长度相符，可以广播</li><li>广播沿着轴长度为1的轴进行，即 $B_{1,n}$ 广播成为 $B_{m,n}^{\prime}$, 然后进行四则运算</li></ul></li><li>矩阵 $A_{m,n}$ 和矩阵 $B_{m,1}$ 进行四则运算:<ul><li>后缘维度轴长度不相符，但一方轴长度为1，可以广播</li><li>广播沿着轴长度为1的轴进行，即 $B_{m,1}$ 广播成为 $B_{m,n}^{\prime}$, 然后进行四则运算</li></ul></li><li>矩阵 $A_{m,1}$ 和常数 $R$ 进行四则运算:<ul><li>后缘维度轴长度不相符，但其中一方轴长度为1，可以广播</li><li>广播沿着缺失维度和轴长度为1的轴进行，即将 $R$ 广播成 $B_{m,1}^{\prime}$, 然后进行四则运算</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（2） 神经网络基础&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述</title>
    <link href="http://sunfeng.online/2019/08/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"/>
    <id>http://sunfeng.online/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/</id>
    <published>2019-08-30T08:28:59.000Z</published>
    <updated>2019-09-05T11:32:02.690Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（1） 深度学习概述</font></strong></p><a id="more"></a><hr><h3 id="什么是神经网络？-What-is-a-Neural-Network"><a href="#什么是神经网络？-What-is-a-Neural-Network" class="headerlink" title="什么是神经网络？(What is a Neural Network ?)"></a>什么是神经网络？(What is a Neural Network ?)</h3><p><strong>深度学习：</strong></p><ul><li>指训练神经网络的过程</li><li>有时候特指训练大规模的神经网络</li></ul><p><strong>神经网络：</strong> 一个房价预测的例子</p><p><img src="/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/figure1.PNG" alt="figure1"></p><h3 id="神经网络的监督学习-Supervised-Learning-with-Neural-Network"><a href="#神经网络的监督学习-Supervised-Learning-with-Neural-Network" class="headerlink" title="神经网络的监督学习 (Supervised Learning with Neural Network)"></a>神经网络的监督学习 (Supervised Learning with Neural Network)</h3><p><strong>常见神经网络的监督学习案例：</strong></p><p><img src="/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/figure2.PNG" alt="figure2"></p><h3 id="为什么深度学习会兴起？-Why-the-Deep-Learning-taking-off"><a href="#为什么深度学习会兴起？-Why-the-Deep-Learning-taking-off" class="headerlink" title="为什么深度学习会兴起？(Why the Deep Learning taking off ?)"></a>为什么深度学习会兴起？(Why the Deep Learning taking off ?)</h3><ul><li><strong>数据</strong> (Data)</li><li><strong>计算能力</strong> (Computation)</li><li><strong>算法</strong> (Algorithm)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（1） 深度学习概述&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)--10个测试题</title>
    <link href="http://sunfeng.online/2019/08/26/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%B8%80%E5%91%A8(Introduction%20to%20Deep%20Learning)--10%E4%B8%AA%E6%B5%8B%E8%AF%95%E9%A2%98/"/>
    <id>http://sunfeng.online/2019/08/26/课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)--10个测试题/</id>
    <published>2019-08-26T12:48:46.000Z</published>
    <updated>2019-08-30T09:11:33.537Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)—10个测试题</font></strong></p><a id="more"></a><p>1、What does the analogy “AI is the new electricity” refer to?  (B)</p><p>A. Through the “smart grid”, AI is delivering a new wave of electricity.</p><p>B. Similar to electricity starting about 100 years ago, AI is transforming multiple industries.</p><p>C. AI is powering personal devices in our homes and offices, similar to electricity.</p><p>D. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.</p><p>2、Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)  (A、B、D)</p><p>A. We have access to a lot more data.</p><p>B. We have access to a lot more computational power.</p><p>C. Neural Networks are a brand new field.</p><p>D. Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition.</p><p>3、Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.) (A、B、D)</p><p>A. Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.</p><p>B. Faster computation can help speed up how long a team takes to iterate to a good idea.</p><p>C. It is faster to train on a big dataset than a small dataset.</p><p>D. Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).</p><p>4、When an experienced deep learning engineer works on a new problem, they can usually use insight from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False?  (B)</p><p>A. True</p><p>B. False</p><p>5、Which one of these plots represents a ReLU activation function? (C)</p><p>A. Figure 1:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152503347-1898711678.png" alt="img"></p><p>B. Figure 2:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152527300-994610622.png" alt="img"></p><p>C. Figure 3:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152536956-442890691.png" alt="img"></p><p>D. Figure4</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152603315-902589773.png" alt="img"></p><p>6、Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False? (B)</p><p>A. True</p><p>B. False</p><p>7、A demographic dataset with statistics on different cities’ population, GDP per capita, economic growth is an example of “unstructured” data because it contains data coming from different sources. True/False?(B)</p><p>A. True</p><p>B. False</p><p>8、Why is an RNN (Recurrent Neural Network) used for machine translation, say translating English to French? (Check all that apply.) (A、C)</p><p>A. It can be trained as a supervised learning problem.</p><p>B. It is strictly more powerful than a Convolutional Neural Network (CNN).</p><p>C. It is applicable when the input/output is a sequence (e.g., a sequence of words).</p><p>D. RNNs represent the recurrent process of Idea-&gt;Code-&gt;Experiment-&gt;Idea-&gt;….</p><p>9、In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent? (A)</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152854394-1385682683.png" alt="img"></p><p>A.</p><p>x-axis is the amount of data<br>y-axis (vertical axis) is the performance of the algorithm.</p><p>B.</p><p>x-axis is the performance of the algorithm<br>y-axis (vertical axis) is the amount of data.</p><p>C.</p><p>x-axis is the amount of data<br>y-axis is the size of the model you train.</p><p>D.</p><p>x-axis is the input to the algorithm<br>y-axis is outputs.</p><p>10、Assuming the trends described in the previous question’s figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.) (A、C)</p><p>A. Increasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>B. Decreasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>C. Increasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>D. Decreasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)—10个测试题&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日乘子法(Lagrange Multiplier)和KKT条件</title>
    <link href="http://sunfeng.online/2019/08/26/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95(Lagrange%20Multiplier)%E5%92%8CKKT%E6%9D%A1%E4%BB%B6/"/>
    <id>http://sunfeng.online/2019/08/26/拉格朗日乘子法(Lagrange Multiplier)和KKT条件/</id>
    <published>2019-08-26T08:28:13.000Z</published>
    <updated>2019-08-30T09:12:15.950Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">深入理解拉格朗日乘子法和KKT条件</font></strong></p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>在求解最优化问题中，<strong>拉格朗日乘子法</strong> (Lagrange Multiplier) 和 <strong>KKT</strong> (Karush Kuhn Tucker) <strong>条件</strong> 是两种常用的方法。</li><li>在有等式约束时使用拉格朗日乘子法</li><li>在有不等式约束时使用KKT条件</li></ul><hr><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><p>&ensp;&ensp; 我们这里提到的 <font color="#0099ff">最优化问题是指对于给定的某一函数，求其在指定作用域上的全局最小值 (因为最小值与最大值可以很容易转化，即最大值问题可以转化为最小值问题)。</font></p><p>&ensp;&ensp; 一般情况下，最优化问题可分为以下三种情况：</p><h3 id="无约束条件"><a href="#无约束条件" class="headerlink" title="无约束条件"></a>无约束条件</h3><ul><li>这是最简单的情况，解决方法通常是函数对变量求导，令求导函数等于0的点可能是极值点。将极值点带回原函数进行验证即可</li></ul><h3 id="等式约束条件"><a href="#等式约束条件" class="headerlink" title="等式约束条件"></a>等式约束条件</h3><p><strong>问题描述</strong></p><ul><li><p>设目标函数为 $f(x)$</p></li><li><p>约束条件为 $h_k(x)$</p></li><li><p>求：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\min}_{x}f(x)\\&s.t\ \ h_k(x) = 0\ \ k=1,2,...,l\end {align}\tag{1}</script><p>此问题的解决方法是 <font color="#0099ff">消元法或拉格朗日法</font></p></li></ul><p><strong>拉格朗日乘子法</strong></p><p>&ensp;&ensp;<font color="#0099ff">拉格朗日乘子法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法</font>。</p><p>&ensp;&ensp;<font color="#0099ff">这种方法可以将一个拥有 n 个变量与 k 个约束条件的最优化问题转换为一个求解有 n+k 个变量的方程组的解的问题。</font></p><ul><li><p>首先定义拉格朗日函数：</p><script type="math/tex; mode=display">F(x,\lambda) = f(x)+\sum_{k=1}^l\lambda_kh_k(x)\tag{2}</script><p>其中，$\lambda_k$ 是各个约束条件的待定系数, 称作 <strong>拉格朗日乘数</strong></p></li><li><p>然后解变量的偏导数方程：</p><script type="math/tex; mode=display">\begin {align}\frac{\partial F}{\partial x} &= 0 \\\frac{\partial F}{\partial \lambda_1} &= 0\\&\vdots\\\frac{\partial F}{\partial \lambda_l} &= 0\end {align}\tag{3}</script><p>如果有 $l$ 个约束条件，则有 $l+1$ 个方程。求出方程的解就可能是最优化解。</p></li></ul><p><strong>一个例子</strong></p><ul><li><p>给定一个椭球：</p><script type="math/tex; mode=display">\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1\tag{4}</script></li><li><p>求这个椭球的内接长方体的最大体积</p></li><li><p>此问题实际上就是条件极值问题，即在条件 $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ 求 $f(x,y,z) = 8xyz$ 的最大值</p></li><li><p>用拉格朗日法求解该问题：</p><script type="math/tex; mode=display">\begin {align}F(x,y,z,\lambda) &= f(x,y,z)+\lambda\varphi(x,y,z)\\&= 8xyz+\lambda\left(\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1 \right)\end {align}\tag{5}</script></li><li><p>对 $F(x,y,z,\lambda)$ 求偏导得到：</p><script type="math/tex; mode=display">\begin {align}\frac{\partial F(x,y,z,\lambda)}{\partial x} &= 8yz+\frac{2\lambda x}{a^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial y} &= 8xz+\frac{2\lambda y}{b^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial z} &= 8xy+\frac{2\lambda z}{a^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial \lambda} &= \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1 = 0\end {align}\tag{6}</script></li><li><p>求解方程 (6) 可得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}x = \frac{\sqrt 3}{3}a\\y = \frac{\sqrt 3}{3}b\\z = \frac{\sqrt 3}{3}c\end {cases}\end {equation}\tag{7}</script></li><li><p>带入解得最大体积为：</p><script type="math/tex; mode=display">V_{max} = f\left(\frac{\sqrt 3}{3}a, \frac{\sqrt 3}{3}b, \frac{\sqrt 3}{3}c\right)= \frac{8\sqrt 3}{9}abc\tag{8}</script></li></ul><p><strong>直观解释</strong></p><p>&ensp;&ensp;举个二维最优化的例子：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\min}_{x,y}f(x,y)\\&s.t\ \ g(x,y) = c\end {align}\tag{9}下图是 $z = f(x,y)$ 的等高线：</script><p>&ensp;下图是 $z = f(x,y)$ 的等高线：</p><p><img src="https://pic002.cnblogs.com/images/2012/103496/2012101621500549.png" alt="img"></p><p>&ensp;&ensp;绿线是约束条件 $g(x,y) = c$ 的轨迹；蓝线是 $f(x,y)$ 的等高线；箭头表示斜率，和等高线的发现平行。从梯度方向上来看，显然 $d_1 &gt; d_2$</p><p>&ensp;&ensp;<font color="#0099">绿色的线表示约束，也就是说，只要正好落在这条绿线上的点才可能是满足要求的点。如果没有这条约束，f(x,y)的最小值应该会是会落在最小那圈等高线内部的某一点上。</font></p><p>&ensp;&ensp;<font color="#099ff">而现在加上了约束条件，最小值点应该是 f(x,y)等高线和约束条件相切的位置，因为如果只是相交意味着肯定还存在其他等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小</font></p><p>&ensp;&ensp;<font color="#ff0000">只有到等高线与约束条件的函数曲线相切的时候，才可能取到最优值</font></p><p>&ensp;&ensp;如果我们对约束也求梯度 $\nabla g(x,y)$, 则其梯度如图中绿色箭头所示；<strong>显然，当目标函数 $f(x,y)$ 的等高线和约束相切，则它们切点的梯度一定在一条直线上($f$ 和 $g$ 的斜率平行)</strong></p><p>&ensp;&ensp;即在最优解的时候: $\nabla f(x,y) = \lambda(\nabla g(x,y) - c)$, 即 $\nabla[f(x,y) + \lambda(g(x,y)-c) ] = 0$</p><p>&ensp;&ensp;那么拉格朗日函数： $F(x,y) = f(x,y)+\lambda(g(x,y)-c)$ 在达到极值时与 $f(x,y)$ 相等，因为 $g(x,y)-c$ 总为0</p><h3 id="不等式约束条件"><a href="#不等式约束条件" class="headerlink" title="不等式约束条件"></a>不等式约束条件</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;深入理解拉格朗日乘子法和KKT条件&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数学方法整理" scheme="http://sunfeng.online/categories/%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86/"/>
    
    
      <category term="SVM" scheme="http://sunfeng.online/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（十）-- 降维与度量学习</title>
    <link href="http://sunfeng.online/2019/08/22/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89--%20%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://sunfeng.online/2019/08/22/《机器学习》西瓜书学习笔记（十）-- 降维与度量学习/</id>
    <published>2019-08-22T07:49:17.000Z</published>
    <updated>2019-08-23T08:11:35.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（十）降维与度量学习"><a href="#《机器学习》西瓜书学习笔记（十）降维与度量学习" class="headerlink" title="《机器学习》西瓜书学习笔记（十）降维与度量学习"></a>《机器学习》西瓜书学习笔记（十）降维与度量学习</h2><a id="more"></a><h3 id="k-近邻学习"><a href="#k-近邻学习" class="headerlink" title="k 近邻学习"></a>k 近邻学习</h3><p><strong>k 近邻简介：</strong></p><ul><li>k 近邻 (k-Nearest Neighbor, 简称KNN)</li><li>一种常用的监督学习方法</li></ul><p><strong>k 近邻思想：</strong></p><ul><li>给定测试样本，基于某种距离度量找出训练集中与其最靠近的 $k$ 个训练样本</li><li>然后基于这 $k$ 个邻居的信息来进行预测</li><li>在分类任务中，可采用 “投票法”<ul><li>即选择 $k$ 个样本中出现最多的类别标记作为预测结果</li></ul></li><li>在回归任务中，可采用 “平均法”<ul><li>即将这 $k$ 个样本的实值输出标记的平均值作为预测结果</li></ul></li></ul><p><strong>k 近邻特点：</strong> </p><ul><li>没有显式的训练过程</li></ul><hr><h3 id="低维嵌入"><a href="#低维嵌入" class="headerlink" title="低维嵌入"></a>低维嵌入</h3><h4 id="维数灾难-curse-of-dimensionality"><a href="#维数灾难-curse-of-dimensionality" class="headerlink" title="维数灾难 (curse of dimensionality)"></a>维数灾难 (curse of dimensionality)</h4><ul><li>在高维情形下出现的数据样本稀疏、距离计算困难等问题</li></ul><h4 id="降维-dimension-reduction"><a href="#降维-dimension-reduction" class="headerlink" title="降维 (dimension reduction)"></a>降维 (dimension reduction)</h4><ul><li>缓解维数灾难的一个重要途径</li><li>通过某种数学变换将原始高维属性空间转变为一个低维 “子空间” </li><li>在子空间中样本密度大幅提高，距离计算也变得更为容易</li></ul><h4 id="多维缩放-Multiple-Dimensional-Scaling，简称-MDS"><a href="#多维缩放-Multiple-Dimensional-Scaling，简称-MDS" class="headerlink" title="多维缩放 (Multiple Dimensional Scaling，简称 MDS)"></a>多维缩放 (Multiple Dimensional Scaling，简称 MDS)</h4><ul><li><p>设 $m$ 个样本在原始空间的距离矩阵为 $\boldsymbol D\in {\Bbb R}^{m\times m}$</p></li><li><p>其第 $i$ 行 $j$ 列元素 $dist_{ij}$ 为样本 $\boldsymbol x_i$ 到 $\boldsymbol x_j$ 的距离</p></li><li><p>目标是获得样本在 $d\prime$ 维空间的表示 $\boldsymbol Z\in {\Bbb R}^{d\prime\times m}$, $d\prime \le d$</p></li><li><p>且任意两个样本在 $d\prime$ 维空间中欧式距离等于在原始空间中的距离，即 $\left|z_i-z_j \right| = dist_{ij}$</p></li><li><p>令 $\boldsymbol B$ 为降维后样本的内积矩阵，即 $\boldsymbol B = \boldsymbol Z^T\boldsymbol Z \in {\Bbb R}^{m\times m}$,  其中，$b_{ij} = z_i^Tz_j$</p></li><li><p>则：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}dist_{ij}^2 &= \left\| z_i\right\|^2 +\left\| z_j\right\|^2 -2z_i^Tz_j\\&=b_{ii} + b_{jj}-2b_{ij}\end{split}\tag{1}\end{equation}</script></li><li></li><li><p>则矩阵 $\boldsymbol B$ 的行和列之和为零：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}\sum_{i=1}^{m} b_{ij} &= \sum_{i=1}^{m}(z_i^Tz_j) \\&=z_j\sum_{i=1}^{m}z_i^T\\&= 0\\\sum_{j=1}^{m} b_{ij} &= \sum_{j=1}^{m}(z_i^Tz_j) \\&= z_i\sum_{j=1}^{m}z_j^T\\&= 0\end {split}\end {equation}</script></li><li><p>易知：</p><script type="math/tex; mode=display">\sum_{i=1}^{m}dist_{ij}^2 = \text tr(\boldsymbol B) + mb_{jj}\tag{2}</script><script type="math/tex; mode=display">\sum_{j=1}^{m}dist_{ij}^2 = \text tr(\boldsymbol B) + mb_{ii}\tag{3}</script><script type="math/tex; mode=display">\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 = 2m \ \text tr(\boldsymbol B)\tag{4}</script><p>其中，$\text{tr} (\cdot)$ 表示矩阵的迹，$\text{tr} = \sum_{i=1}^{m}{\left|z_i \right|}$</p></li><li><p>令：</p><script type="math/tex; mode=display">\begin{align}dist_{i \cdot}^2 &= \frac{1}{m}\sum_{j=1}^{m}dist_{ij}^2 \tag{5} \\dist_{\cdot j}^2 &= \frac{1}{m}\sum_{i=1}^{m}dist_{ij}^2 \tag{6} \\dist_{\cdot \cdot}^2 &= \frac{1}{m^2}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 \tag{7}\\\end{align}</script></li><li><p>由式(1)和(2)~(7)可得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)\tag{8}</script><p>推导过程如下：</p><p>由式(1)得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}-b_{ii}-b_{jj})</script><p>由式(4)和(7)得：</p><script type="math/tex; mode=display">\begin{align}\text tr(\boldsymbol B) &= \frac{1}{2m}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2\\&=\frac{1}{2m} \cdot m^2 dist_{\cdot \cdot}^2 \\&=\frac{m}{2}dist_{\cdot \cdot}^2\end{align}</script><p>由式(2)和(6)得：</p><script type="math/tex; mode=display">\begin{align}b_{jj} &= \frac{1}{m}\sum_{i=1}^{m}dist_{ij}^2 - \frac{1}{m}\text tr(\boldsymbol B)\\&=dist_{\cdot j}^2 - \frac{1}{2}dist_{\cdot \cdot}^2\end{align}</script><p>由式(3)和(5)得:</p><script type="math/tex; mode=display">\begin{align}b_{jj} &= \frac{1}{m}\sum_{j=1}^{m}dist_{ij}^2 - \frac{1}{m}\text tr(\boldsymbol B)\\&=dist_{i\cdot}^2 - \frac{1}{2}dist_{\cdot \cdot}^2\end{align}</script><p>综上得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)</script></li><li><p><strong>由此可通过降维前后保持不变的距离矩阵 $\boldsymbol D$ 求得内积矩阵 $\boldsymbol B$ </strong></p></li><li><p>对矩阵 $\boldsymbol B$ 做特征值分解 $\boldsymbol B = \boldsymbol V \boldsymbol \Lambda \boldsymbol V^T$</p></li><li><p>其中 $\Lambda = \text{diag}(\lambda_1,\lambda_2,….,\lambda_d)$ 为特征值构成的特征矩阵，$\lambda_1\ge\lambda_2\ge…\ge\lambda_d$ </p></li><li><p>$\boldsymbol V $ 为特征向量矩阵</p></li><li><p>取 $d\prime &lt;&lt; d$ 个最大特征值构成对角矩阵 $\tilde{\Lambda} = \text{diag}(\lambda_1,\lambda_2,….,\lambda_{d\prime})$ </p></li><li><p>令 $\tilde{\boldsymbol V }$ 为相应的特征向量矩阵</p></li><li><p>则 $\boldsymbol Z$ 可表达为:</p><script type="math/tex; mode=display">\boldsymbol Z = \tilde{\Lambda}^{\frac{1}{2}}\tilde{\boldsymbol V }^T\ \in {\Bbb R}^{d\prime \times m}\tag{9}</script></li></ul><p><strong>MDS 算法描述：</strong></p><p><img src="/2019/08/22/《机器学习》西瓜书学习笔记（十）-- 降维与度量学习/figure1.PNG" alt="figure1"></p><h4 id="线性降维"><a href="#线性降维" class="headerlink" title="线性降维"></a>线性降维</h4><ul><li><p>对原始高维空间进行线性变换，得到低维子空间</p></li><li><p>给定 $d$ 维空间中的样本 $\boldsymbol X = (\boldsymbol x_1, \boldsymbol x_2,…, \boldsymbol x_m ) \in {\Bbb R}^{d\times m}$</p></li><li><p>变换之后得到 $d\prime \le d$ 维空间中的样本</p><script type="math/tex; mode=display">\boldsymbol Z = \boldsymbol W^T\boldsymbol X\tag{10}</script></li><li><p>$\boldsymbol W \in {\Bbb R}^{d\times d\prime}$ 是变换矩阵</p></li><li><p>$\boldsymbol Z \in {\Bbb R}^{d\times m}$ 是样本在新空间中的表达</p></li></ul><hr><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><hr><h3 id="核化线性降维"><a href="#核化线性降维" class="headerlink" title="核化线性降维"></a>核化线性降维</h3><hr><h3 id="流形学习"><a href="#流形学习" class="headerlink" title="流形学习"></a>流形学习</h3><h4 id="等度量映射"><a href="#等度量映射" class="headerlink" title="等度量映射"></a>等度量映射</h4><h4 id="局部线性嵌入"><a href="#局部线性嵌入" class="headerlink" title="局部线性嵌入"></a>局部线性嵌入</h4><hr><h3 id="度量学习"><a href="#度量学习" class="headerlink" title="度量学习"></a>度量学习</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（十）降维与度量学习&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（十）降维与度量学习&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（十）降维与度量学习&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（十）降维与度量学习&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（九）-- 聚类</title>
    <link href="http://sunfeng.online/2019/08/19/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89--%20%E8%81%9A%E7%B1%BB/"/>
    <id>http://sunfeng.online/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/</id>
    <published>2019-08-19T08:41:05.000Z</published>
    <updated>2019-08-22T07:44:09.400Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（九）-聚类"><a href="#《机器学习》西瓜书学习笔记（九）-聚类" class="headerlink" title="《机器学习》西瓜书学习笔记（九） 聚类"></a>《机器学习》西瓜书学习笔记（九） 聚类</h2><a id="more"></a><h3 id="聚类任务"><a href="#聚类任务" class="headerlink" title="聚类任务"></a>聚类任务</h3><p><strong>聚类</strong> (clustering)</p><ul><li>无监督学习 (unsupervised learning) 的一种</li><li>试图将数据划分为若干个不相交的子集，每个子集为一个 <strong>簇</strong> (cluster)</li><li>每个簇可能对应于一些潜在的概念(类别)</li></ul><p><strong>聚类的形式化解释</strong></p><ul><li>样本集 $D = \{\boldsymbol x_1,  \boldsymbol x_2,…,\boldsymbol x_m\}$ 包含 $m$ 个样本</li><li>每个样本 $\boldsymbol x_i = (x_{i1}; x_{i2}; …;x_{in})$ 是一个 $n$ 维向量</li><li>聚类算法将样本 $D$ 划分为 $k$ 个不相交的簇 $\{C_l  |  l = 1,2,…,k \}$, 其中 $C_i \bigcap C_j = \emptyset$, $  (i \ne j)$ 且 $D = \bigcup_{l=1}^{k}C_l$</li><li>用 $\lambda_j \in \{1,2,…,k \}$ 表示样本 $\boldsymbol x_j$ 的 “簇标记”，即 $\boldsymbol x_j \in C_{\lambda_j}$ </li><li>聚类结果 $\boldsymbol \lambda = \{\lambda_1;\lambda_2;…;\lambda_m\}$</li></ul><hr><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><h4 id="外部指标-external-index"><a href="#外部指标-external-index" class="headerlink" title="外部指标 (external index)"></a>外部指标 (external index)</h4><p><strong>变量符号定义：</strong></p><ul><li><p>数据集 $D = \{\boldsymbol x_1,  \boldsymbol x_2,…,\boldsymbol x_m\}$</p></li><li><p>聚类的簇划分 ${\cal C} = \{C_1,C_2,…, C_k\}$，簇标记向量 $\boldsymbol \lambda$  </p></li><li><p>参考模型给出的簇划分 ${\cal C^{*}} = \{C_1^{*}, C_2^{*},…, C_k^{*}\}$，簇标记向量 $\boldsymbol \lambda^{*}$</p></li><li><p>定义：</p><script type="math/tex; mode=display">a = |SS|,\ \ SS=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i = \lambda_j,\lambda_i^* = \lambda_j^*, i < j\} \tag{1}</script><script type="math/tex; mode=display">b = |SD|,\ \ SD=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i = \lambda_j,\lambda_i^* \ne \lambda_j^*, i < j\} \tag{2}</script><script type="math/tex; mode=display">c = |DS|,\ \ DS=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i \ne \lambda_j,\lambda_i^* = \lambda_j^*, i < j\} \tag{3}</script><script type="math/tex; mode=display">d = |DD|,\ \ DD=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i \ne \lambda_j,\lambda_i^* \ne \lambda_j^*, i < j\} \tag{4}</script></li></ul><p>  其中：</p><p>  集合 $SS$ 包含了在 $C$ 中隶属于相同簇且在 $C^*$ 中也隶属于相同簇的样本对</p><p>  集合 $SD$ 包含了在 $C$ 中隶属于相同簇但在 $C^*$ 中隶属于不相同簇的样本对</p><p>  ……</p><p><strong>常用外部指标：</strong></p><ul><li><p>Jaccard 系数 (Jaccard Coefficient, 简称 JC)</p><script type="math/tex; mode=display">JC = \frac{a}{a+b+c}\tag{5}</script></li><li><p>FM 指数 (Fowlkes and Mallows Index, 简称FMI)</p><script type="math/tex; mode=display">FMI = \sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}\tag{6}</script></li><li><p>Rand 指数 (Rand Index, 简称 RI)</p><script type="math/tex; mode=display">RI = \frac{2(a+d)}{m(m-1)}\tag{7}</script></li></ul><p><strong>度量方法：</strong><font color="#009f">上述性能度量的结果值均在 [0,1] 区间，值越大越好</font></p><h4 id="内部指标-internal-index"><a href="#内部指标-internal-index" class="headerlink" title="内部指标 (internal index)"></a>内部指标 (internal index)</h4><p><strong>变量符号定义：</strong></p><ul><li><p>$avg(C)$: 簇 $C$ 内样本间的平均距离</p><script type="math/tex; mode=display">avg(C) = \frac{2}{|C|(|C|-1)}\sum_{1\le i <j\le|C|}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{8}</script><p>其中，$dist(\cdot,\cdot)$ 用于计算两个样本间的距离</p></li><li><p>$diam(C)$: 簇 $C$ 内样本间的最远距离</p><script type="math/tex; mode=display">diam(C) = \mathop{\max}_{1\le i <j\le|C|}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{9}</script></li><li><p>$d_{min}(C_i,C_j)$: 簇 $C_i$ 与 $C_j$ 最近样本间的距离</p><script type="math/tex; mode=display">d_{min}(C_i,C_j) = \mathop{\min}_{\boldsymbol x_i \in C_i,\boldsymbol x_j \in C_j}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{10}</script></li><li><p>$d_{cen}(C_i,C_j)$: 簇 $C_i$ 与 $C_j$ 中心点间的距离</p><script type="math/tex; mode=display">d_{cen}(C_i,C_j) = dist(\boldsymbol \mu_i,\boldsymbol \mu_j)\tag{11}</script><p>其中，$\boldsymbol \mu$ 代表簇 $C$ 的中心点 </p></li></ul><p><strong>常用内部指标：</strong></p><ul><li><p>DB指数 (简称 DBI)</p><script type="math/tex; mode=display">DBI = \frac{1}{k}\sum_{i=1}^{k}\mathop{\max}_{j\ne i}\left(\frac{avg(C_i)+avg(C_j)}{d_{cen}(C_i,C_j)} \right)\tag{12}</script></li><li><p>Dunn指数 (简称 DI)</p><script type="math/tex; mode=display">DI = \mathop{\min}_{1\le i \le k}\left\{\mathop {\min}_{j\ne i}\left(\frac{dmin(C_i,C_j)}{\mathop{\max}_{1\le l\le k}diam(C_l)} \right) \right\}</script></li></ul><p><strong>度量方法：</strong> <font color="#099f">DBI 的值越小越好，而 DI 的值越大越好</font></p><hr><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><h4 id="距离度量-distiance-measure"><a href="#距离度量-distiance-measure" class="headerlink" title="距离度量 (distiance measure)"></a>距离度量 (distiance measure)</h4><p><strong>距离度量函数性质：</strong></p><ul><li><p>非负性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) \ge 0 \tag{14}</script></li><li><p>同一性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) = 0 \ \text{当且仅当} \ \boldsymbol x_i=\boldsymbol x_j \tag{15}</script></li><li><p>对称性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) = dist(\boldsymbol x_j,\boldsymbol x_i)\tag{16}</script></li><li><p>直递性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) \le dist(\boldsymbol x_i,\boldsymbol x_k) + dist(\boldsymbol x_k,\boldsymbol x_j)\tag{17}</script></li></ul><h4 id="闵可夫斯基距离-Minkowski-distance"><a href="#闵可夫斯基距离-Minkowski-distance" class="headerlink" title="闵可夫斯基距离 (Minkowski distance)"></a>闵可夫斯基距离 (Minkowski distance)</h4><p><strong>闵可夫斯基距离：</strong></p><ul><li><font color="#0099ff">用于有序属性</font></li><li><p>样本$\boldsymbol x_i$:  $\boldsymbol x_i = (x_{i1}, x_{i2},…,x_{in})$</p></li><li><p>样本$\boldsymbol x_j$:  $\boldsymbol x_j = (x_{j1}, x_{j2},…,x_{jn})$</p></li><li><p>闵可夫斯基距离:</p><script type="math/tex; mode=display">\text{dist}_\text{mk}(\boldsymbol x_i,\boldsymbol x_j) = \left(\sum_{u=1}^{n}{|x_{iu}-x_{ju} |^p} \right)^{\frac{1}{p}}\tag{18}</script><p>其中， $p\ge1$.</p></li></ul><p><strong>欧式距离 (Euclidean distance)：</strong></p><ul><li>当 $p =2$ 时，闵可夫斯基距离即为欧式距离：<script type="math/tex; mode=display">\text{dist}_\text{ed}(\boldsymbol x_i,\boldsymbol x_j) = \left\|\boldsymbol x_i-\boldsymbol x_j \right\|_2 = \sqrt{\sum_{u=1}^{n}{|x_{iu}-x_{ju} |^2}}\tag{19}</script></li></ul><p><strong>曼哈顿距离 (Manhattan distance)：</strong></p><ul><li>当 $p =1$ 时，闵可夫斯基距离即为曼哈顿距离：<script type="math/tex; mode=display">\text{dist}_\text{man}(\boldsymbol x_i,\boldsymbol x_j) = \left\|\boldsymbol x_i-\boldsymbol x_j \right\|_1 = \sum_{u=1}^{n}{|x_{iu}-x_{ju} |}\tag{20}</script></li></ul><h4 id="VDM-Value-Difference-Metric-距离"><a href="#VDM-Value-Difference-Metric-距离" class="headerlink" title="VDM (Value Difference Metric) 距离"></a>VDM (Value Difference Metric) 距离</h4><p><strong>VDM</strong></p><ul><li><font color="#0099ff">用于无序属性</font></li><li><p>$m_{u,a}$: 在属性 $u$ 上取值为 $a$ 的样本数</p></li><li><p>$m_{u,a,i}$: 在第 $i$ 个样本簇中在属性 $u$ 上取值为 $a$ 的样本数</p></li><li><p>$k$: 样本簇数</p></li><li><p>$\text{VDM}_p(a,b)$: 属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 $\text{VDM}$ 距离:</p><script type="math/tex; mode=display">\text{VDM}_p(a,b) = \sum_{i=1}^{k}{\left|\frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}}\right|} \tag{21}</script></li></ul><p><strong>闵可夫斯基距离和 VDM 结合处理混合属性：</strong></p><ul><li>假定有 $n_c$ 个有序属性</li><li>假定有 $n-n_c$ 个无序属性</li></ul><script type="math/tex; mode=display">\text{MinkovDM}_p(\boldsymbol x_i,\boldsymbol x_j) = \left(\sum_{u=1}^{n_c}{|x_{iu}-x_{ju} |^p} + \sum_{u=n_c+1}^{n} \text{VDM}_p(x_{iu} ,x_{ju}) \right)^{\frac{1}{p}}\tag{22}</script><hr><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><h4 id="K-均值聚类"><a href="#K-均值聚类" class="headerlink" title="K 均值聚类"></a>K 均值聚类</h4><font color="#099ff">K 均值算法也称为 K-Means 算法</font><p><strong>K-Means 算法思想:</strong> </p><ul><li><p>给定样本集 $D = \{\boldsymbol x_1, \boldsymbol x_2, …, \boldsymbol x_m \}$</p></li><li><p>针对聚类所得的簇划分 ${\cal C} = \{C_1,C_2,…,C_k\}$ 最小化平方误差：</p><script type="math/tex; mode=display">E = \sum_{i=1}^{k}\sum_{\boldsymbol x \in C_i}\left\| \boldsymbol x - \boldsymbol\mu_i \right\|_2^2\tag{24}</script><p>其中 $\boldsymbol \mu_i = \frac{1}{|C_i|}{\sum_{\boldsymbol x \in C_i}\boldsymbol x}$ 是簇 $C_i$ 的均值向量</p></li></ul><p><strong>K-Means 算法流程：</strong></p><ul><li>初始化均值向量 (第1行)</li><li>对当前簇划分进行迭代更新 (第4~8行)</li><li>对当前簇的均值向量进行迭代更新 (第9~16行)</li><li>若迭代更新后聚类结果保持不变，则返回当前簇划分结果 (第17~18行)</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure1.PNG" alt="figure1"></p><h4 id="学习向量量化"><a href="#学习向量量化" class="headerlink" title="学习向量量化"></a>学习向量量化</h4><font color="#099ff">学习向量量化  (Learning Vector Quantization, 简称 LVQ) 假设数据样本带有类别标记</font><p><strong>LVQ 算法思想：</strong></p><ul><li><p>给定样本集 $D = \{(\boldsymbol x_1, y_1), (\boldsymbol x_2,y_2), …, (\boldsymbol x_m,y_m) \}$</p></li><li><p>特征向量 $\boldsymbol x_j = \{x_{j1};x_{j2};…;x_{jn} \}$</p></li><li><p>类别标记 ${\cal Y} = \{y_1,y_2,…,y_m\}$</p></li><li><p>LVQ 试图学得一组 $n$ 维原型向量 $\{\boldsymbol p_1, \boldsymbol p_2, …,\boldsymbol p_q \}$ , 每个原型向量代表一个聚类簇，簇标记 $t_i \in \cal Y$</p></li></ul><p><strong>LVQ 算法流程:</strong></p><ul><li><p>初始化原型聚类 (第1行)</p></li><li><p>对原型向量进行迭代优化 (第2~12行)</p><ul><li><p>随机选取一个有标记的训练样本 $\boldsymbol x_j$ </p></li><li><p>找出与其距离最近的原型向量 $\boldsymbol p_{i^*}$ </p></li><li><p>若原型向量 $\boldsymbol p_{i^*}$ 与 $\boldsymbol x_j$ 的类别标记相同，则令 $\boldsymbol p_{i^*}$ 向 $\boldsymbol x_j$ 方向靠拢</p></li><li><p>此时，新原型向量为：</p><script type="math/tex; mode=display">\boldsymbol p\prime = \boldsymbol p_{i^*} +\eta\cdot(\boldsymbol x_j-\boldsymbol p_{i^*})\tag{25}</script><p>其中，$\eta\in(0,1)$ 为学习率</p></li><li><p>若原型向量 $\boldsymbol p_{i^*}$ 与 $\boldsymbol x_j$ 的类别标记不同，则令 $\boldsymbol p_{i^*}$ 从 $\boldsymbol x_j$ 方向远离</p></li><li><p>此时，新原型向量为：</p><script type="math/tex; mode=display">\boldsymbol p\prime = \boldsymbol p_{i^*} +\eta\cdot(\boldsymbol x_j-\boldsymbol p_{i^*})\tag{26}</script></li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure2.PNG" alt="figure2"></p><h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><hr><h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><h4 id="密度聚类-1"><a href="#密度聚类-1" class="headerlink" title="密度聚类"></a>密度聚类</h4><p><strong>基于密度的聚类 (density-based clustering)</strong></p><ul><li>简称密度聚类</li><li>此类算法假设聚类结构能通过样本分布的紧密程度确定</li><li>常见的算法：DBSCAN 算法</li></ul><h4 id="DBSCAN-算法"><a href="#DBSCAN-算法" class="headerlink" title="DBSCAN 算法"></a>DBSCAN 算法</h4><font color="#0099">DBSCAN 基于一组 "领域" 参数 </font> $(\epsilon, MinPts)$ <font color="#0099">来刻画样本分布的紧密程度</font><p><strong>DBSCAN 相关概念：</strong></p><ul><li><p><strong>$\epsilon$-邻域 </strong> </p><ul><li><p>对 $\boldsymbol x_j\in D$，其 $\epsilon$ -邻域包含样本集 $D$ 中与 $\boldsymbol x_j$ 距离不大于 $\epsilon$ 的样本，即 </p><p>$N_{\epsilon}(\boldsymbol x_j) = \{\boldsymbol x_j  | dist(\boldsymbol x_i,\boldsymbol x_j) \le \epsilon \}$</p></li></ul></li><li><p><strong>核心对象</strong> (core object)</p><ul><li>若 $\boldsymbol x_j$ 的 $\epsilon$-邻域内至少含有 $MinPts$ 个样本，即 $|N_{\epsilon}(\boldsymbol x_j)|\ge MinPts$, 则 $\boldsymbol x_j$ 是一个核心对象</li></ul></li><li><p><strong>密度直达</strong> (directly density-reachable)</p><ul><li>若 $\boldsymbol x_j$ 位于 $\boldsymbol x_i$ 的 $\epsilon$-邻域内，且 $\boldsymbol x_i$ 是核心对象，则称 $\boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度直达</li></ul></li><li><p><strong>密度可达</strong> (density-reachable)</p><ul><li>对 $\boldsymbol x_i$ 和 $\boldsymbol x_j$ , 若存在样本序列 $\boldsymbol p_1$, $\boldsymbol p_2$, …… ,$\boldsymbol p_n$, 其中 $\boldsymbol p_1 = \boldsymbol x_i$, $\boldsymbol p_n =\boldsymbol x_j$, 且 $\boldsymbol p_{i+1}$ 由 $\boldsymbol p_i$ 密度直达，则称 $\boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度可达</li></ul></li><li><p><strong>密度相连</strong> (density-connected)</p><ul><li>对 $\boldsymbol x_i$ 与 $\boldsymbol x_j$，若存在 $\boldsymbol x_k$ 使得 $\boldsymbol x_i$ 与 $\boldsymbol x_j$ 均由 $\boldsymbol x_k$ 密度可达，则 $\boldsymbol x_i$ 与 $\boldsymbol x_j$ 密度相连</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure4.PNG" alt="figure4"></p><p><strong>DBSCAN 算法思想：</strong></p><ul><li>DBSCAN 中 “簇” 的定义：由密度可达关系导出的最大密度相连的样本集合</li><li>给定领域参数 $(\epsilon, MinPts)$, 簇 $C \subseteq D$ 是满足以下性质的非空样本子集：<ul><li>连接性：$\boldsymbol x_i \in C, \boldsymbol x_j \in C\Rightarrow \boldsymbol x_i $ 与$ \boldsymbol x_j  $密度相连</li><li>最大性：$\boldsymbol x_i \in C, \boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度可达 $\Rightarrow \boldsymbol x_j \in C$</li></ul></li><li>如何生成簇：若 $\boldsymbol x$ 为核心对象，由 $\boldsymbol x$ 密度可达的所有样本组成的集合记为 $ X = \{\boldsymbol x\prime \in D  | \boldsymbol x\prime  {\text 由} \boldsymbol x  {\text 密度可达} \}$ ，则 $X$ 记为满足最大性和连接性的簇</li></ul><p><strong>DBSCAN 算法流程：</strong> </p><ul><li>任选数据集中的一个核心对象作为 “种子” (第1行)</li><li>根据给定的领域参数 $(\epsilon, MinPts)$ 找出所有核心对象 (第1~7行)</li><li>以任一核心对象为出发点，找出由其密度可达的样本生成的聚类簇，直至所有的对象都被访问过为止 (第10~24行)</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure3.PNG" alt="figure3"></p><hr><h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><h4 id="层次聚类-1"><a href="#层次聚类-1" class="headerlink" title="层次聚类"></a>层次聚类</h4><p><strong>层次聚类 (hierarchical clustering)</strong></p><ul><li>试图在不同层次对数据集进行划分，从而形成属性结构</li><li>数据集划分的策略：<ul><li>“自底向上”的聚合策略</li><li>“自顶向下”的分拆策略</li></ul></li></ul><h4 id="AGNES-算法"><a href="#AGNES-算法" class="headerlink" title="AGNES 算法"></a>AGNES 算法</h4><p><strong>AGNES 算法简介：</strong></p><ul><li>一种采用自底向上聚合策略的层次聚类算法</li></ul><p><strong>AGNES 算法思想：</strong></p><ul><li><p>先将数据集中的每个样本看作一个初始聚类簇</p></li><li><p>然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，其中两个簇间的距离计算方法如下：</p><ul><li><p><strong>最小距离：</strong></p><script type="math/tex; mode=display">d_{min}(C_i,C_j) = \mathop{\min}_{\boldsymbol x\in C_i,\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z)\tag{41}</script></li><li><p><strong>最大距离：</strong></p><script type="math/tex; mode=display">d_{max}(C_i,C_j) = \mathop{\max}_{\boldsymbol x\in C_i,\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z)\tag{42}</script></li><li><p><strong>平均距离：</strong> </p><script type="math/tex; mode=display">d_{avg}(C_i,C_j) = \frac{1}{|C_i||C_j|} \sum_{\boldsymbol x\in C_i}\sum_{\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z) \tag{43}</script></li></ul></li><li><p>将上述过程不断重复，直到达到预设的聚类簇个数</p></li></ul><p><strong>AGNES 算法流程：</strong></p><ul><li>对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化 (第1-9行)</li><li>不断合并距离最近的聚类簇，并对合并得到的聚类簇的聚类矩阵进行更新</li><li>上述过程不断重复，直到达到预设的聚类簇数</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（九）-聚类&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（九）-聚类&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（九） 聚类&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（九） 聚类&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（八）-- 集成学习</title>
    <link href="http://sunfeng.online/2019/08/19/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89--%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://sunfeng.online/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/</id>
    <published>2019-08-19T08:34:44.000Z</published>
    <updated>2019-08-26T02:22:28.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（八）集成学习"><a href="#《机器学习》西瓜书学习笔记（八）集成学习" class="headerlink" title="《机器学习》西瓜书学习笔记（八）集成学习"></a>《机器学习》西瓜书学习笔记（八）集成学习</h2><a id="more"></a><h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><h4 id="集成学习-ensemble-learning"><a href="#集成学习-ensemble-learning" class="headerlink" title="集成学习 (ensemble learning)"></a>集成学习 (ensemble learning)</h4><p><strong>集成学习的一般思路：</strong></p><ul><li>集成学习通过构建并结合多个学习器来完成学习任务</li></ul><p><strong>集成学习的一般结构：</strong></p><ul><li>先产生一组 “个体学习器”</li><li>在用某种策略将它们结合起来</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure1.PNG" alt="figure1"></p><p><strong>集成学习的集成分类：</strong></p><ul><li><strong>同质</strong> (homogeneous)<ul><li>集成中只包含同种类型的个体学习器</li></ul></li><li><strong>异质</strong> (heterogenous)<ul><li>集成中包含不同类型的个体学习器</li></ul></li></ul><p><strong>集成学习的基本原则：</strong>若要获得好的集成</p><ul><li>个体学习器应该 “好而不同”</li><li>即个体学习器要有一定的 “准确性”</li><li>即学习器不能太坏，并且要有 “多样性”，即学习器间有差异</li></ul><p><strong>集成学习的方法分类：</strong></p><ul><li><strong>序列化方法</strong><ul><li>个体学习器间存在强依赖关系、必须串行生成</li><li>代表：Boosting</li></ul></li><li><strong>并行化方法</strong><ul><li>个体学习器间不存在强依赖关系、可同时生成</li><li>代表：Bagging、随机森林</li></ul></li></ul><hr><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><h4 id="Boosting简介"><a href="#Boosting简介" class="headerlink" title="Boosting简介"></a>Boosting简介</h4><font color="#0099">Boosting 是一族可将弱学习器提升为强学习其的算法</font><p><strong>工作机制</strong>：</p><ul><li>先从初始训练集训练出一个基学习器</li><li>再根据基学习器的表现对训练样本分布进行调整</li><li>使得先前基学习器做错的训练样本在后续受到更多关注</li><li>然后再基于调整后的样本分布来训练下一个基学习器</li><li>如此重复，直到基学习器的数目达到指定值 $T$</li><li>再将 $T$ 个基学习器进行加权结合</li></ul><h4 id="AdaBoost-算法"><a href="#AdaBoost-算法" class="headerlink" title="AdaBoost 算法"></a>AdaBoost 算法</h4><font color="#0099">AdaBoost算法是Boosting 族算法的代表</font><p><strong>AdaBoost算法描述：</strong></p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure2.PNG" alt="figure2"></p><ul><li>其中：$y_i \in \{-1,+1 \}$, $f$ 是真实函数</li></ul><p><strong>AdaBoost 算法推导：</strong></p><hr><h3 id="Bagging-与随机森林"><a href="#Bagging-与随机森林" class="headerlink" title="Bagging 与随机森林"></a>Bagging 与随机森林</h3><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p><strong>Bagging算法简介：</strong></p><ul><li>Bagging 是并行式集成学习方法最著名的代表</li></ul><p><strong>Bagging算法流程：</strong></p><ul><li>基于自助采样法采样出 $T$ 个含有 $m$ 个样本的训练集</li><li>然后基于每个采样集训练出一个基学习器</li><li>再将这些基学习器结合</li><li><strong>在对预测输出进行结合时：</strong><ul><li>分类任务：简单投票法</li><li>回归任务：简单平均法</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure3.PNG" alt="figure3"></p><h4 id="随机森林-Random-Forrest-简称-RF"><a href="#随机森林-Random-Forrest-简称-RF" class="headerlink" title="随机森林 (Random Forrest, 简称 RF)"></a>随机森林 (Random Forrest, 简称 RF)</h4><p><strong>随机森林算法简介：</strong></p><ul><li>RF 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中加入随机属性的选择</li></ul><p><strong>随机森林算法流程：</strong></p><ul><li>在选择划分属性时，对决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集<ul><li>$k$ 控制了随机性的引入程度，其推荐值为 $k = \log_2d$ </li></ul></li><li>然后再从这个子集中选择一个最优属性用于划分</li></ul><hr><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p><strong>学习器结合的好处：</strong></p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure4.PNG" alt="figure4"></p><ul><li>假定集成包含 $T$ 个基学习器 $\{h_1,h_2,…,h_T \}$, 其中 $h_i$ 在示例 $\boldsymbol x$ 上的输出为 $h_i(\boldsymbol x)$</li></ul><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><p><strong>平均法是回归任务，即数值型输出 $h_i(\boldsymbol x) \in {\Bbb R}$, 常使用的结合策略</strong></p><ul><li><p><strong>简单平均法</strong> (simple average)</p><script type="math/tex; mode=display">H(\boldsymbol x) = \frac{1}{T}\sum_{i=1}^{T}h_i(\boldsymbol x)</script></li><li><p><strong>加权平均法</strong> (weighted average)</p><script type="math/tex; mode=display">H(\boldsymbol x) = \sum_{i=1}^{T}w_ih_i(\boldsymbol x)</script><p>其中 $w_i$ 为个体学习器 $h_i$ 的权重，<strong>权重一般需要从训练数据中学习而得</strong></p></li></ul><font color="#0099">在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法</font><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><p><strong>投票法是分类任务，即学习器 $h_i$ 将从类别标记集合 $\{c_1,c_2,…,c_N \}$ 中预测出一个标记，常用的结合策略</strong></p><p>假定 $h_i$ 在样本 $\boldsymbol x$ 上的预测输出为一个 $N$ 维向量 $(h_i^1(\boldsymbol x);h_i^2(\boldsymbol x);…;h_i^N(\boldsymbol x))$, 其中 $h_i^j(\boldsymbol x)$ 是 $h_i$ 在类别标记 $c_j$ 上的输出</p><ul><li><p><strong>绝对多数投票法</strong></p><script type="math/tex; mode=display">\begin {equation}H(\boldsymbol x) = \begin {cases}c_j \qquad &{\text if}\ \sum_{i=1}^{T}h_i^j(x)>0.5\sum_{k=1}^{T}\sum_{i=1}^{T}h_i^j(x)\\{\text reject} &{\text otherwise}\end {cases}\end {equation}</script><p>即，若某类标记得票过半数，则预测为该标记，否则拒绝预测</p></li><li><p><strong>相对多数投票法</strong></p><script type="math/tex; mode=display">H(\boldsymbol x) = c_{\mathop{\arg \max}_{j}\sum_{i=1}^{T}h_i^j(\boldsymbol x)}</script><p>即，预测为得票最多的标记</p></li><li><p><strong>加权投票法</strong></p><script type="math/tex; mode=display">H(\boldsymbol x) = c_{\mathop{\arg \max}_{j}\sum_{i=1}^{T}w_ih_i^j(\boldsymbol x)}</script><p>$w_i$ 是 $h_i$ 的权重</p></li></ul><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p><strong>学习法简介：</strong></p><ul><li>当训练数据很多时，一种更为强大的结合策略是使用 “学习法”</li><li>即通过另一个学习器来进行结合<ul><li>其中个体学习器称为初级学习器</li><li>用于结合的学习器称为次级学习器</li></ul></li><li>典型代表：Stacking</li></ul><p><strong>Stack算法流程：</strong></p><ul><li>先从初始训练集中训练出初级学习器</li><li>然后 “生成” 一个新数据集用于训练次级学习器<ul><li>在新数据集中，初级学习器的输出被当做样例输入特征</li><li>初始样本的标记仍被当做样例标记</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure5.PNG" alt="figure5"></p><hr><h3 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h3><h4 id="误差-分歧分解"><a href="#误差-分歧分解" class="headerlink" title="误差-分歧分解"></a>误差-分歧分解</h4><p><strong>问题描述：</strong></p><ul><li>假定用个体学习器 $h_1,h_2,…,h_T$ 通过加权平均法结合产生的集成来完成回归任务 $f :  {\Bbb R}^d\mapsto {\Bbb R} $</li></ul><p><strong>误差-分歧分解：</strong></p><ul><li><p>对于示例 $\boldsymbol x$ ，定义学习器 $h_i$ 的 <strong>分歧</strong> 为：</p><script type="math/tex; mode=display">A(h_i|\boldsymbol x) = \left(h_i(\boldsymbol x)-H(\boldsymbol x) \right)^2\tag{8.27}</script></li><li><p>则集成的 <strong>分歧</strong> 是：</p><script type="math/tex; mode=display">\begin {align}\overline A(h|\boldsymbol x) &= \sum_{i=1}^Tw_iA(h_i|\boldsymbol x) \\&= \sum_{i=1}^Tw_i\left(h_i(\boldsymbol x)-H(\boldsymbol x) \right)^2\tag{8.28}\end {align}</script></li><li><p><strong>分歧</strong> 项表征了个体学习器在样本 $\boldsymbol x$ 上的不一致性</p></li><li><p>即在一定程度上反映了个体学习器的多样性</p></li><li><p>个体学习器 $h_i$ 的平方误差为：</p><script type="math/tex; mode=display">E(h_i|\boldsymbol x) = (f(\boldsymbol x)-h_i(\boldsymbol x))^2\tag{8.29}</script></li><li><p>集成 $H$ 的平方误差为：</p><script type="math/tex; mode=display">E(H|\boldsymbol x) = (f(\boldsymbol x)-H(\boldsymbol x))^2\tag{8.30}</script></li><li><p>令个体学习器误差的加权值为：$\overline E(h|\boldsymbol) = \sum_{i=1}^Tw_i\cdot E(h_i|\boldsymbol x) $</p></li><li><p>则，集成的分歧</p><script type="math/tex; mode=display">\begin {align}\overline A(h|\boldsymbol x) &= \sum_{i=1}^{T}w_iE(h_i|\boldsymbol x)-E(H|\boldsymbol x)\\&= \overline E(h|\boldsymbol x)-E(H|\boldsymbol x)\tag{8.31}\end {align}</script></li><li><p>令 $p(\boldsymbol x)$ 表示样本的概率密度，则在全样本上有：</p></li></ul><script type="math/tex; mode=display">\sum_{i=1}^{T}w_i\int A(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x = \sum_{i=1}^{T}w_i\int E(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x -\int E(H|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.32}</script><ul><li><p>令个体学习器 $h_i$ 在全样本上的泛化误差和分歧项分别为:</p><script type="math/tex; mode=display">E_i = \int E(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x\tag{8.33}</script><script type="math/tex; mode=display">A_i = \int A(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.34}</script></li><li><p>集成的泛化误差为：</p><script type="math/tex; mode=display">E = \int E(H|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.35}</script></li><li><p>令 $\overline E = \sum_{i=1}^Tw_iE_i$ 表示个体学习器泛化误差的加权均值</p></li><li><p>令 $\overline A = \sum_{i=1}^Tw_iA_i$ 表示个体学习器的加权分歧值</p></li><li><p>将 (8.33)~(8.34)带入(8.32)得：</p><script type="math/tex; mode=display">E = \overline E-\overline A \tag{8.36}</script><font color="#0099">个体学习器的准确性越高、多样性越大，则集成越好</font></li></ul><h4 id="多样性度量"><a href="#多样性度量" class="headerlink" title="多样性度量"></a>多样性度量</h4><p><strong>多样性度量</strong> (diversity measure)</p><ul><li>用于度量集成中个体分类器的多样性</li><li>即估算个体学习器的多样化程度</li></ul><p><strong>列联表 </strong> </p><ul><li><p>给定数据集 $D = \{ (\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),…,(\boldsymbol x_m,y_m)\}$</p></li><li><p>对二分类任务，$y_i \in \{-1, +1\}$</p></li><li><p>分类器 $h_i$ 和 $h_j$ 的预测结果列联表为:</p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure6.PNG" alt="figure6"></p><p>其中，$a$ 表示 $h_i$ 和 $h_j$ 均预测为正类的样本数目</p></li></ul><p>……</p><p><strong>常见多样性性能度量</strong></p><ul><li><p><strong>不合度量</strong> (disagreement measure)</p><script type="math/tex; mode=display">dis_{ij} =\frac{b+c}{m}\tag{8.37}</script><p>$dis_{ij}$ 的值域为 [0, 1], 值越大则多样性越大</p></li><li><p><strong>相关系数</strong> (correlation coefficient)</p><script type="math/tex; mode=display">\rho_{ij} = \frac{ad-bc}{\sqrt{a+b}\sqrt{a+c}\sqrt{c+d}\sqrt{b+d}}\tag{8.38}</script><p>$\rho_{ij}$ 的值域为 [-1,1], 若 $h_i$ 和 $h_j$ 无关，则值为0；若 $h_i$ 和 $h_j$ 正相关则值为正，否则为负</p></li><li><p><strong>Q-统计量</strong> (Q-statistic)</p><script type="math/tex; mode=display">Q_{ij} = \frac{ad-bc}{ad+bc}\tag{8.39}</script><p>$Q_{ij}$ 的值域为 [-1,1], 若 $h_i$ 和 $h_j$ 无关，则值为0；若 $h_i$ 和 $h_j$ 正相关则值为正，否则为负</p></li><li><p>$\kappa$<strong>-统计量 </strong>($\kappa$-statistics)</p><script type="math/tex; mode=display">\kappa = \frac{p_1-p_2}{1-p_2}\tag{8.40}</script><p>其中，$p_1$ 是两个分类器取得一致的概率：</p><script type="math/tex; mode=display">p_1 = \frac{a+d}{m}\tag{8.41}</script><p>$p_2$ 是 两个分类器偶然达成一致的概率：</p><script type="math/tex; mode=display">p_2 = \frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}\tag{8.42}</script><p>若分类器 $h_i$ 与 $h_j$ 在 $D$ 上完全一致，则 $\kappa = 1$; 若它们仅是偶然达成一致，则 $\kappa = 0$</p></li></ul><h4 id="多样性增强"><a href="#多样性增强" class="headerlink" title="多样性增强"></a>多样性增强</h4><ul><li><p><strong>数据样本扰动</strong></p></li><li><p><strong>输入属性扰动</strong></p></li><li><p><strong>输出表示扰动</strong></p></li><li><p><strong>算法参数扰动</strong></p></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（八）集成学习&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（八）集成学习&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（八）集成学习&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（八）集成学习&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器</title>
    <link href="http://sunfeng.online/2019/08/17/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89--%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>http://sunfeng.online/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/</id>
    <published>2019-08-17T03:52:20.000Z</published>
    <updated>2019-08-19T08:34:44.560Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（七）贝叶斯分类器"><a href="#《机器学习》西瓜书学习笔记（七）贝叶斯分类器" class="headerlink" title="《机器学习》西瓜书学习笔记（七）贝叶斯分类器"></a>《机器学习》西瓜书学习笔记（七）贝叶斯分类器</h2><a id="more"></a><h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p><strong>条件风险 (conditional risk)</strong></p><ul><li><p>假设有 $N$ 种可能的类别标记，即 $ {\cal Y} = \{c_1,c_2,…,c_N \}$ </p></li><li><p>将样本 $\boldsymbol x$ 分类为 $c_i$ 所产生的期望损失 ，即在样本 $\boldsymbol x$ 上的 “条件风险” 为：</p><script type="math/tex; mode=display">R(c_i|\boldsymbol x) = \sum_{j=1}^{N}\lambda_{ij}P(c_j|\boldsymbol x) \tag{1}</script><p>其中：</p><p> $\lambda_{ij}$ 是将一个真实标记为 $c_j$ 的样本分类为 $c_i$ 所产生的损失；</p><p>$P(c_i|\boldsymbol x)$ 表示样本 $\boldsymbol x $ 属于类别 $c_j$ 的后验概率 </p></li></ul><p><strong>贝叶斯决策论 (Bayesian decision theory)</strong></p><ul><li><p>基于相关概率和误判损失来选择最优类别标记</p></li><li><p>即寻找一个判定准则 $h: {\cal X} \mapsto {\cal Y}$ 以最小化总体风险</p><script type="math/tex; mode=display">R(h) = E_x[R(h(\boldsymbol x)\ | \ \boldsymbol  x)] \tag{2}</script></li></ul><p><strong>贝叶斯判定准则  (Bayes decision rule)</strong></p><ul><li><p>为最小化总体风险，只需在每个样本上选择那个能使条件风险 $R(c|\boldsymbol x)$ 最小的类别标记，即：</p><script type="math/tex; mode=display">h^*(\boldsymbol x) = \mathop{\arg\min}_{c\in {\cal Y}}\ {R(c\ |\ \boldsymbol x)} \tag{3}</script></li><li><p>此时：</p><ul><li>$h^*$ 称为贝叶斯最优分类器 (Bayes optimal classifier)</li><li>总体风险 $R(h^*)$ 称为贝叶斯风险  (Bayes risk)</li><li>$1-R(h^*)$ 反映了分类器所能达到的最好性能</li></ul></li></ul><p><strong>最小化的错误率的贝叶斯最优分类器：</strong></p><ul><li><p>若目标是最小化分类错误率，则误判损失 $\lambda_{ij}$ 可写为：</p><script type="math/tex; mode=display">\lambda_{ij} = \begin{cases}0, & \text{if} \ \ i = j\\1, & \text{otherwise}\end{cases}\tag{4}</script></li><li><p>此时条件风险为</p><script type="math/tex; mode=display">R(c\ |\ \boldsymbol x) = 1-P(c\ | \ \boldsymbol x) \tag{5}</script><p>解析：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}&R(c_i|\boldsymbol x) = 1 * P(c_1|\boldsymbol x) + 1 * P(c_2|\boldsymbol x) + ... + 0 *  P(c_i|\boldsymbol x)+ ... + 1 * P(c_N|\boldsymbol x) \\&\because \ \sum_{j=1}^{N}P(c_j|\boldsymbol x) = 1 \\&\therefore \  R(c_i|\boldsymbol x) = 1-P(c_i|\boldsymbol x)\end {split}\end {equation}</script></li><li><p>于是，最小化分类错误率的贝叶斯最优分类器为：</p><script type="math/tex; mode=display">h^*{(\boldsymbol x)} = \mathop{\arg\max}_{c\in\cal Y}\ P(c \ |\ \boldsymbol x) \tag {6}</script><p>即，对每个样本 $x$, 选择能使后验概率 $P(c  | \boldsymbol x)$ 最大的类别标记</p></li><li><p><font color="#0099ff">欲使用贝叶斯判定准则最小化决策风险，首先要获得后验概率 </font> $P(c  | \boldsymbol x)$ </p></li></ul><p><strong>估计后验概率 $P(c  | \boldsymbol x)$ 的两种策略：</strong></p><ul><li><p><strong>判别式模型</strong> (discriminative models)</p><ul><li>给定 $x$, 可通过直接建模  $P(c  | \boldsymbol x)$ 来预测 $c$</li><li>如：决策树、BP神经网络、支持向量机</li></ul></li><li><p><strong>生成式模型</strong> (generative models)</p><ul><li><p>先对联合概率分布 $P(\boldsymbol x,c)$ 建模，然后由此获得 $P(c  | \boldsymbol x)$ ，即：</p><script type="math/tex; mode=display">P(c \ |\ \boldsymbol x) = \frac{P(\boldsymbol x, c)}{P(\boldsymbol x)}\tag{7}</script></li><li><p>基于贝叶斯定理，得：</p><script type="math/tex; mode=display">P(c \ |\ \boldsymbol x) = \frac{P(c)\ P(\boldsymbol x \ |\ c)}{P(\boldsymbol x)}\tag{8}</script></li><li><p>$P(c)$: 是类先验概率</p><ul><li>表达了样本空间中各类样本所占的比例</li><li>根据大数定律，当训练集中包含充足的独立同分布样本时，$P(c)$ 可通过各类样本出现的频率来进行估计</li></ul></li><li><p>$P(\boldsymbol x  | c)$: 是样本 $\boldsymbol x$ 相对于类标记 $c$ 的类条件概率，或称为“似然”</p><ul><li>类条件概率涉及关于 $\boldsymbol x$ 所有属性的联合概率</li><li>不能直接根据样本出现的频率来进行估计</li></ul></li><li><p>$P(\boldsymbol x)$: 是用于归一化的证据因子</p><ul><li>对给定的样本 $\boldsymbol x$, 证据因子与类别标记无关</li></ul></li></ul></li></ul><hr><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p><strong>极大似然估计 (Maximum Likehood Estimation)</strong></p><ul><li><p>估计类条件概率的一种常用策略</p></li><li><p>令 $D_c$ 表示训练集 $D$ 中第 $c$ 类样本组成的集合</p></li><li><p>假设这些样本时独立同分布的，则参数 $\theta_c$ 对于数据集 $D_c$ 的似然是：</p><script type="math/tex; mode=display">P(D_c \ | \ \theta_c) = \prod_{\boldsymbol x\in D_c} \ P(\boldsymbol x \ |\ \theta_c)\tag{9}</script></li><li><p>对 $\theta_c$ 进行极大似然估计，就是寻找能最大化似然 $P(D_c  |  \theta_c)$ 的参数值 $\hat \theta_c$.</p></li><li><p>为了避免下溢，通常使用对数似然：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}LL(\theta_c) &= \log\ P(D_c\ | \ \theta_c)\\&= \sum_{x\in D_C}\ \log\ P(\boldsymbol x\ | \ \theta_c)\end{split}\end{equation}\tag{10}</script></li><li><p>此时，参数 $\theta_c$ 的极大似然估计 $\hat \theta_c$ 为：</p><script type="math/tex; mode=display">\hat \theta = \mathop{\arg \max}_{\theta_c}\ LL(\theta_c) \tag{11}</script></li></ul><p><strong>极大似然估计的例子：</strong></p><ul><li><p>例如，在连续属性情形下，假设概率密度函数 $p(\boldsymbol x |  c) $ ~ $\cal N(\boldsymbol\mu_c,\boldsymbol\sigma_c)$. 则参数 $\boldsymbol\mu_c$ 和 $\boldsymbol\sigma_c$ 的极大似然估计为:</p><script type="math/tex; mode=display">\boldsymbol{\hat \mu_c} = \frac{1}{|D_c|}\ \sum_{\boldsymbol x\in D_c}\ \boldsymbol x \tag{12}</script><script type="math/tex; mode=display">\boldsymbol{\hat \sigma_c} = \frac{1}{|D_c|} \ \sum_{\boldsymbol x \in D_c}(\boldsymbol x - \boldsymbol {\hat \mu})(\boldsymbol x - \boldsymbol {\hat \mu})^T\tag{13}</script></li><li><p>解析过程如下：</p></li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure1.jpg" alt="figure1"></p><hr><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p><strong>朴素贝叶斯分类器 (naive Bayes classifier)：</strong></p><ul><li><p>采用了<strong>属性条件独立性假设</strong> (attribute conditional independence assumption)：对已知类别，假设所有属性相互独立</p></li><li><p>基于属性独立性条件假设，式 (8) 可重写为：</p><script type="math/tex; mode=display">P(c\ |\ \boldsymbol x) = \frac{P(c)P(\boldsymbol x\ |\ c)}{P(\boldsymbol x)} = \frac{P(c)}{P(\boldsymbol x)}\prod_{i=1}^{d}P(x_i\ |\ c)\tag{14}</script><p>其中 $d$  为属性数目，$x_i$ 为 $\boldsymbol x$ 在第 $i$ 个属性上的取值</p></li><li><p><strong>朴素贝叶斯分类器</strong>的表达式：</p><script type="math/tex; mode=display">h_{nb}(x) = \mathop{\arg\max}_{c\in \cal Y} \ P(c)\prod_{i=1}^{d}\ P(x_i\ |\ c)\tag{15}</script></li><li><p><font color="#0099ff">朴素贝叶斯分类器的训练过程就是基于训练集 D 来估计类先验概率 P(c), 并为每个属性估计条件概率 </font> $P(x_i | c)$ .</p><ul><li><p>$P(c)$ : 类先验概率</p><script type="math/tex; mode=display">P(c) = \frac{|D_c|}{|D|}\tag{16}</script><p>其中, $D_c$ 表示训练集 $D$ 中，第 $c$ 类样本组成的集合</p></li><li><p>$P(x_i  |  c)$ : 表示第 $c$ 类样本在 i 个属性上取值为 $x_i$ 的条件概率</p><script type="math/tex; mode=display">P(x_i\ | \ c) = \frac{|D_{c,x_i}|}{|D_c|}\tag{17}</script><p>其中，$D_{c,x_i}$ 表示 $D_c$ 中在第 $i$ 个属性上取值为 $x_i$ 的样本组成的集合</p></li><li><p>对于连续属性，假设服从正态分布，则：</p><script type="math/tex; mode=display">P(x_i\ | \ c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp\left(-\frac{(x_i - \mu_{c,i})^2}{2\sigma^2_{c,i}} \right)\tag{18}</script><p>其中，$\mu_{c,i}$ 和 $\sigma^2_{c,i}$ 分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差</p></li></ul></li></ul><p><strong>朴素贝叶斯分类器例子：</strong> 用西瓜数据集 3.0 训练一个朴素贝叶斯分类器，对测试例 “测1” 进行分类：</p><ul><li>西瓜数据集 3.0</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure2.PNG" alt="figure2"></p><ul><li>首先估计类先验概率 $P(c)$</li></ul><script type="math/tex; mode=display">P(好瓜 = 是) = \frac{8}{17}\approx 0.471\\P(好瓜 = 否) = \frac{9}{17}\approx 0.529\\</script><ul><li>然后，为每个属性估计类条件概率 $P(x_i  |  c)$</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure3.PNG" alt="figure3"></p><ul><li>测试例</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure4.PNG" alt="figure4"></p><ul><li>于是，有：</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure5.PNG" alt="figure5"></p><ul><li>因此，朴素贝叶斯分类器将测试样本 “测1” 判别为 “好瓜”</li></ul><p><strong>拉普拉斯修正</strong> (Laplacian correction)：</p><ul><li><p>为了避免其他属性携带的信息被训练集中未出现的属性值 “抹去”，在估计概率值是通常要进行 “平滑”</p></li><li><p>拉普拉斯修正：</p><script type="math/tex; mode=display">\begin{align}\hat P(c) &= \frac{|D_c|+1}{|D|+N} \tag{19}\\\hat P(x_i\ |\ c) &= \frac{|D_{c,x_i}|+1}{|D|+N_i}   \tag{20}\\\end{align}</script><p>其中，$N$ 表示训练集 $D$ 中可能的类别数，$N_i$ 表示第 $i$ 个属性可能的取值数</p></li></ul><hr><h3 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h3><hr><h3 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h3><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><hr><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（六）-- 支持向量机</title>
    <link href="http://sunfeng.online/2019/08/16/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89--%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://sunfeng.online/2019/08/16/《机器学习》西瓜书学习笔记（六）-- 支持向量机/</id>
    <published>2019-08-16T06:53:52.000Z</published>
    <updated>2019-08-27T13:18:28.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（六）-支持向量机"><a href="#《机器学习》西瓜书学习笔记（六）-支持向量机" class="headerlink" title="《机器学习》西瓜书学习笔记（六） 支持向量机"></a>《机器学习》西瓜书学习笔记（六） 支持向量机</h2><a id="more"></a><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p><strong><font color="#0099">给定训练集，分类学习最基本的想法就是基于训练集在样本空间中找到一个划分超平面，将不同类别的样本划分开来，且要求划分超平面产生的分类结果要尽可能鲁棒，对未见示例的泛化能力要尽可能强</font></strong></p><p><strong>划分超平面</strong></p><ul><li><p>划分超平面的线性方程描述：</p><script type="math/tex; mode=display">\boldsymbol w^T \boldsymbol x + b = 0\tag{6.1}</script><p>其中 $\boldsymbol w = (w_1;w_2;…;w_d)$ 为法向量，决定超平面的方向；</p><p>$b$ 为位移项，决定了超平面与原点之间的距离；</p><p>将此超平面记为 $(\boldsymbol w, b)$</p></li><li><p>样本空间中的任意点 $\boldsymbol x$ 到超平面的 $(\boldsymbol w, b)$ 距离为:</p><script type="math/tex; mode=display">r = \frac{|\boldsymbol w^T \boldsymbol x + b|}{||\boldsymbol w||}\tag{6.2}</script></li></ul><p>假设超平面 $(\boldsymbol w, b)$ 能将训练样本正确分类，即对于 $(\boldsymbol x_i, y_i) \in D$:</p><ul><li><p>若 $y_i = +1$, 则有 $\boldsymbol w^T \boldsymbol x_i + b &gt; 0$</p></li><li><p>若 $y_i = -1$, 则有 $\boldsymbol w^T \boldsymbol x_i + b &lt; 0$ </p></li><li><p>令</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}\boldsymbol w^T \boldsymbol x_i + b \ge +1, \ \ y_i = +1 \\\boldsymbol w^T \boldsymbol x_i + b \le -1, \ \ y_i = -1\end {cases}\tag{6.3}\end {equation}</script></li></ul><p><strong>式 (6.3) 推导</strong></p><ul><li><p>假设超平面是 $(\boldsymbol w^\prime)^T\boldsymbol x + \boldsymbol b^\prime= 0 $, 对于 $(\boldsymbol x_i, y_i)$, 有：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime > 0, \ \ y_i = +1 \\(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime < 0, \ \ y_i = -1\end {cases}\tag{*}\end {equation}</script></li><li><p>根据几何间隔，将以上关系可修正为:</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime \ge +\xi, \ \ y_i = +1 \\(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime \le -\xi, \ \ y_i = -1\end {cases}\tag{**}\end {equation}</script></li><li><p>由 $\xi &gt; 0$ 得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\frac{1}{\xi} \boldsymbol w^\prime)^T \boldsymbol x_i + \frac{1}{\xi}b^\prime \ge +1, \ \ y_i = +1 \\(\frac{1}{\xi}\boldsymbol w^\prime)^T \boldsymbol x_i + \frac{1}{\xi}b^\prime \le -1, \ \ y_i = -1\end {cases}\tag{***}\end {equation}</script></li><li><p>令 $\boldsymbol w = \frac{1}{\xi} \boldsymbol w^\prime$, $b = \frac{1}{\xi}b^\prime$, 得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}\boldsymbol w^T \boldsymbol x_i + b \ge +1, \ \ y_i = +1 \\\boldsymbol w^T \boldsymbol x_i + b \le -1, \ \ y_i = -1\end {cases}\tag{****}\end {equation}</script></li></ul><p><strong>支持向量 (support vector)：</strong> </p><ul><li>距离超平面最近的几个训练样本可使上式 (6.6) 的等号成立，它们被称为 “支持向量”</li></ul><p><strong>间隔 (margin): </strong></p><ul><li>两个异类支持向量到超平面的距离之和为<script type="math/tex; mode=display">\gamma = \frac{2}{||\boldsymbol w ||}\tag{6.4}</script>它被称为 “间隔”</li></ul><p><img src="/2019/08/16/《机器学习》西瓜书学习笔记（六）-- 支持向量机/figure2.PNG" alt="figure2"></p><p><strong>支持向量机(Support Vector Machine, SVM)</strong></p><ul><li><p>最大化间隔：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}& \mathop{\max} _{\boldsymbol w,b}\frac{2}{||\boldsymbol w||}\\& s.t.\ y_i(\boldsymbol w^T \boldsymbol x_i + b) \ge 1,i = 1,2,...,m\end {split}\tag{6.5}\end {equation}</script></li><li><p>最大化 $||\boldsymbol w||^{-1}:$ </p></li><li><p>最小化 $||\boldsymbol w||^{2}:$ </p><script type="math/tex; mode=display">\begin {equation}\begin {split}& \mathop{\min}_{\boldsymbol w,b} \frac{1}{2} {||\boldsymbol w||}^2\\& s.t.\ y_i(\boldsymbol w^T \boldsymbol x_i + b) \ge 1,i = 1,2,...,m\end {split}\tag{6.6}\end {equation}</script></li></ul>   <font color="#0099">此为支持向量机 (Support vector Machine，简称SVM) 的基本型</font><hr><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><h4 id="SVM-基本型的对偶问题-dual-problem"><a href="#SVM-基本型的对偶问题-dual-problem" class="headerlink" title="SVM 基本型的对偶问题 (dual problem)"></a><strong>SVM 基本型的对偶问题</strong> (dual problem)</h4><ul><li><p>式 (6.6) 对应的拉格朗日函数：</p><script type="math/tex; mode=display">L(\boldsymbol w, b, \alpha) = \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\alpha_i\left( 1-y_i(\boldsymbol w^T\boldsymbol x_i+b)\right )}\tag{6.8}</script><p>其中，$\alpha = (\alpha_i;\alpha_2;…;\alpha_m)$.</p></li><li><p>令 $L(\boldsymbol w, b, \alpha)$ 对 $\boldsymbol w$ 和 $b$ 求偏导为零可得:</p><script type="math/tex; mode=display">\begin {align}\boldsymbol w &= \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i}\tag{6.9}\\0 &= \sum_{i=1}^{m}{\alpha_i y_i}\tag{6.10}\end {align}</script></li><li><p>式 (6.9) 和 (6.10) 推导：</p><script type="math/tex; mode=display">\begin {align}L(\boldsymbol w, b, \alpha) &= \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\alpha_i\left( 1-y_i(\boldsymbol w^T\boldsymbol x_i+b)\right )}\\&= \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\left( \alpha_i-\alpha_iy_i\boldsymbol w^T\boldsymbol x_i+\alpha_iy_ib)\right )}\\&= \frac{1}{2}\left\|\boldsymbol w \right\|^2 + \sum_{i=1}^{m}\alpha_i - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib\end {align}</script><p>对 $\boldsymbol w$ 和 $b$ 求偏导为零可得:</p><script type="math/tex; mode=display">\begin {align}\frac{\partial L}{\partial \boldsymbol w} &= \frac{1}{2}\times 2\times \boldsymbol w + 0 - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i - 0 = 0 \Rightarrow \boldsymbol w = \sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i \\\frac{\partial L}{\partial \boldsymbol b} &= 0 + 0 - 0 - \sum_{i=1}^{m}\alpha_iy_i\Rightarrow 0 = \sum_{i=1}^{m}\alpha_iy_i\end {align}</script></li><li><p>将式 (6.9) 带入 (6.8) 得:</p><script type="math/tex; mode=display">\begin {align}L(\boldsymbol w, b, \alpha) &= \frac{1}{2}\boldsymbol w^T\boldsymbol w +\sum_{i=1}^{m}{\alpha_i} - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib \\&= \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i}+\sum_{i=1}^{m}{\alpha_i} - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib \\&= \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} - \boldsymbol w^T\sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&=- \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&= - \frac{1}{2}\boldsymbol (\sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i})^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&= \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\boldsymbol x_i^T\boldsymbol x_j} -b\sum_{i=1}^{m}\alpha_iy_i \\\end {align}</script></li><li><p>再考虑式 (6.10) 的约束，得到式 (6.6) 的对对偶问题：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\max}_{\alpha}{\sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\boldsymbol x_i^T\boldsymbol x_j}}\tag{6.11}\\&s.t \  \sum_{i=1}^{m}\alpha_iy_i = 0, \ \alpha_i \ge0,i = 1, 2,...,m\end {align}</script></li></ul><hr><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><hr><h3 id="间隔与正则化"><a href="#间隔与正则化" class="headerlink" title="间隔与正则化"></a>间隔与正则化</h3><hr><h3 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（六）-支持向量机&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（六）-支持向量机&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（六） 支持向量机&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（六） 支持向量机&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（五）-- 神经网络</title>
    <link href="http://sunfeng.online/2019/08/14/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89--%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://sunfeng.online/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/</id>
    <published>2019-08-14T07:26:34.000Z</published>
    <updated>2019-08-27T04:38:21.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（五）-神经网络"><a href="#《机器学习》西瓜书学习笔记（五）-神经网络" class="headerlink" title="《机器学习》西瓜书学习笔记（五） 神经网络"></a>《机器学习》西瓜书学习笔记（五） 神经网络</h2><a id="more"></a><h3 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h3><p><strong>神经网络 (neural network):</strong> </p><ul><li>神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的反应</li><li>神经网络中最基本的成分是<strong>神经元模型</strong></li></ul><p><strong>神经元 (neuron) 模型: </strong> M-P神经元模型</p><ul><li>神经元接收到来自 $n$ 个其他神经元传递过来的输入信号</li><li>这些输入信号通过权重的连接进行传递</li><li>神经元收到的总输入值将与神经元的阈值进行比较，然后通过 “激活函数” 处理以产生神经元的输出</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure1.PNG" alt="figure1"></p><p><strong>激活函数 (activation function):</strong></p><ul><li><p><strong>阶跃函数</strong></p><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure2.PNG" alt="figure2"></p></li><li><p><strong>Sigmoid函数</strong> </p><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure3.PNG" alt="figure3"></p></li></ul><hr><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><h4 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机 (Perceptron)"></a>感知机 (Perceptron)</h4><p><strong>感知机：</strong></p><ul><li>由两层神经元组成</li><li>输入层接收外界输入信号传递给输出层</li><li>输出层是M-P神经元</li><li>给定训练数据集，权重 $w_i (i = 1,2,…,n)$ 以及阈值 $\theta$ 可通过学习得到</li><li>将阈值 $\theta$ 可以看做一个固定输入为 -1.0 的 “哑结点” 所对应的权重$w_{n+1}$</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure4.PNG" alt="figure4"></p><p><strong>感知机学习规则：</strong></p><ul><li>对训练样例 $(\boldsymbol x, y)$, 若当前感知机的输出为 $\hat y$, 则感知机的权重将这样调整：<script type="math/tex; mode=display">\begin {equation}\begin {split}w_i &\leftarrow w_i + \Delta w_i,\\\Delta w_i &= \eta(y-\hat y)x_i,\end {split}\end{equation}</script>其中 $\eta \in (0, 1)$ 称为<strong>学习率 (learning rate)</strong>.</li></ul><p><strong>注意：</strong></p><ul><li><p>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限</p></li><li><p>感知机无法解决非线性可分问题</p></li><li><p>要解决非线性可分问题，需要考虑使用多层功能神经元</p></li></ul><h4 id="多层前馈神经网络-multi-layer-feedforward-neural-networks"><a href="#多层前馈神经网络-multi-layer-feedforward-neural-networks" class="headerlink" title="多层前馈神经网络 (multi-layer feedforward neural networks)"></a>多层前馈神经网络 (multi-layer feedforward neural networks)</h4><p><strong>多层前馈神经网络：</strong></p><ul><li>每层神经元与下一层神经元完全互联，神经元之间不存在同层连接，也不存在跨层连接</li><li>输入层神经元接收外界输入</li><li>隐层与输出层神经元对信号进行加工，最终结果有输出层神经元输出</li><li>即，<strong>输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元</strong></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure5.PNG" alt="figure5"></p><p><strong>神经网络学习：</strong></p><ul><li>神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权” (connection weight) 以及每个神经元的阈值</li></ul><hr><h3 id="误差逆传播算法"><a href="#误差逆传播算法" class="headerlink" title="误差逆传播算法"></a>误差逆传播算法</h3><p><strong>误差逆传播算法 (error BackPropagation)：</strong></p><ul><li>一种用来训练神经网络的强大学习算法，简称 <strong>BP 算法</strong></li><li><strong>BP网络</strong>通常是指用 BP 算法训练的多层前馈神经网络</li></ul><p><strong>符号定义：</strong></p><ul><li><p><strong>训练集：</strong> $D = \{(\boldsymbol x_1, \boldsymbol y_1), (\boldsymbol x_2, \boldsymbol y_12, …, (\boldsymbol x_m, \boldsymbol y_m) \},  \boldsymbol x_i \in R^d, \boldsymbol y_i \in R^l$ , 即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量</p></li><li><p><strong>网络结构：</strong> </p><ul><li>$d$ 个输入神经元</li><li>$l$  个输出神经元</li><li>$q$ 个隐层神经元</li></ul></li><li><p><strong>阈值：</strong></p><ul><li>$\theta_j$: 输出层第 $j$ 个神经元的阈值</li><li>$\gamma_h$: 隐层第 $h$ 个神经元的阈值</li></ul></li><li><p><strong>连接权：</strong></p><ul><li>$v_{ih}$: 输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间连接权</li><li>$w_{hj}$: 隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权</li></ul></li><li><p><strong>神经元输出：</strong></p><ul><li>$b_h$: 隐层第 $h$ 个神经元的输出</li></ul></li><li><p><strong>神经元输入：</strong></p><ul><li>$a_h$: 隐层第 $h$ 个神经元接收到的输入 $a_h = \sum_{i=1}^{d}v_{ih}x_i$</li><li>$\beta_j$: 输出层第 $j$ 个神经元接收到的输入 $\beta_j = \sum_{i=1}^{d}w_{hj}b_h$ </li></ul></li><li><p><strong>激活函数：</strong></p><ul><li>假设隐层和输出层神经元都使用 <strong><em>Sigmoid</em> 函数</strong></li></ul></li><li><p><strong>神经网络输出：</strong></p><ul><li>对训练样例 $(\boldsymbol x_k, \boldsymbol y_k)$, 假定神经网络输出为 $\hat y_k = (\hat y_1 ^k,\hat y_2 ^k,…,\hat y_l ^k)$, 即：<script type="math/tex; mode=display">\hat y_j^k = f(\beta _j - \theta_j) \tag {1}</script></li></ul></li><li><p><strong>均方误差：</strong></p><ul><li>网络在样例 $(\boldsymbol x_k, \boldsymbol y_k)$ 上的误差为：<script type="math/tex; mode=display">E_k = \frac{1}{2}\sum_{j=1}^{l}{(\hat y_j^k - y_j^k)^2} \tag{2}</script></li></ul></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure6.PNG" alt="figure6"></p><p><strong>需要学习的参数：</strong> 基于上述定义，网络中有 $(d + l + 1)q + l$ 个参数需要确定</p><ul><li>输入层到隐层的 $d\times q$ 个权值</li><li>隐层到输出层的 $q \times l$ 个权值</li><li>隐层的 $q$ 个阈值</li><li>输出层的 $l$ 个阈值</li></ul><p><strong>参数更新：</strong></p><ul><li><p>BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，即对任意参数 $v$ 的更新为:</p><script type="math/tex; mode=display">v \leftarrow v + \Delta v</script></li><li><p>基于<strong>梯度下降 (gradient descent)</strong>策略，以目标的负梯度方向对参数进行调整，对式(2)的误差 $E_k$ , 给定学习率 $\eta$, 有：</p><script type="math/tex; mode=display">\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}\tag{3}</script></li><li><p><strong>反向传播</strong>：由于 $w_{hj}$ 先影响到第 $j$ 个输出层神经元的输入值 $\beta_j$, 再影响到其输出值 $\hat y_j^k$ , 然后影响到 $E_k$, 由链式法则得:</p><script type="math/tex; mode=display">\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat y_j^k} \cdot \frac{\partial y_j^k}{\partial \beta _j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}\tag{4}</script></li><li><p>根据 $\beta_j$ 的定义，得:</p><script type="math/tex; mode=display">\frac{\partial \beta_j}{\partial w_{hj}} = b_h \tag{5}</script></li><li><p>Sigmoid函数 $f(x)$ 的导数：</p><script type="math/tex; mode=display">f\prime(x) = f(x)(1-f(x)) \tag{6}</script></li><li><p>根据式(1)(2)和(6)，有:</p><script type="math/tex; mode=display">\begin {equation}\begin {split}g_j &= - \frac{\partial E_k}{\partial \hat y_j^k} \cdot \frac{\partial y_j^k}{\partial \beta _j}\\&= -(\hat y_j^k - y_j^k)f\prime(\beta_j-\theta_j)\\&= \hat y_j ^k(1-\hat y_j^k)(y_j^k-\hat y_j^k) \end {split}\end {equation}\tag{7}</script></li><li>由式(3)(4)(5)和(7)即可得 BP 算法中关于 $w_{hj}$ 的更新公式:<script type="math/tex; mode=display">  \Delta w_{hj} = \eta g_jb_h \tag{8}</script></li></ul><ul><li><p>类似可得：</p><script type="math/tex; mode=display">\begin {align}\Delta \theta_j &= -\eta g_j\tag{9}\\ \Delta v_{ih} &= \eta e_hx_i \tag{10}\\ \Delta \gamma_h &= -\eta e_h \tag{11}\end {align}</script></li><li><p>其中：</p><script type="math/tex; mode=display">\begin {equation}  \begin {split}  e_h &= -\frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial\alpha_h}\\  &= -\sum_{j=1}^{l}\frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f\prime(\alpha_h-\gamma_h)\\  &=\sum_{j=1}^{l}w_{hj}g_jf\prime(\alpha_h-\gamma_h)\\  &= b_h(1-b_h)\sum_{j=1}^{l}w_{hj}g_j   \end {split}  \end {equation}  \tag{12}</script></li></ul><p><strong>BP算法工作流程:</strong> 对于每个输入样例：</p><ul><li>先将输入示例提供给输入层神经元</li><li>然后逐层将信号前传，直到产生输出层的结果</li><li>然后计算输出层的误差 (4-5行)</li><li>再将误差逆向传播至隐层神经元 (第6行)</li><li>最后根据隐层神经元的误差来对连接权和阈值进行调整 (第7行)</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure7.PNG" alt="figure7"></p><p><strong>标准BP算法：</strong> </p><ul><li>以上介绍的误差逆传播算法称为 <strong>标准BP算法</strong> </li><li>更新规则基于单个的 $E_k$ 推导而得</li><li>每次仅针对一个训练样本更新连接权和阈值</li><li>参数更新的非常频繁，而且对于不同的样例进行更新的效果可能出现 “抵消” 现象</li><li>需要更多的迭代次数</li></ul><p><strong>累积BP算法：</strong> </p><ul><li>累积误差逆传播算法是要最小化训练集 $D$ 上的累积误差: $E = \frac{1}{m}\sum_{k=1}^{m}E_k$</li><li>在读取整个训练集 $D$ 一遍后才对参数进行更新</li><li>参数更新频率低得多</li></ul><p><strong>缓解BP网络过拟合的策略：</strong></p><ul><li><p><strong>早停 (early stopping):</strong> </p><ul><li>将数据集分为训练集和验证集</li><li>训练集用来计算梯度、更新连接权和阈值</li><li>验证集用来估计误差</li><li>若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值</li></ul></li><li><p><strong>正则化 (regularization):</strong> </p><ul><li><p>基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分</p></li><li><p>例如连接权和阈值的平方和</p></li><li><p>仍令 $E_k$ 表示第 $k$ 个训练样本上的误差， $w_i$ 表示连接权和阈值，则误差目标函数改变为：</p><script type="math/tex; mode=display">E_k = \lambda\frac{1}{m}\sum_{k=1}^{m}E_k + (1-\lambda)\sum_{i}{w_i^2}</script><p>其中 $\lambda \in (0,1)$ 用于对经验误差和网络复杂度这两项进行折中，常通过交叉验证法来估计。</p></li></ul></li></ul><hr><h3 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h3><h4 id="局部极小-local-minimum"><a href="#局部极小-local-minimum" class="headerlink" title="局部极小 (local minimum)"></a>局部极小 (local minimum)</h4><p><strong>局部极小解：</strong> </p><ul><li>对 $ \boldsymbol w^\prime$ 和 $ \theta^\prime $，若存在 $\epsilon &gt; 0$ 使得：</li></ul><script type="math/tex; mode=display">\forall (\boldsymbol w;\theta) \in \{(\boldsymbol w; \theta) \boldsymbol | \ ||(\boldsymbol w;\theta) - (\boldsymbol w^\prime;\theta^\prime)|| \le \epsilon \}</script><p>都有 $E(\boldsymbol w;\theta ) &gt; E(\boldsymbol w^\prime;\theta^\prime)$ 成立，则 $(\boldsymbol w^\prime;\theta^\prime)$ 为局部极小解</p><ul><li><font color="#0099ff">局部极小解是参数空间中的某个点，其领域点的误差函数值均不小于该点的函数值</font></li><li><p>参数空间内梯度为零的点，只要其误差函数值小于邻点的误差函数值，就是局部极小点</p></li><li><p>可能存在多个局部极小值，但却只会有一个全局最小值</p></li></ul><h4 id="全局最小-global-minimum"><a href="#全局最小-global-minimum" class="headerlink" title="全局最小 (global minimum)"></a>全局最小 (global minimum)</h4><p><strong>全局最小解：</strong></p><ul><li><p>对 $\boldsymbol w^\prime$ 和 $\theta^\prime$，若存在 $\epsilon &gt; 0$ 使得：</p><script type="math/tex; mode=display">\forall (\boldsymbol w;\theta) \in \{(\boldsymbol w; \theta) \boldsymbol | \ ||(\boldsymbol w;\theta) - (\boldsymbol w^\prime;\theta^\prime)|| \le \epsilon \}</script><p>对参数空间的任意 $(\boldsymbol w;\theta)$, $E(\boldsymbol w;\theta ) &gt; E(\boldsymbol w^\prime;\theta^\prime)$ 成立，则 $(\boldsymbol w^\prime;\theta^\prime)$ 为全局最小解</p></li><li><font color="#0099ff">全局最小解则是指参数空间中所有点的误差函数值均不小于该点的误差函数值</font></li><li><p>全局最小一定是局部极小，反之不成立</p></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure8.PNG" alt="figure8"></p><h4 id="参数寻优"><a href="#参数寻优" class="headerlink" title="参数寻优"></a>参数寻优</h4><p><strong>参数寻优：</strong></p><ul><li>参数寻优过程中就是希望找到全局最小</li><li>参数寻优方法：<strong>梯度下降</strong></li></ul><p><strong>梯度下降法参数寻优：</strong></p><ul><li>从某些初始解出发，迭代寻找参数最优解</li><li>每次迭代中，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向</li><li>由于梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜素最优解</li><li>若误差函数的当前梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止</li><li>如果误差函数仅有一个局部极小，则此时找到的局部极小就是全局最小</li><li>如果误差函数具有多个局部极小，则不能保证找的的解是全局最小，即<strong>参数寻优陷入了局部极小</strong></li></ul><p><strong>“跳出”局部极小，达到全局最小：</strong></p><ul><li>以多组不同参数初始化多个神经网络，选择最接近全局最小的</li><li><strong>模拟退火</strong>：每一步以一定的概率接收比当前解更差的结果</li><li><strong>随机梯度下降</strong></li></ul><hr><h3 id="其他常见的神经网络"><a href="#其他常见的神经网络" class="headerlink" title="其他常见的神经网络"></a>其他常见的神经网络</h3><h4 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h4><h4 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h4><h4 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h4><h4 id="级联相关网络"><a href="#级联相关网络" class="headerlink" title="级联相关网络"></a>级联相关网络</h4><h4 id="Elman网络"><a href="#Elman网络" class="headerlink" title="Elman网络"></a>Elman网络</h4><h4 id="Boltzmann机"><a href="#Boltzmann机" class="headerlink" title="Boltzmann机"></a>Boltzmann机</h4><hr><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（五）-神经网络&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（五）-神经网络&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（五） 神经网络&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（五） 神经网络&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（四）-- 决策树</title>
    <link href="http://sunfeng.online/2019/08/13/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89--%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://sunfeng.online/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/</id>
    <published>2019-08-13T08:43:48.000Z</published>
    <updated>2019-08-27T03:31:46.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（四）-决策树"><a href="#《机器学习》西瓜书学习笔记（四）-决策树" class="headerlink" title="《机器学习》西瓜书学习笔记（四） 决策树"></a>《机器学习》西瓜书学习笔记（四） 决策树</h2><a id="more"></a><h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><p><strong>决策树 (decision tree)：</strong></p><ul><li>一类常见的机器学习算法</li><li>决策树是基于树形结构进行决策的</li><li>决策树学习的目的是产生一棵泛化能力强，即处理未见示例能力强的决策树</li></ul><p><strong>基本流程：</strong>决策树基本流程遵循简单直观的“分而治之”策略，如下图所示：</p><p><img src="/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/figure1.PNG" alt="figure1"></p><p><strong>在决策树基本算法中，有三种情形会导致递归返回：</strong></p><ul><li>当前结点包含的样本全属于同一类别，无需划分</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分<ul><li>将当前结点标记为叶子结点</li><li>并将其类别设定为该结点所含样本最多的类别</li></ul></li><li>当前结点包含的样本集合为空，不能划分<ul><li>将当前结点标记为叶子结点</li><li>并将类别设定为其父节点所含样本最多的类别</li></ul></li></ul><hr><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><p>决策树学习的关键是 <strong>如何选择最优划分属性</strong>，<font color="#0099ff">一般而言，随着划分过程的不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。</font> </p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p><strong>信息熵 (information entropy):</strong></p><ul><li><p>信息熵是度量样本集合<strong>纯度</strong>最常用的一种指标。</p></li><li><p>假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k (k = 1, 2, …, |{\cal Y}|)$, 则 $D$ 的信息熵定义为：</p><script type="math/tex; mode=display">Ent(D) = - \sum _{k = 1}^{|{\cal Y}|}p_k\log_2p_k</script><p><strong>$Ent(D)$ 的值越小，则 $D$ 的纯度越高</strong></p></li></ul><p><strong>信息增益 (information gain):</strong></p><ul><li><p>假设离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, … , a^V\}$</p></li><li><p>使用 $a$ 对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$.</p><script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum _{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v).</script><p><strong>信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的 “纯度提升”越大</strong>。</p></li></ul><p><strong>以信息增益为准则选择划分属性：</strong></p><script type="math/tex; mode=display">a_* = \mathop{\arg\max}_{a\in A} Gain(D, a)</script><h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4><p><em>信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性。</em></p><p><strong>增益率 (gain ratio):</strong></p><script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D, a)}{IV(a)}</script><p>其中：</p><script type="math/tex; mode=display">IV(a) = - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}</script><p>称为属性 $a$ 的 “固有值”</p><h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p><em>GART 决策树使用 “基尼指数”来选择划分属性</em></p><p><strong>基尼值：</strong>用来度量数据集 $D$ 的纯度</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gini(D) &= \sum_{k=1}^{|y|}\sum_{k\prime \ne k} p_kp_{k\prime}\\&= 1 - \sum_{k=1}^{|y|}{p_k^2}\end {split}\end {equation}</script><p><strong>$Gini(D)$ 越小，则数据集 $D$ 的纯度越高</strong></p><p><strong>基尼指数 (Gini index):</strong></p><script type="math/tex; mode=display">Gini\_index(D, a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)</script><hr><h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><p><strong>剪枝 (pruning):</strong> 决策树学习算法对付“过拟合”的主要手段</p><h4 id="预剪枝-pre-pruning"><a href="#预剪枝-pre-pruning" class="headerlink" title="预剪枝 (pre-pruning)"></a>预剪枝 (pre-pruning)</h4><ul><li>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分并将当前结点标记为叶结点</li><li>具体示例详见课本 <em>P-80</em></li></ul><h4 id="后剪枝-post-pruning"><a href="#后剪枝-post-pruning" class="headerlink" title="后剪枝 (post-pruning)"></a>后剪枝 (post-pruning)</h4><ul><li>先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应子树替换成叶结点能带来决策树泛化能力的提升，则将该子树替换为叶结点</li><li>具体示例详见课本 <em>P-82</em></li></ul><hr><h3 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h3><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p><em>常用的将连续属性离散化技术是二分法策略。</em></p><p><strong>二分法 (bi-partition):</strong></p><ul><li><p>给定样本集 $D$ 和连续属性 $a$ , 假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值</p></li><li><p>将这些值从小到大进行排序，记为 $\{a^1, a^2,…,a^n\}$.</p></li><li><p>基于划分点 $t$, 将 $D$ 分为两个子集 $D_t^-$ 和 $D_t^+$ </p></li><li><p>其中 $D_t^-$ 包含那些在属性 $a$ 取值不大于 $t$ 的样本，而 $D_t^+$ 则包含那些在属性 $a$ 取值大于 $t$ 的样本</p></li><li><p>对于相邻属性取值 $a^i$ 和 $a^{i+1}$ 来说，$t$ 在区间 $[a^i, a^{i+1})$ 中取任意值所产生的划分结果相同</p></li><li><p>因此，对于连续属性 $a$, 我们可考察包含 $n-1$ 个元素的候选划分点集合：</p><script type="math/tex; mode=display">T_a = \{\frac{a^i + a^{i+1}}{2} \ |\ 1 \le i \le n-1 \}</script><p>即，以区间 $[a^i, a^{i+1})$ 的中位点 $\frac{a^i + a^{i+1}}{2}$ 作为候选划分点</p></li><li><p>选取最优划分点进行样本集合划分：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gain(D, a) &= max_{t \in T_a} Gain(D,a,t) \\&= max_{t \in Ta} Ent(D) - \sum_{\lambda \in \{-, +\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)\end {split}\end {equation}</script><p>其中 $Gain(D, a, t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益</p></li><li><p>于是，我们就可选出使  $Gain(D, a, t)$ 最大化的划分点</p></li></ul><h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><h5 id="如何在属性值缺失的情况下进行划分属性选择？"><a href="#如何在属性值缺失的情况下进行划分属性选择？" class="headerlink" title="如何在属性值缺失的情况下进行划分属性选择？"></a>如何在属性值缺失的情况下进行划分属性选择？</h5><ul><li><p>给定训练集 $D$ 和属性 $a$ , 令 $\tilde D$ 表示 $D$ 在属性 $a$ 上没有缺失值的样本子集</p></li><li><p>假定属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, … ,a^V \}$</p></li><li><p>令 $\tilde D^v$ 表示 $\tilde D$ 中在属性 $a$ 取值为 $a^v$ 的样本子集，则 $\tilde D  = \bigcup_{v=1}^{V}\tilde D^v.$</p></li><li><p>令 $\tilde D_k$ 表示 $\tilde D$ 中属于第  $k$ 类 $(k = 1,2 , … , |y|)$ 的样本子集 $\tilde D  = \bigcup_{k=1}^{|y|}\tilde D_k.$</p></li><li><p>假定为每个样本 $x$ 赋予一个权重 $w_x$, 并定义：</p><ul><li><p>$\rho$ 表示无缺失值样本所占的比例：</p><script type="math/tex; mode=display">\rho = \frac{\sum_{x\in\tilde D}w_x}{\sum_{x\in D}w_x}</script></li><li><p>$\tilde p_k$ 表示无缺失值样本中第 $k$ 类所占的比例：</p><script type="math/tex; mode=display">\tilde p_k = \frac{\sum_{x\in\tilde D_k}w_x}{\sum_{x\in\tilde D}w_x}</script><p>显然：$\sum_{k=1}^{|y|}\tilde p_k = 1$</p></li><li><p>$\tilde r_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例：</p><script type="math/tex; mode=display">\tilde r_v = \frac{\sum_{x\in\tilde D^v}w_x}{\sum_{x\in\tilde D}w_x}</script><p>显然：$\sum_{v=1}^{V}\tilde r_v = 1$</p></li></ul></li><li><p>基于上述定义，我们可以将信息增益的计算式推广为：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gain(D, a) &= \rho \times Gain(\tilde D, a)\\&= \rho \times (Ent(\tilde D) - \sum_{v =1}^{V}\tilde r_v Ent(\tilde D^v))\end {split}\end{equation}</script><p>其中：</p><script type="math/tex; mode=display">Ent(\tilde D) = -\sum _{k=1}^{|y|}{\tilde p_k \log_2 \tilde p_k}</script></li></ul><h5 id="给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？"><a href="#给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？" class="headerlink" title="给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？"></a>给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？</h5><ul><li><strong>若样本 $x$ 在划分属性 $a$ 上的取值已知：</strong> 则将 $x$ 划入与其取值对应的子结点，且样本权值在子结点中保持为 $w_x$ </li><li><strong>若样本 $x$ 在划分属性 $a$ 上的取值未知：</strong> 则将 $x$ 同时划分到所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde r^v \cdot w_x$ </li></ul><hr><h3 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h3><p><strong>多变量决策树 (multivariate decision tree)：</strong></p><ul><li>能实现 <strong>“斜划分”</strong> 甚至更复杂划分的决策树</li><li>非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试</li><li>在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（四）-决策树&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（四）-决策树&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（四） 决策树&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（四） 决策树&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
</feed>
