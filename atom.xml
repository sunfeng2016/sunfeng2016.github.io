<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunFeng&#39;s Blog</title>
  
  <subtitle>学习，敲码，孤独终老！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunfeng.online/"/>
  <updated>2019-09-06T11:38:02.863Z</updated>
  <id>http://sunfeng.online/</id>
  
  <author>
    <name>SunFeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础</title>
    <link href="http://sunfeng.online/2019/09/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89--%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://sunfeng.online/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/</id>
    <published>2019-09-05T09:41:52.000Z</published>
    <updated>2019-09-06T11:38:02.863Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（2） 神经网络基础</font></strong></p><a id="more"></a><hr><h3 id="二分类-Binary-Classification"><a href="#二分类-Binary-Classification" class="headerlink" title="二分类 (Binary Classification)"></a>二分类 (Binary Classification)</h3><p><strong>神经网络的训练过程：</strong></p><ul><li>先有一个叫做前向暂停 <strong>(forward pause)</strong> 或前向传播 <strong>(forward propagation)</strong> 的步骤</li><li>接着有一个叫做反向暂停 <strong>(backward pause)</strong> 或反向传播 <strong>(backward propagation)</strong> 的步骤</li></ul><p><strong>一个二分类的例子：</strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure1.PNG" alt="figure1"></p><p><strong>输入：</strong> 一张图片</p><p><strong>输出：</strong> 1 (识别这张图片含有猫)、0 (识别这张图片不含猫)</p><p><strong>符号定义：</strong></p><ul><li>$x$: 表示一个 $n_x$ 维数据，为输入数据，维度为 $(n_x,1)$</li><li>$y$: 表示输出结果，取值为 $(0,1)$</li><li>$(x^{(i)},y^{(i)})$: 表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据</li><li>$X = [x^{(1)},x^{(2)},…,x^{(m)}]$: 表示所有训练数据集的输入值，放在一个 $n_x \times m$ 的矩阵中，$m$ 表示样本数目</li><li>$Y = [y^{(1)},y^{(2)},…,y^{(m)}]$: 对应所有训练数据集的输出值，维度为 $1\times m$</li></ul><hr><h3 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归 (Logistic Regression)"></a>逻辑回归 (Logistic Regression)</h3><p><strong><font color="#0099ff">Logistic 回归是一个用于二分类的算法</font></strong></p><p><strong>Logistic 回归中使用的参数如下：</strong></p><ul><li><p>输入的特征向量：$x \in R^{n_x}$, 其中 $n_x$ 是特征数量</p></li><li><p>用于训练的标签：$y \in \{0,1\}$</p></li><li><p>权重：$w\in R^{n_x}$</p></li><li><p>偏置：$b\in R$</p></li><li><p>输出：$\hat y = \sigma(w^Tx+b)$</p></li><li><p>激活函数：Sigmoid 函数</p><script type="math/tex; mode=display">s = \sigma(w^Tx+b) = \sigma(z) = \frac{1}{1+e^{-z}}</script><p>为将 $w^Tx+b$ 约束在 [0,1]间，引入 Sigmoid函数</p></li></ul><p><strong><font color="#0099ff">Logistic 回归可以看作是一个非常小的神经网络，下图是一个典型的例子：</font></strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure2.PNG" alt="figure2"></p><hr><h3 id="逻辑回归的代价函数-Logistic-Regression-Cost-Function"><a href="#逻辑回归的代价函数-Logistic-Regression-Cost-Function" class="headerlink" title="逻辑回归的代价函数 (Logistic Regression Cost Function)"></a>逻辑回归的代价函数 (Logistic Regression Cost Function)</h3><p><strong>损失函数 (loss function)</strong></p><ul><li><font color="#0099">用于衡量预测结果与真实值之间的误差</font></li><li><p>最简单的损失函数定义方式为平方差损失：</p><script type="math/tex; mode=display">L(\hat y,y) = \frac{1}{2}(\hat y-y)^2</script><font color="#0099">但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法找不到全局最优值</font></li><li><p>一般使用交叉熵损失:</p><script type="math/tex; mode=display">L(\hat y,y) = -y\log(\hat y)-(1-y)\log(1-\hat y)</script></li><li><font color="#0099">损失函数是在单个训练样本中定义的，它衡量了算法在单个训练样本上的表现</font></li></ul><p><strong>代价函数 (Cost function)</strong></p><ul><li><font color="#0099">代价函数用于衡量算法在全部样本上的表现</font></li><li><p>代价函数是对 $m$ 个样本的损失函数求和然后除以 $m$:</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})= \frac{1}{m}\left(-y^{(i)}\log(\hat y^{(i)})-(1-y^{(i)})\log(1-\hat y^{(i)})\right)</script></li></ul><hr><h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 (Gradient Descent)"></a>梯度下降 (Gradient Descent)</h3><p><strong>梯度 (gradient)</strong></p><ul><li>梯度是一个向量，其方向是函数增长最快的方向，其大小表示函数的最大增长速度</li><li>按梯度的方向走，函数增长的最快</li><li>按梯度的负方向走，函数降低的最快</li></ul><p><strong>模型训练目标</strong></p><ul><li>模型的训练目标即是寻找合适的 $w$ 和 $b$ 以最小化代价函数值</li><li>简单起见，先假设 $w$ 与 $b$ 都是一维实数，那么可以得到函数 $J$ 关于 $w$ 和 $b$ 的图如下：</li></ul><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure3.PNG" alt="figure3"></p><ul><li><font color="#0099">成本函数 J 是一个凸函数，与非凸函数的区别在于其不含有多个局部最低点，选择这样的代价函数就保证了无论怎样初始化模型参数值，都能寻找到合适的最优解</font></li></ul><p><strong>参数更新：</strong></p><ul><li><p>参数 $w$ 的更新公式：</p><script type="math/tex; mode=display">w := w-\alpha\frac{\partial J(w,b)}{\partial w}</script></li><li><p>参数 $b$ 的更新公式：</p><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial J(w,b)}{\partial b}</script><p>其中， $\alpha$ 为学习率，控制步长</p></li></ul><hr><h3 id="逻辑回归中的梯度下降-Logistic-Regression-Gradient-Descent"><a href="#逻辑回归中的梯度下降-Logistic-Regression-Gradient-Descent" class="headerlink" title="逻辑回归中的梯度下降 (Logistic Regression Gradient Descent)"></a>逻辑回归中的梯度下降 (Logistic Regression Gradient Descent)</h3><p><strong>问题假设：</strong></p><ul><li>假设样本只有两个特征 $x_1$ 和 $x_2$</li></ul><p><strong>参数回顾：</strong></p><ul><li>权值：$w_1$ 和 $w_2$</li><li>偏置：$b$</li><li>线性回归输出：$z = w_1x_1+w_2x_2+b$</li><li>逻辑回归输出：$\hat y  = a = \sigma(z)$, 其中 $z = w_1x_1+w_2x_2+b$, $\sigma(z) = \frac{1}{1-e^{-z}}$</li><li>损失函数：$L(\hat y^{(i)},y^{(i)}) = -y^{(i)}\log(\hat y^{(i)})-(1-y^{(i)})\log(1-\hat y^{(i)})$</li><li>代价函数：$J(w,b) = \frac{1}{m}\sum_{i =1}^{m}L(\hat y^{(i)},y^{(i)})$</li></ul><p><strong>单个样本的梯度下降：</strong></p><ul><li><p>根据上述假设，输入参数有5个：$x_1$, $x_2$, $w_1$, $w_2$ 和 $b$, 可以推导出如下的计算图：</p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure4.PNG" alt="figure4"></p></li><li><p>其中 $L(a,y) = -y\log(a)-(1-y)\log(1-a)$</p></li><li><p>首先反向求出 $L$ 对于 $a$ 的导数:</p><script type="math/tex; mode=display">da = \frac{d L}{d a} = -y/a+(1-y)/(1-a)</script></li><li><p>然后继续求出 $L$ 对于 $z$ 的导数：</p><script type="math/tex; mode=display">\begin{align}dz = \frac{d L}{d z}&= \frac{d L}{d a} \cdot\frac{d a}{d z}\\&= \left( -y/a+(1-y)/(1-a) \right)\cdot\left(a(1-a) \right)\\&= a- y\end {align}</script></li><li><p>最终求出 $L$ 对于参数 $w$ 和 $b$ 的导数：</p><script type="math/tex; mode=display">\begin {align}dw_1 = \frac{\partial L}{\partial w_1} &= \frac{dL}{dz}\cdot\frac{dz}{dw_1} \\&= (a-y) \cdot x_1 \\dw_2 = \frac{\partial L}{\partial w_2} &= \frac{dL}{dz}\cdot\frac{dz}{dw_2} \\&= (a-y) \cdot x_2 \\db = \frac{\partial L}{\partial w_1} &= \frac{dL}{dz}\cdot\frac{dz}{db} \\&= (a-y) \end {align}</script></li><li><p>根据如下公式进行参数更新：</p><script type="math/tex; mode=display">\begin {align}w_1 &:= w_1-\alpha dw_1 \\w_2 &:= w_2-\alpha dw_2 \\b &:= b - \alpha db\end {align}</script></li></ul><p><strong>$m$个样本的梯度下降：</strong></p><ul><li><p>接下来，我们需要将对于单个样本的损失函数扩展到整个训练集的代价函数：</p><script type="math/tex; mode=display">J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)})\\a^{(i)} = \hat y^{(i)} = \sigma(z^{(i)})= \sigma(w^Tx^{(i)}+b)</script></li><li><p>我们可以对于某个权重参数 $w_1$, 其导数计算为:</p><script type="math/tex; mode=display">\frac{\partial J(w,b)}{\partial w_1} = \frac{1}{m}\sum_{i=1}^{m}\frac{\partial L(a^{(i)},y^{(i)})}{\partial w_1}</script></li></ul><p><strong>完成的逻辑回归中某次训练流程如下，这里仅假设特征向量的维度为2：</strong></p><p><img src="/2019/09/05/吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础/figure5.PNG" alt="figure5"></p><ul><li>然后，对 $w_1$, $w_2$ 和 $b$ 进行迭代</li><li><font color="#ff00">上述过程在计算时有一个缺点：需要编写两个 for 循环</font><ul><li>第一个 for 循环用于遍历所有的样本</li><li>第二个 for 循环用于遍历所有的特征</li></ul></li></ul><hr><h3 id="向量化-Vectorization"><a href="#向量化-Vectorization" class="headerlink" title="向量化 (Vectorization)"></a>向量化 (Vectorization)</h3><ul><li><p>在逻辑回归中，需要计算:</p><script type="math/tex; mode=display">z = w^Tx+b</script></li><li><p>如果是非向量化的循环方式，代码可能如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_x):</span><br><span class="line">    z += w[i] * x[i]</span><br><span class="line">z += b</span><br></pre></td></tr></table></figure></li><li><p>而如果是项量化的操作，代码则会简介很多，并带来近百倍性能的提升(并行指令):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></li><li><p>不用显示 for 循环，实现逻辑回归的梯度下降一次迭代 (对应之前蓝色代码的for循环部分，这里公式和 Numpy 的代码混杂，注意分辨)：</p><script type="math/tex; mode=display">\begin {align}Z &= w^TX+b=np.dot(w.T,x)+b\\A &= \sigma(Z)\\dZ &= A-Y\\dw &= \frac{1}{m}XdZ^T\\db &= \frac{1}{m}np.sum(dZ)\\w &:= w-\sigma dw\\b &:= b -\sigma db\end {align}</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（2） 神经网络基础&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述</title>
    <link href="http://sunfeng.online/2019/08/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"/>
    <id>http://sunfeng.online/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/</id>
    <published>2019-08-30T08:28:59.000Z</published>
    <updated>2019-09-05T11:32:02.690Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">吴恩达《神经网络与深度学习》课程笔记（1） 深度学习概述</font></strong></p><a id="more"></a><hr><h3 id="什么是神经网络？-What-is-a-Neural-Network"><a href="#什么是神经网络？-What-is-a-Neural-Network" class="headerlink" title="什么是神经网络？(What is a Neural Network ?)"></a>什么是神经网络？(What is a Neural Network ?)</h3><p><strong>深度学习：</strong></p><ul><li>指训练神经网络的过程</li><li>有时候特指训练大规模的神经网络</li></ul><p><strong>神经网络：</strong> 一个房价预测的例子</p><p><img src="/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/figure1.PNG" alt="figure1"></p><h3 id="神经网络的监督学习-Supervised-Learning-with-Neural-Network"><a href="#神经网络的监督学习-Supervised-Learning-with-Neural-Network" class="headerlink" title="神经网络的监督学习 (Supervised Learning with Neural Network)"></a>神经网络的监督学习 (Supervised Learning with Neural Network)</h3><p><strong>常见神经网络的监督学习案例：</strong></p><p><img src="/2019/08/30/吴恩达《神经网络与深度学习》课程笔记（1）-- 深度学习概述/figure2.PNG" alt="figure2"></p><h3 id="为什么深度学习会兴起？-Why-the-Deep-Learning-taking-off"><a href="#为什么深度学习会兴起？-Why-the-Deep-Learning-taking-off" class="headerlink" title="为什么深度学习会兴起？(Why the Deep Learning taking off ?)"></a>为什么深度学习会兴起？(Why the Deep Learning taking off ?)</h3><ul><li><strong>数据</strong> (Data)</li><li><strong>计算能力</strong> (Computation)</li><li><strong>算法</strong> (Algorithm)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;吴恩达《神经网络与深度学习》课程笔记（1） 深度学习概述&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络与深度学习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)--10个测试题</title>
    <link href="http://sunfeng.online/2019/08/26/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%B8%80%E5%91%A8(Introduction%20to%20Deep%20Learning)--10%E4%B8%AA%E6%B5%8B%E8%AF%95%E9%A2%98/"/>
    <id>http://sunfeng.online/2019/08/26/课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)--10个测试题/</id>
    <published>2019-08-26T12:48:46.000Z</published>
    <updated>2019-08-30T09:11:33.537Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)—10个测试题</font></strong></p><a id="more"></a><p>1、What does the analogy “AI is the new electricity” refer to?  (B)</p><p>A. Through the “smart grid”, AI is delivering a new wave of electricity.</p><p>B. Similar to electricity starting about 100 years ago, AI is transforming multiple industries.</p><p>C. AI is powering personal devices in our homes and offices, similar to electricity.</p><p>D. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.</p><p>2、Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)  (A、B、D)</p><p>A. We have access to a lot more data.</p><p>B. We have access to a lot more computational power.</p><p>C. Neural Networks are a brand new field.</p><p>D. Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition.</p><p>3、Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.) (A、B、D)</p><p>A. Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.</p><p>B. Faster computation can help speed up how long a team takes to iterate to a good idea.</p><p>C. It is faster to train on a big dataset than a small dataset.</p><p>D. Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).</p><p>4、When an experienced deep learning engineer works on a new problem, they can usually use insight from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False?  (B)</p><p>A. True</p><p>B. False</p><p>5、Which one of these plots represents a ReLU activation function? (C)</p><p>A. Figure 1:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152503347-1898711678.png" alt="img"></p><p>B. Figure 2:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152527300-994610622.png" alt="img"></p><p>C. Figure 3:</p><p> <img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152536956-442890691.png" alt="img"></p><p>D. Figure4</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152603315-902589773.png" alt="img"></p><p>6、Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False? (B)</p><p>A. True</p><p>B. False</p><p>7、A demographic dataset with statistics on different cities’ population, GDP per capita, economic growth is an example of “unstructured” data because it contains data coming from different sources. True/False?(B)</p><p>A. True</p><p>B. False</p><p>8、Why is an RNN (Recurrent Neural Network) used for machine translation, say translating English to French? (Check all that apply.) (A、C)</p><p>A. It can be trained as a supervised learning problem.</p><p>B. It is strictly more powerful than a Convolutional Neural Network (CNN).</p><p>C. It is applicable when the input/output is a sequence (e.g., a sequence of words).</p><p>D. RNNs represent the recurrent process of Idea-&gt;Code-&gt;Experiment-&gt;Idea-&gt;….</p><p>9、In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent? (A)</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127152854394-1385682683.png" alt="img"></p><p>A.</p><p>x-axis is the amount of data<br>y-axis (vertical axis) is the performance of the algorithm.</p><p>B.</p><p>x-axis is the performance of the algorithm<br>y-axis (vertical axis) is the amount of data.</p><p>C.</p><p>x-axis is the amount of data<br>y-axis is the size of the model you train.</p><p>D.</p><p>x-axis is the input to the algorithm<br>y-axis is outputs.</p><p>10、Assuming the trends described in the previous question’s figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.) (A、C)</p><p>A. Increasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>B. Decreasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>C. Increasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.</p><p>D. Decreasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;课程一(Neural Networks and Deep Learning)，第一周(Introduction to Deep Learning)—10个测试题&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日乘子法(Lagrange Multiplier)和KKT条件</title>
    <link href="http://sunfeng.online/2019/08/26/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95(Lagrange%20Multiplier)%E5%92%8CKKT%E6%9D%A1%E4%BB%B6/"/>
    <id>http://sunfeng.online/2019/08/26/拉格朗日乘子法(Lagrange Multiplier)和KKT条件/</id>
    <published>2019-08-26T08:28:13.000Z</published>
    <updated>2019-08-30T09:12:15.950Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="4">深入理解拉格朗日乘子法和KKT条件</font></strong></p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>在求解最优化问题中，<strong>拉格朗日乘子法</strong> (Lagrange Multiplier) 和 <strong>KKT</strong> (Karush Kuhn Tucker) <strong>条件</strong> 是两种常用的方法。</li><li>在有等式约束时使用拉格朗日乘子法</li><li>在有不等式约束时使用KKT条件</li></ul><hr><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><p>&ensp;&ensp; 我们这里提到的 <font color="#0099ff">最优化问题是指对于给定的某一函数，求其在指定作用域上的全局最小值 (因为最小值与最大值可以很容易转化，即最大值问题可以转化为最小值问题)。</font></p><p>&ensp;&ensp; 一般情况下，最优化问题可分为以下三种情况：</p><h3 id="无约束条件"><a href="#无约束条件" class="headerlink" title="无约束条件"></a>无约束条件</h3><ul><li>这是最简单的情况，解决方法通常是函数对变量求导，令求导函数等于0的点可能是极值点。将极值点带回原函数进行验证即可</li></ul><h3 id="等式约束条件"><a href="#等式约束条件" class="headerlink" title="等式约束条件"></a>等式约束条件</h3><p><strong>问题描述</strong></p><ul><li><p>设目标函数为 $f(x)$</p></li><li><p>约束条件为 $h_k(x)$</p></li><li><p>求：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\min}_{x}f(x)\\&s.t\ \ h_k(x) = 0\ \ k=1,2,...,l\end {align}\tag{1}</script><p>此问题的解决方法是 <font color="#0099ff">消元法或拉格朗日法</font></p></li></ul><p><strong>拉格朗日乘子法</strong></p><p>&ensp;&ensp;<font color="#0099ff">拉格朗日乘子法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法</font>。</p><p>&ensp;&ensp;<font color="#0099ff">这种方法可以将一个拥有 n 个变量与 k 个约束条件的最优化问题转换为一个求解有 n+k 个变量的方程组的解的问题。</font></p><ul><li><p>首先定义拉格朗日函数：</p><script type="math/tex; mode=display">F(x,\lambda) = f(x)+\sum_{k=1}^l\lambda_kh_k(x)\tag{2}</script><p>其中，$\lambda_k$ 是各个约束条件的待定系数, 称作 <strong>拉格朗日乘数</strong></p></li><li><p>然后解变量的偏导数方程：</p><script type="math/tex; mode=display">\begin {align}\frac{\partial F}{\partial x} &= 0 \\\frac{\partial F}{\partial \lambda_1} &= 0\\&\vdots\\\frac{\partial F}{\partial \lambda_l} &= 0\end {align}\tag{3}</script><p>如果有 $l$ 个约束条件，则有 $l+1$ 个方程。求出方程的解就可能是最优化解。</p></li></ul><p><strong>一个例子</strong></p><ul><li><p>给定一个椭球：</p><script type="math/tex; mode=display">\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1\tag{4}</script></li><li><p>求这个椭球的内接长方体的最大体积</p></li><li><p>此问题实际上就是条件极值问题，即在条件 $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ 求 $f(x,y,z) = 8xyz$ 的最大值</p></li><li><p>用拉格朗日法求解该问题：</p><script type="math/tex; mode=display">\begin {align}F(x,y,z,\lambda) &= f(x,y,z)+\lambda\varphi(x,y,z)\\&= 8xyz+\lambda\left(\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1 \right)\end {align}\tag{5}</script></li><li><p>对 $F(x,y,z,\lambda)$ 求偏导得到：</p><script type="math/tex; mode=display">\begin {align}\frac{\partial F(x,y,z,\lambda)}{\partial x} &= 8yz+\frac{2\lambda x}{a^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial y} &= 8xz+\frac{2\lambda y}{b^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial z} &= 8xy+\frac{2\lambda z}{a^2} =0\\\frac{\partial F(x,y,z,\lambda)}{\partial \lambda} &= \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1 = 0\end {align}\tag{6}</script></li><li><p>求解方程 (6) 可得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}x = \frac{\sqrt 3}{3}a\\y = \frac{\sqrt 3}{3}b\\z = \frac{\sqrt 3}{3}c\end {cases}\end {equation}\tag{7}</script></li><li><p>带入解得最大体积为：</p><script type="math/tex; mode=display">V_{max} = f\left(\frac{\sqrt 3}{3}a, \frac{\sqrt 3}{3}b, \frac{\sqrt 3}{3}c\right)= \frac{8\sqrt 3}{9}abc\tag{8}</script></li></ul><p><strong>直观解释</strong></p><p>&ensp;&ensp;举个二维最优化的例子：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\min}_{x,y}f(x,y)\\&s.t\ \ g(x,y) = c\end {align}\tag{9}下图是 $z = f(x,y)$ 的等高线：</script><p>&ensp;下图是 $z = f(x,y)$ 的等高线：</p><p><img src="https://pic002.cnblogs.com/images/2012/103496/2012101621500549.png" alt="img"></p><p>&ensp;&ensp;绿线是约束条件 $g(x,y) = c$ 的轨迹；蓝线是 $f(x,y)$ 的等高线；箭头表示斜率，和等高线的发现平行。从梯度方向上来看，显然 $d_1 &gt; d_2$</p><p>&ensp;&ensp;<font color="#0099">绿色的线表示约束，也就是说，只要正好落在这条绿线上的点才可能是满足要求的点。如果没有这条约束，f(x,y)的最小值应该会是会落在最小那圈等高线内部的某一点上。</font></p><p>&ensp;&ensp;<font color="#099ff">而现在加上了约束条件，最小值点应该是 f(x,y)等高线和约束条件相切的位置，因为如果只是相交意味着肯定还存在其他等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小</font></p><p>&ensp;&ensp;<font color="#ff0000">只有到等高线与约束条件的函数曲线相切的时候，才可能取到最优值</font></p><p>&ensp;&ensp;如果我们对约束也求梯度 $\nabla g(x,y)$, 则其梯度如图中绿色箭头所示；<strong>显然，当目标函数 $f(x,y)$ 的等高线和约束相切，则它们切点的梯度一定在一条直线上($f$ 和 $g$ 的斜率平行)</strong></p><p>&ensp;&ensp;即在最优解的时候: $\nabla f(x,y) = \lambda(\nabla g(x,y) - c)$, 即 $\nabla[f(x,y) + \lambda(g(x,y)-c) ] = 0$</p><p>&ensp;&ensp;那么拉格朗日函数： $F(x,y) = f(x,y)+\lambda(g(x,y)-c)$ 在达到极值时与 $f(x,y)$ 相等，因为 $g(x,y)-c$ 总为0</p><h3 id="不等式约束条件"><a href="#不等式约束条件" class="headerlink" title="不等式约束条件"></a>不等式约束条件</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;font size=&quot;4&quot;&gt;深入理解拉格朗日乘子法和KKT条件&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数学方法整理" scheme="http://sunfeng.online/categories/%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86/"/>
    
    
      <category term="SVM" scheme="http://sunfeng.online/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（十）-- 降维与度量学习</title>
    <link href="http://sunfeng.online/2019/08/22/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89--%20%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://sunfeng.online/2019/08/22/《机器学习》西瓜书学习笔记（十）-- 降维与度量学习/</id>
    <published>2019-08-22T07:49:17.000Z</published>
    <updated>2019-08-23T08:11:35.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（十）降维与度量学习"><a href="#《机器学习》西瓜书学习笔记（十）降维与度量学习" class="headerlink" title="《机器学习》西瓜书学习笔记（十）降维与度量学习"></a>《机器学习》西瓜书学习笔记（十）降维与度量学习</h2><a id="more"></a><h3 id="k-近邻学习"><a href="#k-近邻学习" class="headerlink" title="k 近邻学习"></a>k 近邻学习</h3><p><strong>k 近邻简介：</strong></p><ul><li>k 近邻 (k-Nearest Neighbor, 简称KNN)</li><li>一种常用的监督学习方法</li></ul><p><strong>k 近邻思想：</strong></p><ul><li>给定测试样本，基于某种距离度量找出训练集中与其最靠近的 $k$ 个训练样本</li><li>然后基于这 $k$ 个邻居的信息来进行预测</li><li>在分类任务中，可采用 “投票法”<ul><li>即选择 $k$ 个样本中出现最多的类别标记作为预测结果</li></ul></li><li>在回归任务中，可采用 “平均法”<ul><li>即将这 $k$ 个样本的实值输出标记的平均值作为预测结果</li></ul></li></ul><p><strong>k 近邻特点：</strong> </p><ul><li>没有显式的训练过程</li></ul><hr><h3 id="低维嵌入"><a href="#低维嵌入" class="headerlink" title="低维嵌入"></a>低维嵌入</h3><h4 id="维数灾难-curse-of-dimensionality"><a href="#维数灾难-curse-of-dimensionality" class="headerlink" title="维数灾难 (curse of dimensionality)"></a>维数灾难 (curse of dimensionality)</h4><ul><li>在高维情形下出现的数据样本稀疏、距离计算困难等问题</li></ul><h4 id="降维-dimension-reduction"><a href="#降维-dimension-reduction" class="headerlink" title="降维 (dimension reduction)"></a>降维 (dimension reduction)</h4><ul><li>缓解维数灾难的一个重要途径</li><li>通过某种数学变换将原始高维属性空间转变为一个低维 “子空间” </li><li>在子空间中样本密度大幅提高，距离计算也变得更为容易</li></ul><h4 id="多维缩放-Multiple-Dimensional-Scaling，简称-MDS"><a href="#多维缩放-Multiple-Dimensional-Scaling，简称-MDS" class="headerlink" title="多维缩放 (Multiple Dimensional Scaling，简称 MDS)"></a>多维缩放 (Multiple Dimensional Scaling，简称 MDS)</h4><ul><li><p>设 $m$ 个样本在原始空间的距离矩阵为 $\boldsymbol D\in {\Bbb R}^{m\times m}$</p></li><li><p>其第 $i$ 行 $j$ 列元素 $dist_{ij}$ 为样本 $\boldsymbol x_i$ 到 $\boldsymbol x_j$ 的距离</p></li><li><p>目标是获得样本在 $d\prime$ 维空间的表示 $\boldsymbol Z\in {\Bbb R}^{d\prime\times m}$, $d\prime \le d$</p></li><li><p>且任意两个样本在 $d\prime$ 维空间中欧式距离等于在原始空间中的距离，即 $\left|z_i-z_j \right| = dist_{ij}$</p></li><li><p>令 $\boldsymbol B$ 为降维后样本的内积矩阵，即 $\boldsymbol B = \boldsymbol Z^T\boldsymbol Z \in {\Bbb R}^{m\times m}$,  其中，$b_{ij} = z_i^Tz_j$</p></li><li><p>则：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}dist_{ij}^2 &= \left\| z_i\right\|^2 +\left\| z_j\right\|^2 -2z_i^Tz_j\\&=b_{ii} + b_{jj}-2b_{ij}\end{split}\tag{1}\end{equation}</script></li><li></li><li><p>则矩阵 $\boldsymbol B$ 的行和列之和为零：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}\sum_{i=1}^{m} b_{ij} &= \sum_{i=1}^{m}(z_i^Tz_j) \\&=z_j\sum_{i=1}^{m}z_i^T\\&= 0\\\sum_{j=1}^{m} b_{ij} &= \sum_{j=1}^{m}(z_i^Tz_j) \\&= z_i\sum_{j=1}^{m}z_j^T\\&= 0\end {split}\end {equation}</script></li><li><p>易知：</p><script type="math/tex; mode=display">\sum_{i=1}^{m}dist_{ij}^2 = \text tr(\boldsymbol B) + mb_{jj}\tag{2}</script><script type="math/tex; mode=display">\sum_{j=1}^{m}dist_{ij}^2 = \text tr(\boldsymbol B) + mb_{ii}\tag{3}</script><script type="math/tex; mode=display">\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 = 2m \ \text tr(\boldsymbol B)\tag{4}</script><p>其中，$\text{tr} (\cdot)$ 表示矩阵的迹，$\text{tr} = \sum_{i=1}^{m}{\left|z_i \right|}$</p></li><li><p>令：</p><script type="math/tex; mode=display">\begin{align}dist_{i \cdot}^2 &= \frac{1}{m}\sum_{j=1}^{m}dist_{ij}^2 \tag{5} \\dist_{\cdot j}^2 &= \frac{1}{m}\sum_{i=1}^{m}dist_{ij}^2 \tag{6} \\dist_{\cdot \cdot}^2 &= \frac{1}{m^2}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 \tag{7}\\\end{align}</script></li><li><p>由式(1)和(2)~(7)可得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)\tag{8}</script><p>推导过程如下：</p><p>由式(1)得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}-b_{ii}-b_{jj})</script><p>由式(4)和(7)得：</p><script type="math/tex; mode=display">\begin{align}\text tr(\boldsymbol B) &= \frac{1}{2m}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2\\&=\frac{1}{2m} \cdot m^2 dist_{\cdot \cdot}^2 \\&=\frac{m}{2}dist_{\cdot \cdot}^2\end{align}</script><p>由式(2)和(6)得：</p><script type="math/tex; mode=display">\begin{align}b_{jj} &= \frac{1}{m}\sum_{i=1}^{m}dist_{ij}^2 - \frac{1}{m}\text tr(\boldsymbol B)\\&=dist_{\cdot j}^2 - \frac{1}{2}dist_{\cdot \cdot}^2\end{align}</script><p>由式(3)和(5)得:</p><script type="math/tex; mode=display">\begin{align}b_{jj} &= \frac{1}{m}\sum_{j=1}^{m}dist_{ij}^2 - \frac{1}{m}\text tr(\boldsymbol B)\\&=dist_{i\cdot}^2 - \frac{1}{2}dist_{\cdot \cdot}^2\end{align}</script><p>综上得：</p><script type="math/tex; mode=display">b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)</script></li><li><p><strong>由此可通过降维前后保持不变的距离矩阵 $\boldsymbol D$ 求得内积矩阵 $\boldsymbol B$ </strong></p></li><li><p>对矩阵 $\boldsymbol B$ 做特征值分解 $\boldsymbol B = \boldsymbol V \boldsymbol \Lambda \boldsymbol V^T$</p></li><li><p>其中 $\Lambda = \text{diag}(\lambda_1,\lambda_2,….,\lambda_d)$ 为特征值构成的特征矩阵，$\lambda_1\ge\lambda_2\ge…\ge\lambda_d$ </p></li><li><p>$\boldsymbol V $ 为特征向量矩阵</p></li><li><p>取 $d\prime &lt;&lt; d$ 个最大特征值构成对角矩阵 $\tilde{\Lambda} = \text{diag}(\lambda_1,\lambda_2,….,\lambda_{d\prime})$ </p></li><li><p>令 $\tilde{\boldsymbol V }$ 为相应的特征向量矩阵</p></li><li><p>则 $\boldsymbol Z$ 可表达为:</p><script type="math/tex; mode=display">\boldsymbol Z = \tilde{\Lambda}^{\frac{1}{2}}\tilde{\boldsymbol V }^T\ \in {\Bbb R}^{d\prime \times m}\tag{9}</script></li></ul><p><strong>MDS 算法描述：</strong></p><p><img src="/2019/08/22/《机器学习》西瓜书学习笔记（十）-- 降维与度量学习/figure1.PNG" alt="figure1"></p><h4 id="线性降维"><a href="#线性降维" class="headerlink" title="线性降维"></a>线性降维</h4><ul><li><p>对原始高维空间进行线性变换，得到低维子空间</p></li><li><p>给定 $d$ 维空间中的样本 $\boldsymbol X = (\boldsymbol x_1, \boldsymbol x_2,…, \boldsymbol x_m ) \in {\Bbb R}^{d\times m}$</p></li><li><p>变换之后得到 $d\prime \le d$ 维空间中的样本</p><script type="math/tex; mode=display">\boldsymbol Z = \boldsymbol W^T\boldsymbol X\tag{10}</script></li><li><p>$\boldsymbol W \in {\Bbb R}^{d\times d\prime}$ 是变换矩阵</p></li><li><p>$\boldsymbol Z \in {\Bbb R}^{d\times m}$ 是样本在新空间中的表达</p></li></ul><hr><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><hr><h3 id="核化线性降维"><a href="#核化线性降维" class="headerlink" title="核化线性降维"></a>核化线性降维</h3><hr><h3 id="流形学习"><a href="#流形学习" class="headerlink" title="流形学习"></a>流形学习</h3><h4 id="等度量映射"><a href="#等度量映射" class="headerlink" title="等度量映射"></a>等度量映射</h4><h4 id="局部线性嵌入"><a href="#局部线性嵌入" class="headerlink" title="局部线性嵌入"></a>局部线性嵌入</h4><hr><h3 id="度量学习"><a href="#度量学习" class="headerlink" title="度量学习"></a>度量学习</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（十）降维与度量学习&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（十）降维与度量学习&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（十）降维与度量学习&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（十）降维与度量学习&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（九）-- 聚类</title>
    <link href="http://sunfeng.online/2019/08/19/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89--%20%E8%81%9A%E7%B1%BB/"/>
    <id>http://sunfeng.online/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/</id>
    <published>2019-08-19T08:41:05.000Z</published>
    <updated>2019-08-22T07:44:09.400Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（九）-聚类"><a href="#《机器学习》西瓜书学习笔记（九）-聚类" class="headerlink" title="《机器学习》西瓜书学习笔记（九） 聚类"></a>《机器学习》西瓜书学习笔记（九） 聚类</h2><a id="more"></a><h3 id="聚类任务"><a href="#聚类任务" class="headerlink" title="聚类任务"></a>聚类任务</h3><p><strong>聚类</strong> (clustering)</p><ul><li>无监督学习 (unsupervised learning) 的一种</li><li>试图将数据划分为若干个不相交的子集，每个子集为一个 <strong>簇</strong> (cluster)</li><li>每个簇可能对应于一些潜在的概念(类别)</li></ul><p><strong>聚类的形式化解释</strong></p><ul><li>样本集 $D = \{\boldsymbol x_1,  \boldsymbol x_2,…,\boldsymbol x_m\}$ 包含 $m$ 个样本</li><li>每个样本 $\boldsymbol x_i = (x_{i1}; x_{i2}; …;x_{in})$ 是一个 $n$ 维向量</li><li>聚类算法将样本 $D$ 划分为 $k$ 个不相交的簇 $\{C_l  |  l = 1,2,…,k \}$, 其中 $C_i \bigcap C_j = \emptyset$, $  (i \ne j)$ 且 $D = \bigcup_{l=1}^{k}C_l$</li><li>用 $\lambda_j \in \{1,2,…,k \}$ 表示样本 $\boldsymbol x_j$ 的 “簇标记”，即 $\boldsymbol x_j \in C_{\lambda_j}$ </li><li>聚类结果 $\boldsymbol \lambda = \{\lambda_1;\lambda_2;…;\lambda_m\}$</li></ul><hr><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><h4 id="外部指标-external-index"><a href="#外部指标-external-index" class="headerlink" title="外部指标 (external index)"></a>外部指标 (external index)</h4><p><strong>变量符号定义：</strong></p><ul><li><p>数据集 $D = \{\boldsymbol x_1,  \boldsymbol x_2,…,\boldsymbol x_m\}$</p></li><li><p>聚类的簇划分 ${\cal C} = \{C_1,C_2,…, C_k\}$，簇标记向量 $\boldsymbol \lambda$  </p></li><li><p>参考模型给出的簇划分 ${\cal C^{*}} = \{C_1^{*}, C_2^{*},…, C_k^{*}\}$，簇标记向量 $\boldsymbol \lambda^{*}$</p></li><li><p>定义：</p><script type="math/tex; mode=display">a = |SS|,\ \ SS=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i = \lambda_j,\lambda_i^* = \lambda_j^*, i < j\} \tag{1}</script><script type="math/tex; mode=display">b = |SD|,\ \ SD=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i = \lambda_j,\lambda_i^* \ne \lambda_j^*, i < j\} \tag{2}</script><script type="math/tex; mode=display">c = |DS|,\ \ DS=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i \ne \lambda_j,\lambda_i^* = \lambda_j^*, i < j\} \tag{3}</script><script type="math/tex; mode=display">d = |DD|,\ \ DD=\{(\boldsymbol x_i,\boldsymbol x_j)\ |\ \lambda_i \ne \lambda_j,\lambda_i^* \ne \lambda_j^*, i < j\} \tag{4}</script></li></ul><p>  其中：</p><p>  集合 $SS$ 包含了在 $C$ 中隶属于相同簇且在 $C^*$ 中也隶属于相同簇的样本对</p><p>  集合 $SD$ 包含了在 $C$ 中隶属于相同簇但在 $C^*$ 中隶属于不相同簇的样本对</p><p>  ……</p><p><strong>常用外部指标：</strong></p><ul><li><p>Jaccard 系数 (Jaccard Coefficient, 简称 JC)</p><script type="math/tex; mode=display">JC = \frac{a}{a+b+c}\tag{5}</script></li><li><p>FM 指数 (Fowlkes and Mallows Index, 简称FMI)</p><script type="math/tex; mode=display">FMI = \sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}\tag{6}</script></li><li><p>Rand 指数 (Rand Index, 简称 RI)</p><script type="math/tex; mode=display">RI = \frac{2(a+d)}{m(m-1)}\tag{7}</script></li></ul><p><strong>度量方法：</strong><font color="#009f">上述性能度量的结果值均在 [0,1] 区间，值越大越好</font></p><h4 id="内部指标-internal-index"><a href="#内部指标-internal-index" class="headerlink" title="内部指标 (internal index)"></a>内部指标 (internal index)</h4><p><strong>变量符号定义：</strong></p><ul><li><p>$avg(C)$: 簇 $C$ 内样本间的平均距离</p><script type="math/tex; mode=display">avg(C) = \frac{2}{|C|(|C|-1)}\sum_{1\le i <j\le|C|}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{8}</script><p>其中，$dist(\cdot,\cdot)$ 用于计算两个样本间的距离</p></li><li><p>$diam(C)$: 簇 $C$ 内样本间的最远距离</p><script type="math/tex; mode=display">diam(C) = \mathop{\max}_{1\le i <j\le|C|}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{9}</script></li><li><p>$d_{min}(C_i,C_j)$: 簇 $C_i$ 与 $C_j$ 最近样本间的距离</p><script type="math/tex; mode=display">d_{min}(C_i,C_j) = \mathop{\min}_{\boldsymbol x_i \in C_i,\boldsymbol x_j \in C_j}dist(\boldsymbol x_i,\boldsymbol x_j)\tag{10}</script></li><li><p>$d_{cen}(C_i,C_j)$: 簇 $C_i$ 与 $C_j$ 中心点间的距离</p><script type="math/tex; mode=display">d_{cen}(C_i,C_j) = dist(\boldsymbol \mu_i,\boldsymbol \mu_j)\tag{11}</script><p>其中，$\boldsymbol \mu$ 代表簇 $C$ 的中心点 </p></li></ul><p><strong>常用内部指标：</strong></p><ul><li><p>DB指数 (简称 DBI)</p><script type="math/tex; mode=display">DBI = \frac{1}{k}\sum_{i=1}^{k}\mathop{\max}_{j\ne i}\left(\frac{avg(C_i)+avg(C_j)}{d_{cen}(C_i,C_j)} \right)\tag{12}</script></li><li><p>Dunn指数 (简称 DI)</p><script type="math/tex; mode=display">DI = \mathop{\min}_{1\le i \le k}\left\{\mathop {\min}_{j\ne i}\left(\frac{dmin(C_i,C_j)}{\mathop{\max}_{1\le l\le k}diam(C_l)} \right) \right\}</script></li></ul><p><strong>度量方法：</strong> <font color="#099f">DBI 的值越小越好，而 DI 的值越大越好</font></p><hr><h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><h4 id="距离度量-distiance-measure"><a href="#距离度量-distiance-measure" class="headerlink" title="距离度量 (distiance measure)"></a>距离度量 (distiance measure)</h4><p><strong>距离度量函数性质：</strong></p><ul><li><p>非负性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) \ge 0 \tag{14}</script></li><li><p>同一性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) = 0 \ \text{当且仅当} \ \boldsymbol x_i=\boldsymbol x_j \tag{15}</script></li><li><p>对称性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) = dist(\boldsymbol x_j,\boldsymbol x_i)\tag{16}</script></li><li><p>直递性：</p><script type="math/tex; mode=display">dist(\boldsymbol x_i,\boldsymbol x_j) \le dist(\boldsymbol x_i,\boldsymbol x_k) + dist(\boldsymbol x_k,\boldsymbol x_j)\tag{17}</script></li></ul><h4 id="闵可夫斯基距离-Minkowski-distance"><a href="#闵可夫斯基距离-Minkowski-distance" class="headerlink" title="闵可夫斯基距离 (Minkowski distance)"></a>闵可夫斯基距离 (Minkowski distance)</h4><p><strong>闵可夫斯基距离：</strong></p><ul><li><font color="#0099ff">用于有序属性</font></li><li><p>样本$\boldsymbol x_i$:  $\boldsymbol x_i = (x_{i1}, x_{i2},…,x_{in})$</p></li><li><p>样本$\boldsymbol x_j$:  $\boldsymbol x_j = (x_{j1}, x_{j2},…,x_{jn})$</p></li><li><p>闵可夫斯基距离:</p><script type="math/tex; mode=display">\text{dist}_\text{mk}(\boldsymbol x_i,\boldsymbol x_j) = \left(\sum_{u=1}^{n}{|x_{iu}-x_{ju} |^p} \right)^{\frac{1}{p}}\tag{18}</script><p>其中， $p\ge1$.</p></li></ul><p><strong>欧式距离 (Euclidean distance)：</strong></p><ul><li>当 $p =2$ 时，闵可夫斯基距离即为欧式距离：<script type="math/tex; mode=display">\text{dist}_\text{ed}(\boldsymbol x_i,\boldsymbol x_j) = \left\|\boldsymbol x_i-\boldsymbol x_j \right\|_2 = \sqrt{\sum_{u=1}^{n}{|x_{iu}-x_{ju} |^2}}\tag{19}</script></li></ul><p><strong>曼哈顿距离 (Manhattan distance)：</strong></p><ul><li>当 $p =1$ 时，闵可夫斯基距离即为曼哈顿距离：<script type="math/tex; mode=display">\text{dist}_\text{man}(\boldsymbol x_i,\boldsymbol x_j) = \left\|\boldsymbol x_i-\boldsymbol x_j \right\|_1 = \sum_{u=1}^{n}{|x_{iu}-x_{ju} |}\tag{20}</script></li></ul><h4 id="VDM-Value-Difference-Metric-距离"><a href="#VDM-Value-Difference-Metric-距离" class="headerlink" title="VDM (Value Difference Metric) 距离"></a>VDM (Value Difference Metric) 距离</h4><p><strong>VDM</strong></p><ul><li><font color="#0099ff">用于无序属性</font></li><li><p>$m_{u,a}$: 在属性 $u$ 上取值为 $a$ 的样本数</p></li><li><p>$m_{u,a,i}$: 在第 $i$ 个样本簇中在属性 $u$ 上取值为 $a$ 的样本数</p></li><li><p>$k$: 样本簇数</p></li><li><p>$\text{VDM}_p(a,b)$: 属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 $\text{VDM}$ 距离:</p><script type="math/tex; mode=display">\text{VDM}_p(a,b) = \sum_{i=1}^{k}{\left|\frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}}\right|} \tag{21}</script></li></ul><p><strong>闵可夫斯基距离和 VDM 结合处理混合属性：</strong></p><ul><li>假定有 $n_c$ 个有序属性</li><li>假定有 $n-n_c$ 个无序属性</li></ul><script type="math/tex; mode=display">\text{MinkovDM}_p(\boldsymbol x_i,\boldsymbol x_j) = \left(\sum_{u=1}^{n_c}{|x_{iu}-x_{ju} |^p} + \sum_{u=n_c+1}^{n} \text{VDM}_p(x_{iu} ,x_{ju}) \right)^{\frac{1}{p}}\tag{22}</script><hr><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><h4 id="K-均值聚类"><a href="#K-均值聚类" class="headerlink" title="K 均值聚类"></a>K 均值聚类</h4><font color="#099ff">K 均值算法也称为 K-Means 算法</font><p><strong>K-Means 算法思想:</strong> </p><ul><li><p>给定样本集 $D = \{\boldsymbol x_1, \boldsymbol x_2, …, \boldsymbol x_m \}$</p></li><li><p>针对聚类所得的簇划分 ${\cal C} = \{C_1,C_2,…,C_k\}$ 最小化平方误差：</p><script type="math/tex; mode=display">E = \sum_{i=1}^{k}\sum_{\boldsymbol x \in C_i}\left\| \boldsymbol x - \boldsymbol\mu_i \right\|_2^2\tag{24}</script><p>其中 $\boldsymbol \mu_i = \frac{1}{|C_i|}{\sum_{\boldsymbol x \in C_i}\boldsymbol x}$ 是簇 $C_i$ 的均值向量</p></li></ul><p><strong>K-Means 算法流程：</strong></p><ul><li>初始化均值向量 (第1行)</li><li>对当前簇划分进行迭代更新 (第4~8行)</li><li>对当前簇的均值向量进行迭代更新 (第9~16行)</li><li>若迭代更新后聚类结果保持不变，则返回当前簇划分结果 (第17~18行)</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure1.PNG" alt="figure1"></p><h4 id="学习向量量化"><a href="#学习向量量化" class="headerlink" title="学习向量量化"></a>学习向量量化</h4><font color="#099ff">学习向量量化  (Learning Vector Quantization, 简称 LVQ) 假设数据样本带有类别标记</font><p><strong>LVQ 算法思想：</strong></p><ul><li><p>给定样本集 $D = \{(\boldsymbol x_1, y_1), (\boldsymbol x_2,y_2), …, (\boldsymbol x_m,y_m) \}$</p></li><li><p>特征向量 $\boldsymbol x_j = \{x_{j1};x_{j2};…;x_{jn} \}$</p></li><li><p>类别标记 ${\cal Y} = \{y_1,y_2,…,y_m\}$</p></li><li><p>LVQ 试图学得一组 $n$ 维原型向量 $\{\boldsymbol p_1, \boldsymbol p_2, …,\boldsymbol p_q \}$ , 每个原型向量代表一个聚类簇，簇标记 $t_i \in \cal Y$</p></li></ul><p><strong>LVQ 算法流程:</strong></p><ul><li><p>初始化原型聚类 (第1行)</p></li><li><p>对原型向量进行迭代优化 (第2~12行)</p><ul><li><p>随机选取一个有标记的训练样本 $\boldsymbol x_j$ </p></li><li><p>找出与其距离最近的原型向量 $\boldsymbol p_{i^*}$ </p></li><li><p>若原型向量 $\boldsymbol p_{i^*}$ 与 $\boldsymbol x_j$ 的类别标记相同，则令 $\boldsymbol p_{i^*}$ 向 $\boldsymbol x_j$ 方向靠拢</p></li><li><p>此时，新原型向量为：</p><script type="math/tex; mode=display">\boldsymbol p\prime = \boldsymbol p_{i^*} +\eta\cdot(\boldsymbol x_j-\boldsymbol p_{i^*})\tag{25}</script><p>其中，$\eta\in(0,1)$ 为学习率</p></li><li><p>若原型向量 $\boldsymbol p_{i^*}$ 与 $\boldsymbol x_j$ 的类别标记不同，则令 $\boldsymbol p_{i^*}$ 从 $\boldsymbol x_j$ 方向远离</p></li><li><p>此时，新原型向量为：</p><script type="math/tex; mode=display">\boldsymbol p\prime = \boldsymbol p_{i^*} +\eta\cdot(\boldsymbol x_j-\boldsymbol p_{i^*})\tag{26}</script></li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure2.PNG" alt="figure2"></p><h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><hr><h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><h4 id="密度聚类-1"><a href="#密度聚类-1" class="headerlink" title="密度聚类"></a>密度聚类</h4><p><strong>基于密度的聚类 (density-based clustering)</strong></p><ul><li>简称密度聚类</li><li>此类算法假设聚类结构能通过样本分布的紧密程度确定</li><li>常见的算法：DBSCAN 算法</li></ul><h4 id="DBSCAN-算法"><a href="#DBSCAN-算法" class="headerlink" title="DBSCAN 算法"></a>DBSCAN 算法</h4><font color="#0099">DBSCAN 基于一组 "领域" 参数 </font> $(\epsilon, MinPts)$ <font color="#0099">来刻画样本分布的紧密程度</font><p><strong>DBSCAN 相关概念：</strong></p><ul><li><p><strong>$\epsilon$-邻域 </strong> </p><ul><li><p>对 $\boldsymbol x_j\in D$，其 $\epsilon$ -邻域包含样本集 $D$ 中与 $\boldsymbol x_j$ 距离不大于 $\epsilon$ 的样本，即 </p><p>$N_{\epsilon}(\boldsymbol x_j) = \{\boldsymbol x_j  | dist(\boldsymbol x_i,\boldsymbol x_j) \le \epsilon \}$</p></li></ul></li><li><p><strong>核心对象</strong> (core object)</p><ul><li>若 $\boldsymbol x_j$ 的 $\epsilon$-邻域内至少含有 $MinPts$ 个样本，即 $|N_{\epsilon}(\boldsymbol x_j)|\ge MinPts$, 则 $\boldsymbol x_j$ 是一个核心对象</li></ul></li><li><p><strong>密度直达</strong> (directly density-reachable)</p><ul><li>若 $\boldsymbol x_j$ 位于 $\boldsymbol x_i$ 的 $\epsilon$-邻域内，且 $\boldsymbol x_i$ 是核心对象，则称 $\boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度直达</li></ul></li><li><p><strong>密度可达</strong> (density-reachable)</p><ul><li>对 $\boldsymbol x_i$ 和 $\boldsymbol x_j$ , 若存在样本序列 $\boldsymbol p_1$, $\boldsymbol p_2$, …… ,$\boldsymbol p_n$, 其中 $\boldsymbol p_1 = \boldsymbol x_i$, $\boldsymbol p_n =\boldsymbol x_j$, 且 $\boldsymbol p_{i+1}$ 由 $\boldsymbol p_i$ 密度直达，则称 $\boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度可达</li></ul></li><li><p><strong>密度相连</strong> (density-connected)</p><ul><li>对 $\boldsymbol x_i$ 与 $\boldsymbol x_j$，若存在 $\boldsymbol x_k$ 使得 $\boldsymbol x_i$ 与 $\boldsymbol x_j$ 均由 $\boldsymbol x_k$ 密度可达，则 $\boldsymbol x_i$ 与 $\boldsymbol x_j$ 密度相连</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure4.PNG" alt="figure4"></p><p><strong>DBSCAN 算法思想：</strong></p><ul><li>DBSCAN 中 “簇” 的定义：由密度可达关系导出的最大密度相连的样本集合</li><li>给定领域参数 $(\epsilon, MinPts)$, 簇 $C \subseteq D$ 是满足以下性质的非空样本子集：<ul><li>连接性：$\boldsymbol x_i \in C, \boldsymbol x_j \in C\Rightarrow \boldsymbol x_i $ 与$ \boldsymbol x_j  $密度相连</li><li>最大性：$\boldsymbol x_i \in C, \boldsymbol x_j$ 由 $\boldsymbol x_i$ 密度可达 $\Rightarrow \boldsymbol x_j \in C$</li></ul></li><li>如何生成簇：若 $\boldsymbol x$ 为核心对象，由 $\boldsymbol x$ 密度可达的所有样本组成的集合记为 $ X = \{\boldsymbol x\prime \in D  | \boldsymbol x\prime  {\text 由} \boldsymbol x  {\text 密度可达} \}$ ，则 $X$ 记为满足最大性和连接性的簇</li></ul><p><strong>DBSCAN 算法流程：</strong> </p><ul><li>任选数据集中的一个核心对象作为 “种子” (第1行)</li><li>根据给定的领域参数 $(\epsilon, MinPts)$ 找出所有核心对象 (第1~7行)</li><li>以任一核心对象为出发点，找出由其密度可达的样本生成的聚类簇，直至所有的对象都被访问过为止 (第10~24行)</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（九）-- 聚类/figure3.PNG" alt="figure3"></p><hr><h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><h4 id="层次聚类-1"><a href="#层次聚类-1" class="headerlink" title="层次聚类"></a>层次聚类</h4><p><strong>层次聚类 (hierarchical clustering)</strong></p><ul><li>试图在不同层次对数据集进行划分，从而形成属性结构</li><li>数据集划分的策略：<ul><li>“自底向上”的聚合策略</li><li>“自顶向下”的分拆策略</li></ul></li></ul><h4 id="AGNES-算法"><a href="#AGNES-算法" class="headerlink" title="AGNES 算法"></a>AGNES 算法</h4><p><strong>AGNES 算法简介：</strong></p><ul><li>一种采用自底向上聚合策略的层次聚类算法</li></ul><p><strong>AGNES 算法思想：</strong></p><ul><li><p>先将数据集中的每个样本看作一个初始聚类簇</p></li><li><p>然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，其中两个簇间的距离计算方法如下：</p><ul><li><p><strong>最小距离：</strong></p><script type="math/tex; mode=display">d_{min}(C_i,C_j) = \mathop{\min}_{\boldsymbol x\in C_i,\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z)\tag{41}</script></li><li><p><strong>最大距离：</strong></p><script type="math/tex; mode=display">d_{max}(C_i,C_j) = \mathop{\max}_{\boldsymbol x\in C_i,\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z)\tag{42}</script></li><li><p><strong>平均距离：</strong> </p><script type="math/tex; mode=display">d_{avg}(C_i,C_j) = \frac{1}{|C_i||C_j|} \sum_{\boldsymbol x\in C_i}\sum_{\boldsymbol z\in C_j}\text{dist}(\boldsymbol x, \boldsymbol z) \tag{43}</script></li></ul></li><li><p>将上述过程不断重复，直到达到预设的聚类簇个数</p></li></ul><p><strong>AGNES 算法流程：</strong></p><ul><li>对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化 (第1-9行)</li><li>不断合并距离最近的聚类簇，并对合并得到的聚类簇的聚类矩阵进行更新</li><li>上述过程不断重复，直到达到预设的聚类簇数</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（九）-聚类&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（九）-聚类&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（九） 聚类&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（九） 聚类&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（八）-- 集成学习</title>
    <link href="http://sunfeng.online/2019/08/19/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89--%20%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://sunfeng.online/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/</id>
    <published>2019-08-19T08:34:44.000Z</published>
    <updated>2019-08-26T02:22:28.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（八）集成学习"><a href="#《机器学习》西瓜书学习笔记（八）集成学习" class="headerlink" title="《机器学习》西瓜书学习笔记（八）集成学习"></a>《机器学习》西瓜书学习笔记（八）集成学习</h2><a id="more"></a><h3 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h3><h4 id="集成学习-ensemble-learning"><a href="#集成学习-ensemble-learning" class="headerlink" title="集成学习 (ensemble learning)"></a>集成学习 (ensemble learning)</h4><p><strong>集成学习的一般思路：</strong></p><ul><li>集成学习通过构建并结合多个学习器来完成学习任务</li></ul><p><strong>集成学习的一般结构：</strong></p><ul><li>先产生一组 “个体学习器”</li><li>在用某种策略将它们结合起来</li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure1.PNG" alt="figure1"></p><p><strong>集成学习的集成分类：</strong></p><ul><li><strong>同质</strong> (homogeneous)<ul><li>集成中只包含同种类型的个体学习器</li></ul></li><li><strong>异质</strong> (heterogenous)<ul><li>集成中包含不同类型的个体学习器</li></ul></li></ul><p><strong>集成学习的基本原则：</strong>若要获得好的集成</p><ul><li>个体学习器应该 “好而不同”</li><li>即个体学习器要有一定的 “准确性”</li><li>即学习器不能太坏，并且要有 “多样性”，即学习器间有差异</li></ul><p><strong>集成学习的方法分类：</strong></p><ul><li><strong>序列化方法</strong><ul><li>个体学习器间存在强依赖关系、必须串行生成</li><li>代表：Boosting</li></ul></li><li><strong>并行化方法</strong><ul><li>个体学习器间不存在强依赖关系、可同时生成</li><li>代表：Bagging、随机森林</li></ul></li></ul><hr><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><h4 id="Boosting简介"><a href="#Boosting简介" class="headerlink" title="Boosting简介"></a>Boosting简介</h4><font color="#0099">Boosting 是一族可将弱学习器提升为强学习其的算法</font><p><strong>工作机制</strong>：</p><ul><li>先从初始训练集训练出一个基学习器</li><li>再根据基学习器的表现对训练样本分布进行调整</li><li>使得先前基学习器做错的训练样本在后续受到更多关注</li><li>然后再基于调整后的样本分布来训练下一个基学习器</li><li>如此重复，直到基学习器的数目达到指定值 $T$</li><li>再将 $T$ 个基学习器进行加权结合</li></ul><h4 id="AdaBoost-算法"><a href="#AdaBoost-算法" class="headerlink" title="AdaBoost 算法"></a>AdaBoost 算法</h4><font color="#0099">AdaBoost算法是Boosting 族算法的代表</font><p><strong>AdaBoost算法描述：</strong></p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure2.PNG" alt="figure2"></p><ul><li>其中：$y_i \in \{-1,+1 \}$, $f$ 是真实函数</li></ul><p><strong>AdaBoost 算法推导：</strong></p><hr><h3 id="Bagging-与随机森林"><a href="#Bagging-与随机森林" class="headerlink" title="Bagging 与随机森林"></a>Bagging 与随机森林</h3><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p><strong>Bagging算法简介：</strong></p><ul><li>Bagging 是并行式集成学习方法最著名的代表</li></ul><p><strong>Bagging算法流程：</strong></p><ul><li>基于自助采样法采样出 $T$ 个含有 $m$ 个样本的训练集</li><li>然后基于每个采样集训练出一个基学习器</li><li>再将这些基学习器结合</li><li><strong>在对预测输出进行结合时：</strong><ul><li>分类任务：简单投票法</li><li>回归任务：简单平均法</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure3.PNG" alt="figure3"></p><h4 id="随机森林-Random-Forrest-简称-RF"><a href="#随机森林-Random-Forrest-简称-RF" class="headerlink" title="随机森林 (Random Forrest, 简称 RF)"></a>随机森林 (Random Forrest, 简称 RF)</h4><p><strong>随机森林算法简介：</strong></p><ul><li>RF 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中加入随机属性的选择</li></ul><p><strong>随机森林算法流程：</strong></p><ul><li>在选择划分属性时，对决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集<ul><li>$k$ 控制了随机性的引入程度，其推荐值为 $k = \log_2d$ </li></ul></li><li>然后再从这个子集中选择一个最优属性用于划分</li></ul><hr><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><p><strong>学习器结合的好处：</strong></p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure4.PNG" alt="figure4"></p><ul><li>假定集成包含 $T$ 个基学习器 $\{h_1,h_2,…,h_T \}$, 其中 $h_i$ 在示例 $\boldsymbol x$ 上的输出为 $h_i(\boldsymbol x)$</li></ul><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><p><strong>平均法是回归任务，即数值型输出 $h_i(\boldsymbol x) \in {\Bbb R}$, 常使用的结合策略</strong></p><ul><li><p><strong>简单平均法</strong> (simple average)</p><script type="math/tex; mode=display">H(\boldsymbol x) = \frac{1}{T}\sum_{i=1}^{T}h_i(\boldsymbol x)</script></li><li><p><strong>加权平均法</strong> (weighted average)</p><script type="math/tex; mode=display">H(\boldsymbol x) = \sum_{i=1}^{T}w_ih_i(\boldsymbol x)</script><p>其中 $w_i$ 为个体学习器 $h_i$ 的权重，<strong>权重一般需要从训练数据中学习而得</strong></p></li></ul><font color="#0099">在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法</font><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><p><strong>投票法是分类任务，即学习器 $h_i$ 将从类别标记集合 $\{c_1,c_2,…,c_N \}$ 中预测出一个标记，常用的结合策略</strong></p><p>假定 $h_i$ 在样本 $\boldsymbol x$ 上的预测输出为一个 $N$ 维向量 $(h_i^1(\boldsymbol x);h_i^2(\boldsymbol x);…;h_i^N(\boldsymbol x))$, 其中 $h_i^j(\boldsymbol x)$ 是 $h_i$ 在类别标记 $c_j$ 上的输出</p><ul><li><p><strong>绝对多数投票法</strong></p><script type="math/tex; mode=display">\begin {equation}H(\boldsymbol x) = \begin {cases}c_j \qquad &{\text if}\ \sum_{i=1}^{T}h_i^j(x)>0.5\sum_{k=1}^{T}\sum_{i=1}^{T}h_i^j(x)\\{\text reject} &{\text otherwise}\end {cases}\end {equation}</script><p>即，若某类标记得票过半数，则预测为该标记，否则拒绝预测</p></li><li><p><strong>相对多数投票法</strong></p><script type="math/tex; mode=display">H(\boldsymbol x) = c_{\mathop{\arg \max}_{j}\sum_{i=1}^{T}h_i^j(\boldsymbol x)}</script><p>即，预测为得票最多的标记</p></li><li><p><strong>加权投票法</strong></p><script type="math/tex; mode=display">H(\boldsymbol x) = c_{\mathop{\arg \max}_{j}\sum_{i=1}^{T}w_ih_i^j(\boldsymbol x)}</script><p>$w_i$ 是 $h_i$ 的权重</p></li></ul><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p><strong>学习法简介：</strong></p><ul><li>当训练数据很多时，一种更为强大的结合策略是使用 “学习法”</li><li>即通过另一个学习器来进行结合<ul><li>其中个体学习器称为初级学习器</li><li>用于结合的学习器称为次级学习器</li></ul></li><li>典型代表：Stacking</li></ul><p><strong>Stack算法流程：</strong></p><ul><li>先从初始训练集中训练出初级学习器</li><li>然后 “生成” 一个新数据集用于训练次级学习器<ul><li>在新数据集中，初级学习器的输出被当做样例输入特征</li><li>初始样本的标记仍被当做样例标记</li></ul></li></ul><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure5.PNG" alt="figure5"></p><hr><h3 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h3><h4 id="误差-分歧分解"><a href="#误差-分歧分解" class="headerlink" title="误差-分歧分解"></a>误差-分歧分解</h4><p><strong>问题描述：</strong></p><ul><li>假定用个体学习器 $h_1,h_2,…,h_T$ 通过加权平均法结合产生的集成来完成回归任务 $f :  {\Bbb R}^d\mapsto {\Bbb R} $</li></ul><p><strong>误差-分歧分解：</strong></p><ul><li><p>对于示例 $\boldsymbol x$ ，定义学习器 $h_i$ 的 <strong>分歧</strong> 为：</p><script type="math/tex; mode=display">A(h_i|\boldsymbol x) = \left(h_i(\boldsymbol x)-H(\boldsymbol x) \right)^2\tag{8.27}</script></li><li><p>则集成的 <strong>分歧</strong> 是：</p><script type="math/tex; mode=display">\begin {align}\overline A(h|\boldsymbol x) &= \sum_{i=1}^Tw_iA(h_i|\boldsymbol x) \\&= \sum_{i=1}^Tw_i\left(h_i(\boldsymbol x)-H(\boldsymbol x) \right)^2\tag{8.28}\end {align}</script></li><li><p><strong>分歧</strong> 项表征了个体学习器在样本 $\boldsymbol x$ 上的不一致性</p></li><li><p>即在一定程度上反映了个体学习器的多样性</p></li><li><p>个体学习器 $h_i$ 的平方误差为：</p><script type="math/tex; mode=display">E(h_i|\boldsymbol x) = (f(\boldsymbol x)-h_i(\boldsymbol x))^2\tag{8.29}</script></li><li><p>集成 $H$ 的平方误差为：</p><script type="math/tex; mode=display">E(H|\boldsymbol x) = (f(\boldsymbol x)-H(\boldsymbol x))^2\tag{8.30}</script></li><li><p>令个体学习器误差的加权值为：$\overline E(h|\boldsymbol) = \sum_{i=1}^Tw_i\cdot E(h_i|\boldsymbol x) $</p></li><li><p>则，集成的分歧</p><script type="math/tex; mode=display">\begin {align}\overline A(h|\boldsymbol x) &= \sum_{i=1}^{T}w_iE(h_i|\boldsymbol x)-E(H|\boldsymbol x)\\&= \overline E(h|\boldsymbol x)-E(H|\boldsymbol x)\tag{8.31}\end {align}</script></li><li><p>令 $p(\boldsymbol x)$ 表示样本的概率密度，则在全样本上有：</p></li></ul><script type="math/tex; mode=display">\sum_{i=1}^{T}w_i\int A(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x = \sum_{i=1}^{T}w_i\int E(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x -\int E(H|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.32}</script><ul><li><p>令个体学习器 $h_i$ 在全样本上的泛化误差和分歧项分别为:</p><script type="math/tex; mode=display">E_i = \int E(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x\tag{8.33}</script><script type="math/tex; mode=display">A_i = \int A(h_i|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.34}</script></li><li><p>集成的泛化误差为：</p><script type="math/tex; mode=display">E = \int E(H|\boldsymbol x)p(\boldsymbol x)d\boldsymbol x \tag{8.35}</script></li><li><p>令 $\overline E = \sum_{i=1}^Tw_iE_i$ 表示个体学习器泛化误差的加权均值</p></li><li><p>令 $\overline A = \sum_{i=1}^Tw_iA_i$ 表示个体学习器的加权分歧值</p></li><li><p>将 (8.33)~(8.34)带入(8.32)得：</p><script type="math/tex; mode=display">E = \overline E-\overline A \tag{8.36}</script><font color="#0099">个体学习器的准确性越高、多样性越大，则集成越好</font></li></ul><h4 id="多样性度量"><a href="#多样性度量" class="headerlink" title="多样性度量"></a>多样性度量</h4><p><strong>多样性度量</strong> (diversity measure)</p><ul><li>用于度量集成中个体分类器的多样性</li><li>即估算个体学习器的多样化程度</li></ul><p><strong>列联表 </strong> </p><ul><li><p>给定数据集 $D = \{ (\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),…,(\boldsymbol x_m,y_m)\}$</p></li><li><p>对二分类任务，$y_i \in \{-1, +1\}$</p></li><li><p>分类器 $h_i$ 和 $h_j$ 的预测结果列联表为:</p><p><img src="/2019/08/19/《机器学习》西瓜书学习笔记（八）-- 集成学习/figure6.PNG" alt="figure6"></p><p>其中，$a$ 表示 $h_i$ 和 $h_j$ 均预测为正类的样本数目</p></li></ul><p>……</p><p><strong>常见多样性性能度量</strong></p><ul><li><p><strong>不合度量</strong> (disagreement measure)</p><script type="math/tex; mode=display">dis_{ij} =\frac{b+c}{m}\tag{8.37}</script><p>$dis_{ij}$ 的值域为 [0, 1], 值越大则多样性越大</p></li><li><p><strong>相关系数</strong> (correlation coefficient)</p><script type="math/tex; mode=display">\rho_{ij} = \frac{ad-bc}{\sqrt{a+b}\sqrt{a+c}\sqrt{c+d}\sqrt{b+d}}\tag{8.38}</script><p>$\rho_{ij}$ 的值域为 [-1,1], 若 $h_i$ 和 $h_j$ 无关，则值为0；若 $h_i$ 和 $h_j$ 正相关则值为正，否则为负</p></li><li><p><strong>Q-统计量</strong> (Q-statistic)</p><script type="math/tex; mode=display">Q_{ij} = \frac{ad-bc}{ad+bc}\tag{8.39}</script><p>$Q_{ij}$ 的值域为 [-1,1], 若 $h_i$ 和 $h_j$ 无关，则值为0；若 $h_i$ 和 $h_j$ 正相关则值为正，否则为负</p></li><li><p>$\kappa$<strong>-统计量 </strong>($\kappa$-statistics)</p><script type="math/tex; mode=display">\kappa = \frac{p_1-p_2}{1-p_2}\tag{8.40}</script><p>其中，$p_1$ 是两个分类器取得一致的概率：</p><script type="math/tex; mode=display">p_1 = \frac{a+d}{m}\tag{8.41}</script><p>$p_2$ 是 两个分类器偶然达成一致的概率：</p><script type="math/tex; mode=display">p_2 = \frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}\tag{8.42}</script><p>若分类器 $h_i$ 与 $h_j$ 在 $D$ 上完全一致，则 $\kappa = 1$; 若它们仅是偶然达成一致，则 $\kappa = 0$</p></li></ul><h4 id="多样性增强"><a href="#多样性增强" class="headerlink" title="多样性增强"></a>多样性增强</h4><ul><li><p><strong>数据样本扰动</strong></p></li><li><p><strong>输入属性扰动</strong></p></li><li><p><strong>输出表示扰动</strong></p></li><li><p><strong>算法参数扰动</strong></p></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（八）集成学习&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（八）集成学习&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（八）集成学习&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（八）集成学习&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器</title>
    <link href="http://sunfeng.online/2019/08/17/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89--%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>http://sunfeng.online/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/</id>
    <published>2019-08-17T03:52:20.000Z</published>
    <updated>2019-08-19T08:34:44.560Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（七）贝叶斯分类器"><a href="#《机器学习》西瓜书学习笔记（七）贝叶斯分类器" class="headerlink" title="《机器学习》西瓜书学习笔记（七）贝叶斯分类器"></a>《机器学习》西瓜书学习笔记（七）贝叶斯分类器</h2><a id="more"></a><h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p><strong>条件风险 (conditional risk)</strong></p><ul><li><p>假设有 $N$ 种可能的类别标记，即 $ {\cal Y} = \{c_1,c_2,…,c_N \}$ </p></li><li><p>将样本 $\boldsymbol x$ 分类为 $c_i$ 所产生的期望损失 ，即在样本 $\boldsymbol x$ 上的 “条件风险” 为：</p><script type="math/tex; mode=display">R(c_i|\boldsymbol x) = \sum_{j=1}^{N}\lambda_{ij}P(c_j|\boldsymbol x) \tag{1}</script><p>其中：</p><p> $\lambda_{ij}$ 是将一个真实标记为 $c_j$ 的样本分类为 $c_i$ 所产生的损失；</p><p>$P(c_i|\boldsymbol x)$ 表示样本 $\boldsymbol x $ 属于类别 $c_j$ 的后验概率 </p></li></ul><p><strong>贝叶斯决策论 (Bayesian decision theory)</strong></p><ul><li><p>基于相关概率和误判损失来选择最优类别标记</p></li><li><p>即寻找一个判定准则 $h: {\cal X} \mapsto {\cal Y}$ 以最小化总体风险</p><script type="math/tex; mode=display">R(h) = E_x[R(h(\boldsymbol x)\ | \ \boldsymbol  x)] \tag{2}</script></li></ul><p><strong>贝叶斯判定准则  (Bayes decision rule)</strong></p><ul><li><p>为最小化总体风险，只需在每个样本上选择那个能使条件风险 $R(c|\boldsymbol x)$ 最小的类别标记，即：</p><script type="math/tex; mode=display">h^*(\boldsymbol x) = \mathop{\arg\min}_{c\in {\cal Y}}\ {R(c\ |\ \boldsymbol x)} \tag{3}</script></li><li><p>此时：</p><ul><li>$h^*$ 称为贝叶斯最优分类器 (Bayes optimal classifier)</li><li>总体风险 $R(h^*)$ 称为贝叶斯风险  (Bayes risk)</li><li>$1-R(h^*)$ 反映了分类器所能达到的最好性能</li></ul></li></ul><p><strong>最小化的错误率的贝叶斯最优分类器：</strong></p><ul><li><p>若目标是最小化分类错误率，则误判损失 $\lambda_{ij}$ 可写为：</p><script type="math/tex; mode=display">\lambda_{ij} = \begin{cases}0, & \text{if} \ \ i = j\\1, & \text{otherwise}\end{cases}\tag{4}</script></li><li><p>此时条件风险为</p><script type="math/tex; mode=display">R(c\ |\ \boldsymbol x) = 1-P(c\ | \ \boldsymbol x) \tag{5}</script><p>解析：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}&R(c_i|\boldsymbol x) = 1 * P(c_1|\boldsymbol x) + 1 * P(c_2|\boldsymbol x) + ... + 0 *  P(c_i|\boldsymbol x)+ ... + 1 * P(c_N|\boldsymbol x) \\&\because \ \sum_{j=1}^{N}P(c_j|\boldsymbol x) = 1 \\&\therefore \  R(c_i|\boldsymbol x) = 1-P(c_i|\boldsymbol x)\end {split}\end {equation}</script></li><li><p>于是，最小化分类错误率的贝叶斯最优分类器为：</p><script type="math/tex; mode=display">h^*{(\boldsymbol x)} = \mathop{\arg\max}_{c\in\cal Y}\ P(c \ |\ \boldsymbol x) \tag {6}</script><p>即，对每个样本 $x$, 选择能使后验概率 $P(c  | \boldsymbol x)$ 最大的类别标记</p></li><li><p><font color="#0099ff">欲使用贝叶斯判定准则最小化决策风险，首先要获得后验概率 </font> $P(c  | \boldsymbol x)$ </p></li></ul><p><strong>估计后验概率 $P(c  | \boldsymbol x)$ 的两种策略：</strong></p><ul><li><p><strong>判别式模型</strong> (discriminative models)</p><ul><li>给定 $x$, 可通过直接建模  $P(c  | \boldsymbol x)$ 来预测 $c$</li><li>如：决策树、BP神经网络、支持向量机</li></ul></li><li><p><strong>生成式模型</strong> (generative models)</p><ul><li><p>先对联合概率分布 $P(\boldsymbol x,c)$ 建模，然后由此获得 $P(c  | \boldsymbol x)$ ，即：</p><script type="math/tex; mode=display">P(c \ |\ \boldsymbol x) = \frac{P(\boldsymbol x, c)}{P(\boldsymbol x)}\tag{7}</script></li><li><p>基于贝叶斯定理，得：</p><script type="math/tex; mode=display">P(c \ |\ \boldsymbol x) = \frac{P(c)\ P(\boldsymbol x \ |\ c)}{P(\boldsymbol x)}\tag{8}</script></li><li><p>$P(c)$: 是类先验概率</p><ul><li>表达了样本空间中各类样本所占的比例</li><li>根据大数定律，当训练集中包含充足的独立同分布样本时，$P(c)$ 可通过各类样本出现的频率来进行估计</li></ul></li><li><p>$P(\boldsymbol x  | c)$: 是样本 $\boldsymbol x$ 相对于类标记 $c$ 的类条件概率，或称为“似然”</p><ul><li>类条件概率涉及关于 $\boldsymbol x$ 所有属性的联合概率</li><li>不能直接根据样本出现的频率来进行估计</li></ul></li><li><p>$P(\boldsymbol x)$: 是用于归一化的证据因子</p><ul><li>对给定的样本 $\boldsymbol x$, 证据因子与类别标记无关</li></ul></li></ul></li></ul><hr><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p><strong>极大似然估计 (Maximum Likehood Estimation)</strong></p><ul><li><p>估计类条件概率的一种常用策略</p></li><li><p>令 $D_c$ 表示训练集 $D$ 中第 $c$ 类样本组成的集合</p></li><li><p>假设这些样本时独立同分布的，则参数 $\theta_c$ 对于数据集 $D_c$ 的似然是：</p><script type="math/tex; mode=display">P(D_c \ | \ \theta_c) = \prod_{\boldsymbol x\in D_c} \ P(\boldsymbol x \ |\ \theta_c)\tag{9}</script></li><li><p>对 $\theta_c$ 进行极大似然估计，就是寻找能最大化似然 $P(D_c  |  \theta_c)$ 的参数值 $\hat \theta_c$.</p></li><li><p>为了避免下溢，通常使用对数似然：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}LL(\theta_c) &= \log\ P(D_c\ | \ \theta_c)\\&= \sum_{x\in D_C}\ \log\ P(\boldsymbol x\ | \ \theta_c)\end{split}\end{equation}\tag{10}</script></li><li><p>此时，参数 $\theta_c$ 的极大似然估计 $\hat \theta_c$ 为：</p><script type="math/tex; mode=display">\hat \theta = \mathop{\arg \max}_{\theta_c}\ LL(\theta_c) \tag{11}</script></li></ul><p><strong>极大似然估计的例子：</strong></p><ul><li><p>例如，在连续属性情形下，假设概率密度函数 $p(\boldsymbol x |  c) $ ~ $\cal N(\boldsymbol\mu_c,\boldsymbol\sigma_c)$. 则参数 $\boldsymbol\mu_c$ 和 $\boldsymbol\sigma_c$ 的极大似然估计为:</p><script type="math/tex; mode=display">\boldsymbol{\hat \mu_c} = \frac{1}{|D_c|}\ \sum_{\boldsymbol x\in D_c}\ \boldsymbol x \tag{12}</script><script type="math/tex; mode=display">\boldsymbol{\hat \sigma_c} = \frac{1}{|D_c|} \ \sum_{\boldsymbol x \in D_c}(\boldsymbol x - \boldsymbol {\hat \mu})(\boldsymbol x - \boldsymbol {\hat \mu})^T\tag{13}</script></li><li><p>解析过程如下：</p></li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure1.jpg" alt="figure1"></p><hr><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p><strong>朴素贝叶斯分类器 (naive Bayes classifier)：</strong></p><ul><li><p>采用了<strong>属性条件独立性假设</strong> (attribute conditional independence assumption)：对已知类别，假设所有属性相互独立</p></li><li><p>基于属性独立性条件假设，式 (8) 可重写为：</p><script type="math/tex; mode=display">P(c\ |\ \boldsymbol x) = \frac{P(c)P(\boldsymbol x\ |\ c)}{P(\boldsymbol x)} = \frac{P(c)}{P(\boldsymbol x)}\prod_{i=1}^{d}P(x_i\ |\ c)\tag{14}</script><p>其中 $d$  为属性数目，$x_i$ 为 $\boldsymbol x$ 在第 $i$ 个属性上的取值</p></li><li><p><strong>朴素贝叶斯分类器</strong>的表达式：</p><script type="math/tex; mode=display">h_{nb}(x) = \mathop{\arg\max}_{c\in \cal Y} \ P(c)\prod_{i=1}^{d}\ P(x_i\ |\ c)\tag{15}</script></li><li><p><font color="#0099ff">朴素贝叶斯分类器的训练过程就是基于训练集 D 来估计类先验概率 P(c), 并为每个属性估计条件概率 </font> $P(x_i | c)$ .</p><ul><li><p>$P(c)$ : 类先验概率</p><script type="math/tex; mode=display">P(c) = \frac{|D_c|}{|D|}\tag{16}</script><p>其中, $D_c$ 表示训练集 $D$ 中，第 $c$ 类样本组成的集合</p></li><li><p>$P(x_i  |  c)$ : 表示第 $c$ 类样本在 i 个属性上取值为 $x_i$ 的条件概率</p><script type="math/tex; mode=display">P(x_i\ | \ c) = \frac{|D_{c,x_i}|}{|D_c|}\tag{17}</script><p>其中，$D_{c,x_i}$ 表示 $D_c$ 中在第 $i$ 个属性上取值为 $x_i$ 的样本组成的集合</p></li><li><p>对于连续属性，假设服从正态分布，则：</p><script type="math/tex; mode=display">P(x_i\ | \ c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp\left(-\frac{(x_i - \mu_{c,i})^2}{2\sigma^2_{c,i}} \right)\tag{18}</script><p>其中，$\mu_{c,i}$ 和 $\sigma^2_{c,i}$ 分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差</p></li></ul></li></ul><p><strong>朴素贝叶斯分类器例子：</strong> 用西瓜数据集 3.0 训练一个朴素贝叶斯分类器，对测试例 “测1” 进行分类：</p><ul><li>西瓜数据集 3.0</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure2.PNG" alt="figure2"></p><ul><li>首先估计类先验概率 $P(c)$</li></ul><script type="math/tex; mode=display">P(好瓜 = 是) = \frac{8}{17}\approx 0.471\\P(好瓜 = 否) = \frac{9}{17}\approx 0.529\\</script><ul><li>然后，为每个属性估计类条件概率 $P(x_i  |  c)$</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure3.PNG" alt="figure3"></p><ul><li>测试例</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure4.PNG" alt="figure4"></p><ul><li>于是，有：</li></ul><p><img src="/2019/08/17/《机器学习》西瓜书学习笔记（七）-- 贝叶斯分类器/figure5.PNG" alt="figure5"></p><ul><li>因此，朴素贝叶斯分类器将测试样本 “测1” 判别为 “好瓜”</li></ul><p><strong>拉普拉斯修正</strong> (Laplacian correction)：</p><ul><li><p>为了避免其他属性携带的信息被训练集中未出现的属性值 “抹去”，在估计概率值是通常要进行 “平滑”</p></li><li><p>拉普拉斯修正：</p><script type="math/tex; mode=display">\begin{align}\hat P(c) &= \frac{|D_c|+1}{|D|+N} \tag{19}\\\hat P(x_i\ |\ c) &= \frac{|D_{c,x_i}|+1}{|D|+N_i}   \tag{20}\\\end{align}</script><p>其中，$N$ 表示训练集 $D$ 中可能的类别数，$N_i$ 表示第 $i$ 个属性可能的取值数</p></li></ul><hr><h3 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h3><hr><h3 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h3><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><hr><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（七）贝叶斯分类器&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（六）-- 支持向量机</title>
    <link href="http://sunfeng.online/2019/08/16/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89--%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://sunfeng.online/2019/08/16/《机器学习》西瓜书学习笔记（六）-- 支持向量机/</id>
    <published>2019-08-16T06:53:52.000Z</published>
    <updated>2019-08-27T13:18:28.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（六）-支持向量机"><a href="#《机器学习》西瓜书学习笔记（六）-支持向量机" class="headerlink" title="《机器学习》西瓜书学习笔记（六） 支持向量机"></a>《机器学习》西瓜书学习笔记（六） 支持向量机</h2><a id="more"></a><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p><strong><font color="#0099">给定训练集，分类学习最基本的想法就是基于训练集在样本空间中找到一个划分超平面，将不同类别的样本划分开来，且要求划分超平面产生的分类结果要尽可能鲁棒，对未见示例的泛化能力要尽可能强</font></strong></p><p><strong>划分超平面</strong></p><ul><li><p>划分超平面的线性方程描述：</p><script type="math/tex; mode=display">\boldsymbol w^T \boldsymbol x + b = 0\tag{6.1}</script><p>其中 $\boldsymbol w = (w_1;w_2;…;w_d)$ 为法向量，决定超平面的方向；</p><p>$b$ 为位移项，决定了超平面与原点之间的距离；</p><p>将此超平面记为 $(\boldsymbol w, b)$</p></li><li><p>样本空间中的任意点 $\boldsymbol x$ 到超平面的 $(\boldsymbol w, b)$ 距离为:</p><script type="math/tex; mode=display">r = \frac{|\boldsymbol w^T \boldsymbol x + b|}{||\boldsymbol w||}\tag{6.2}</script></li></ul><p>假设超平面 $(\boldsymbol w, b)$ 能将训练样本正确分类，即对于 $(\boldsymbol x_i, y_i) \in D$:</p><ul><li><p>若 $y_i = +1$, 则有 $\boldsymbol w^T \boldsymbol x_i + b &gt; 0$</p></li><li><p>若 $y_i = -1$, 则有 $\boldsymbol w^T \boldsymbol x_i + b &lt; 0$ </p></li><li><p>令</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}\boldsymbol w^T \boldsymbol x_i + b \ge +1, \ \ y_i = +1 \\\boldsymbol w^T \boldsymbol x_i + b \le -1, \ \ y_i = -1\end {cases}\tag{6.3}\end {equation}</script></li></ul><p><strong>式 (6.3) 推导</strong></p><ul><li><p>假设超平面是 $(\boldsymbol w^\prime)^T\boldsymbol x + \boldsymbol b^\prime= 0 $, 对于 $(\boldsymbol x_i, y_i)$, 有：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime > 0, \ \ y_i = +1 \\(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime < 0, \ \ y_i = -1\end {cases}\tag{*}\end {equation}</script></li><li><p>根据几何间隔，将以上关系可修正为:</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime \ge +\xi, \ \ y_i = +1 \\(\boldsymbol w^\prime)^T \boldsymbol x_i + b^\prime \le -\xi, \ \ y_i = -1\end {cases}\tag{**}\end {equation}</script></li><li><p>由 $\xi &gt; 0$ 得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}(\frac{1}{\xi} \boldsymbol w^\prime)^T \boldsymbol x_i + \frac{1}{\xi}b^\prime \ge +1, \ \ y_i = +1 \\(\frac{1}{\xi}\boldsymbol w^\prime)^T \boldsymbol x_i + \frac{1}{\xi}b^\prime \le -1, \ \ y_i = -1\end {cases}\tag{***}\end {equation}</script></li><li><p>令 $\boldsymbol w = \frac{1}{\xi} \boldsymbol w^\prime$, $b = \frac{1}{\xi}b^\prime$, 得：</p><script type="math/tex; mode=display">\begin {equation}\begin {cases}\boldsymbol w^T \boldsymbol x_i + b \ge +1, \ \ y_i = +1 \\\boldsymbol w^T \boldsymbol x_i + b \le -1, \ \ y_i = -1\end {cases}\tag{****}\end {equation}</script></li></ul><p><strong>支持向量 (support vector)：</strong> </p><ul><li>距离超平面最近的几个训练样本可使上式 (6.6) 的等号成立，它们被称为 “支持向量”</li></ul><p><strong>间隔 (margin): </strong></p><ul><li>两个异类支持向量到超平面的距离之和为<script type="math/tex; mode=display">\gamma = \frac{2}{||\boldsymbol w ||}\tag{6.4}</script>它被称为 “间隔”</li></ul><p><img src="/2019/08/16/《机器学习》西瓜书学习笔记（六）-- 支持向量机/figure2.PNG" alt="figure2"></p><p><strong>支持向量机(Support Vector Machine, SVM)</strong></p><ul><li><p>最大化间隔：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}& \mathop{\max} _{\boldsymbol w,b}\frac{2}{||\boldsymbol w||}\\& s.t.\ y_i(\boldsymbol w^T \boldsymbol x_i + b) \ge 1,i = 1,2,...,m\end {split}\tag{6.5}\end {equation}</script></li><li><p>最大化 $||\boldsymbol w||^{-1}:$ </p></li><li><p>最小化 $||\boldsymbol w||^{2}:$ </p><script type="math/tex; mode=display">\begin {equation}\begin {split}& \mathop{\min}_{\boldsymbol w,b} \frac{1}{2} {||\boldsymbol w||}^2\\& s.t.\ y_i(\boldsymbol w^T \boldsymbol x_i + b) \ge 1,i = 1,2,...,m\end {split}\tag{6.6}\end {equation}</script></li></ul>   <font color="#0099">此为支持向量机 (Support vector Machine，简称SVM) 的基本型</font><hr><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><h4 id="SVM-基本型的对偶问题-dual-problem"><a href="#SVM-基本型的对偶问题-dual-problem" class="headerlink" title="SVM 基本型的对偶问题 (dual problem)"></a><strong>SVM 基本型的对偶问题</strong> (dual problem)</h4><ul><li><p>式 (6.6) 对应的拉格朗日函数：</p><script type="math/tex; mode=display">L(\boldsymbol w, b, \alpha) = \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\alpha_i\left( 1-y_i(\boldsymbol w^T\boldsymbol x_i+b)\right )}\tag{6.8}</script><p>其中，$\alpha = (\alpha_i;\alpha_2;…;\alpha_m)$.</p></li><li><p>令 $L(\boldsymbol w, b, \alpha)$ 对 $\boldsymbol w$ 和 $b$ 求偏导为零可得:</p><script type="math/tex; mode=display">\begin {align}\boldsymbol w &= \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i}\tag{6.9}\\0 &= \sum_{i=1}^{m}{\alpha_i y_i}\tag{6.10}\end {align}</script></li><li><p>式 (6.9) 和 (6.10) 推导：</p><script type="math/tex; mode=display">\begin {align}L(\boldsymbol w, b, \alpha) &= \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\alpha_i\left( 1-y_i(\boldsymbol w^T\boldsymbol x_i+b)\right )}\\&= \frac{1}{2}\left\|\boldsymbol w \right\|^2+\sum_{i=1}^{m}{\left( \alpha_i-\alpha_iy_i\boldsymbol w^T\boldsymbol x_i+\alpha_iy_ib)\right )}\\&= \frac{1}{2}\left\|\boldsymbol w \right\|^2 + \sum_{i=1}^{m}\alpha_i - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib\end {align}</script><p>对 $\boldsymbol w$ 和 $b$ 求偏导为零可得:</p><script type="math/tex; mode=display">\begin {align}\frac{\partial L}{\partial \boldsymbol w} &= \frac{1}{2}\times 2\times \boldsymbol w + 0 - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i - 0 = 0 \Rightarrow \boldsymbol w = \sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i \\\frac{\partial L}{\partial \boldsymbol b} &= 0 + 0 - 0 - \sum_{i=1}^{m}\alpha_iy_i\Rightarrow 0 = \sum_{i=1}^{m}\alpha_iy_i\end {align}</script></li><li><p>将式 (6.9) 带入 (6.8) 得:</p><script type="math/tex; mode=display">\begin {align}L(\boldsymbol w, b, \alpha) &= \frac{1}{2}\boldsymbol w^T\boldsymbol w +\sum_{i=1}^{m}{\alpha_i} - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib \\&= \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i}+\sum_{i=1}^{m}{\alpha_i} - \sum_{i=1}^{m}\alpha_iy_i\boldsymbol w^T\boldsymbol x_i - \sum_{i=1}^{m}\alpha_iy_ib \\&= \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} - \boldsymbol w^T\sum_{i=1}^{m}\alpha_iy_i\boldsymbol x_i + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&=- \frac{1}{2}\boldsymbol w^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&= - \frac{1}{2}\boldsymbol (\sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i})^T \sum_{i=1}^{m}{\alpha_i y_i\boldsymbol x_i} + \sum_{i=1}^{m}{\alpha_i}  - b\sum_{i=1}^{m}\alpha_iy_i \\&= \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\boldsymbol x_i^T\boldsymbol x_j} -b\sum_{i=1}^{m}\alpha_iy_i \\\end {align}</script></li><li><p>再考虑式 (6.10) 的约束，得到式 (6.6) 的对对偶问题：</p><script type="math/tex; mode=display">\begin {align}&\mathop{\max}_{\alpha}{\sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\boldsymbol x_i^T\boldsymbol x_j}}\tag{6.11}\\&s.t \  \sum_{i=1}^{m}\alpha_iy_i = 0, \ \alpha_i \ge0,i = 1, 2,...,m\end {align}</script></li></ul><hr><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><hr><h3 id="间隔与正则化"><a href="#间隔与正则化" class="headerlink" title="间隔与正则化"></a>间隔与正则化</h3><hr><h3 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（六）-支持向量机&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（六）-支持向量机&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（六） 支持向量机&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（六） 支持向量机&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（五）-- 神经网络</title>
    <link href="http://sunfeng.online/2019/08/14/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89--%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://sunfeng.online/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/</id>
    <published>2019-08-14T07:26:34.000Z</published>
    <updated>2019-08-27T04:38:21.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（五）-神经网络"><a href="#《机器学习》西瓜书学习笔记（五）-神经网络" class="headerlink" title="《机器学习》西瓜书学习笔记（五） 神经网络"></a>《机器学习》西瓜书学习笔记（五） 神经网络</h2><a id="more"></a><h3 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h3><p><strong>神经网络 (neural network):</strong> </p><ul><li>神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的反应</li><li>神经网络中最基本的成分是<strong>神经元模型</strong></li></ul><p><strong>神经元 (neuron) 模型: </strong> M-P神经元模型</p><ul><li>神经元接收到来自 $n$ 个其他神经元传递过来的输入信号</li><li>这些输入信号通过权重的连接进行传递</li><li>神经元收到的总输入值将与神经元的阈值进行比较，然后通过 “激活函数” 处理以产生神经元的输出</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure1.PNG" alt="figure1"></p><p><strong>激活函数 (activation function):</strong></p><ul><li><p><strong>阶跃函数</strong></p><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure2.PNG" alt="figure2"></p></li><li><p><strong>Sigmoid函数</strong> </p><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure3.PNG" alt="figure3"></p></li></ul><hr><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><h4 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机 (Perceptron)"></a>感知机 (Perceptron)</h4><p><strong>感知机：</strong></p><ul><li>由两层神经元组成</li><li>输入层接收外界输入信号传递给输出层</li><li>输出层是M-P神经元</li><li>给定训练数据集，权重 $w_i (i = 1,2,…,n)$ 以及阈值 $\theta$ 可通过学习得到</li><li>将阈值 $\theta$ 可以看做一个固定输入为 -1.0 的 “哑结点” 所对应的权重$w_{n+1}$</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure4.PNG" alt="figure4"></p><p><strong>感知机学习规则：</strong></p><ul><li>对训练样例 $(\boldsymbol x, y)$, 若当前感知机的输出为 $\hat y$, 则感知机的权重将这样调整：<script type="math/tex; mode=display">\begin {equation}\begin {split}w_i &\leftarrow w_i + \Delta w_i,\\\Delta w_i &= \eta(y-\hat y)x_i,\end {split}\end{equation}</script>其中 $\eta \in (0, 1)$ 称为<strong>学习率 (learning rate)</strong>.</li></ul><p><strong>注意：</strong></p><ul><li><p>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限</p></li><li><p>感知机无法解决非线性可分问题</p></li><li><p>要解决非线性可分问题，需要考虑使用多层功能神经元</p></li></ul><h4 id="多层前馈神经网络-multi-layer-feedforward-neural-networks"><a href="#多层前馈神经网络-multi-layer-feedforward-neural-networks" class="headerlink" title="多层前馈神经网络 (multi-layer feedforward neural networks)"></a>多层前馈神经网络 (multi-layer feedforward neural networks)</h4><p><strong>多层前馈神经网络：</strong></p><ul><li>每层神经元与下一层神经元完全互联，神经元之间不存在同层连接，也不存在跨层连接</li><li>输入层神经元接收外界输入</li><li>隐层与输出层神经元对信号进行加工，最终结果有输出层神经元输出</li><li>即，<strong>输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元</strong></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure5.PNG" alt="figure5"></p><p><strong>神经网络学习：</strong></p><ul><li>神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权” (connection weight) 以及每个神经元的阈值</li></ul><hr><h3 id="误差逆传播算法"><a href="#误差逆传播算法" class="headerlink" title="误差逆传播算法"></a>误差逆传播算法</h3><p><strong>误差逆传播算法 (error BackPropagation)：</strong></p><ul><li>一种用来训练神经网络的强大学习算法，简称 <strong>BP 算法</strong></li><li><strong>BP网络</strong>通常是指用 BP 算法训练的多层前馈神经网络</li></ul><p><strong>符号定义：</strong></p><ul><li><p><strong>训练集：</strong> $D = \{(\boldsymbol x_1, \boldsymbol y_1), (\boldsymbol x_2, \boldsymbol y_12, …, (\boldsymbol x_m, \boldsymbol y_m) \},  \boldsymbol x_i \in R^d, \boldsymbol y_i \in R^l$ , 即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量</p></li><li><p><strong>网络结构：</strong> </p><ul><li>$d$ 个输入神经元</li><li>$l$  个输出神经元</li><li>$q$ 个隐层神经元</li></ul></li><li><p><strong>阈值：</strong></p><ul><li>$\theta_j$: 输出层第 $j$ 个神经元的阈值</li><li>$\gamma_h$: 隐层第 $h$ 个神经元的阈值</li></ul></li><li><p><strong>连接权：</strong></p><ul><li>$v_{ih}$: 输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间连接权</li><li>$w_{hj}$: 隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权</li></ul></li><li><p><strong>神经元输出：</strong></p><ul><li>$b_h$: 隐层第 $h$ 个神经元的输出</li></ul></li><li><p><strong>神经元输入：</strong></p><ul><li>$a_h$: 隐层第 $h$ 个神经元接收到的输入 $a_h = \sum_{i=1}^{d}v_{ih}x_i$</li><li>$\beta_j$: 输出层第 $j$ 个神经元接收到的输入 $\beta_j = \sum_{i=1}^{d}w_{hj}b_h$ </li></ul></li><li><p><strong>激活函数：</strong></p><ul><li>假设隐层和输出层神经元都使用 <strong><em>Sigmoid</em> 函数</strong></li></ul></li><li><p><strong>神经网络输出：</strong></p><ul><li>对训练样例 $(\boldsymbol x_k, \boldsymbol y_k)$, 假定神经网络输出为 $\hat y_k = (\hat y_1 ^k,\hat y_2 ^k,…,\hat y_l ^k)$, 即：<script type="math/tex; mode=display">\hat y_j^k = f(\beta _j - \theta_j) \tag {1}</script></li></ul></li><li><p><strong>均方误差：</strong></p><ul><li>网络在样例 $(\boldsymbol x_k, \boldsymbol y_k)$ 上的误差为：<script type="math/tex; mode=display">E_k = \frac{1}{2}\sum_{j=1}^{l}{(\hat y_j^k - y_j^k)^2} \tag{2}</script></li></ul></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure6.PNG" alt="figure6"></p><p><strong>需要学习的参数：</strong> 基于上述定义，网络中有 $(d + l + 1)q + l$ 个参数需要确定</p><ul><li>输入层到隐层的 $d\times q$ 个权值</li><li>隐层到输出层的 $q \times l$ 个权值</li><li>隐层的 $q$ 个阈值</li><li>输出层的 $l$ 个阈值</li></ul><p><strong>参数更新：</strong></p><ul><li><p>BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，即对任意参数 $v$ 的更新为:</p><script type="math/tex; mode=display">v \leftarrow v + \Delta v</script></li><li><p>基于<strong>梯度下降 (gradient descent)</strong>策略，以目标的负梯度方向对参数进行调整，对式(2)的误差 $E_k$ , 给定学习率 $\eta$, 有：</p><script type="math/tex; mode=display">\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}\tag{3}</script></li><li><p><strong>反向传播</strong>：由于 $w_{hj}$ 先影响到第 $j$ 个输出层神经元的输入值 $\beta_j$, 再影响到其输出值 $\hat y_j^k$ , 然后影响到 $E_k$, 由链式法则得:</p><script type="math/tex; mode=display">\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat y_j^k} \cdot \frac{\partial y_j^k}{\partial \beta _j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}\tag{4}</script></li><li><p>根据 $\beta_j$ 的定义，得:</p><script type="math/tex; mode=display">\frac{\partial \beta_j}{\partial w_{hj}} = b_h \tag{5}</script></li><li><p>Sigmoid函数 $f(x)$ 的导数：</p><script type="math/tex; mode=display">f\prime(x) = f(x)(1-f(x)) \tag{6}</script></li><li><p>根据式(1)(2)和(6)，有:</p><script type="math/tex; mode=display">\begin {equation}\begin {split}g_j &= - \frac{\partial E_k}{\partial \hat y_j^k} \cdot \frac{\partial y_j^k}{\partial \beta _j}\\&= -(\hat y_j^k - y_j^k)f\prime(\beta_j-\theta_j)\\&= \hat y_j ^k(1-\hat y_j^k)(y_j^k-\hat y_j^k) \end {split}\end {equation}\tag{7}</script></li><li>由式(3)(4)(5)和(7)即可得 BP 算法中关于 $w_{hj}$ 的更新公式:<script type="math/tex; mode=display">  \Delta w_{hj} = \eta g_jb_h \tag{8}</script></li></ul><ul><li><p>类似可得：</p><script type="math/tex; mode=display">\begin {align}\Delta \theta_j &= -\eta g_j\tag{9}\\ \Delta v_{ih} &= \eta e_hx_i \tag{10}\\ \Delta \gamma_h &= -\eta e_h \tag{11}\end {align}</script></li><li><p>其中：</p><script type="math/tex; mode=display">\begin {equation}  \begin {split}  e_h &= -\frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial\alpha_h}\\  &= -\sum_{j=1}^{l}\frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f\prime(\alpha_h-\gamma_h)\\  &=\sum_{j=1}^{l}w_{hj}g_jf\prime(\alpha_h-\gamma_h)\\  &= b_h(1-b_h)\sum_{j=1}^{l}w_{hj}g_j   \end {split}  \end {equation}  \tag{12}</script></li></ul><p><strong>BP算法工作流程:</strong> 对于每个输入样例：</p><ul><li>先将输入示例提供给输入层神经元</li><li>然后逐层将信号前传，直到产生输出层的结果</li><li>然后计算输出层的误差 (4-5行)</li><li>再将误差逆向传播至隐层神经元 (第6行)</li><li>最后根据隐层神经元的误差来对连接权和阈值进行调整 (第7行)</li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure7.PNG" alt="figure7"></p><p><strong>标准BP算法：</strong> </p><ul><li>以上介绍的误差逆传播算法称为 <strong>标准BP算法</strong> </li><li>更新规则基于单个的 $E_k$ 推导而得</li><li>每次仅针对一个训练样本更新连接权和阈值</li><li>参数更新的非常频繁，而且对于不同的样例进行更新的效果可能出现 “抵消” 现象</li><li>需要更多的迭代次数</li></ul><p><strong>累积BP算法：</strong> </p><ul><li>累积误差逆传播算法是要最小化训练集 $D$ 上的累积误差: $E = \frac{1}{m}\sum_{k=1}^{m}E_k$</li><li>在读取整个训练集 $D$ 一遍后才对参数进行更新</li><li>参数更新频率低得多</li></ul><p><strong>缓解BP网络过拟合的策略：</strong></p><ul><li><p><strong>早停 (early stopping):</strong> </p><ul><li>将数据集分为训练集和验证集</li><li>训练集用来计算梯度、更新连接权和阈值</li><li>验证集用来估计误差</li><li>若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值</li></ul></li><li><p><strong>正则化 (regularization):</strong> </p><ul><li><p>基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分</p></li><li><p>例如连接权和阈值的平方和</p></li><li><p>仍令 $E_k$ 表示第 $k$ 个训练样本上的误差， $w_i$ 表示连接权和阈值，则误差目标函数改变为：</p><script type="math/tex; mode=display">E_k = \lambda\frac{1}{m}\sum_{k=1}^{m}E_k + (1-\lambda)\sum_{i}{w_i^2}</script><p>其中 $\lambda \in (0,1)$ 用于对经验误差和网络复杂度这两项进行折中，常通过交叉验证法来估计。</p></li></ul></li></ul><hr><h3 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h3><h4 id="局部极小-local-minimum"><a href="#局部极小-local-minimum" class="headerlink" title="局部极小 (local minimum)"></a>局部极小 (local minimum)</h4><p><strong>局部极小解：</strong> </p><ul><li>对 $ \boldsymbol w^\prime$ 和 $ \theta^\prime $，若存在 $\epsilon &gt; 0$ 使得：</li></ul><script type="math/tex; mode=display">\forall (\boldsymbol w;\theta) \in \{(\boldsymbol w; \theta) \boldsymbol | \ ||(\boldsymbol w;\theta) - (\boldsymbol w^\prime;\theta^\prime)|| \le \epsilon \}</script><p>都有 $E(\boldsymbol w;\theta ) &gt; E(\boldsymbol w^\prime;\theta^\prime)$ 成立，则 $(\boldsymbol w^\prime;\theta^\prime)$ 为局部极小解</p><ul><li><font color="#0099ff">局部极小解是参数空间中的某个点，其领域点的误差函数值均不小于该点的函数值</font></li><li><p>参数空间内梯度为零的点，只要其误差函数值小于邻点的误差函数值，就是局部极小点</p></li><li><p>可能存在多个局部极小值，但却只会有一个全局最小值</p></li></ul><h4 id="全局最小-global-minimum"><a href="#全局最小-global-minimum" class="headerlink" title="全局最小 (global minimum)"></a>全局最小 (global minimum)</h4><p><strong>全局最小解：</strong></p><ul><li><p>对 $\boldsymbol w^\prime$ 和 $\theta^\prime$，若存在 $\epsilon &gt; 0$ 使得：</p><script type="math/tex; mode=display">\forall (\boldsymbol w;\theta) \in \{(\boldsymbol w; \theta) \boldsymbol | \ ||(\boldsymbol w;\theta) - (\boldsymbol w^\prime;\theta^\prime)|| \le \epsilon \}</script><p>对参数空间的任意 $(\boldsymbol w;\theta)$, $E(\boldsymbol w;\theta ) &gt; E(\boldsymbol w^\prime;\theta^\prime)$ 成立，则 $(\boldsymbol w^\prime;\theta^\prime)$ 为全局最小解</p></li><li><font color="#0099ff">全局最小解则是指参数空间中所有点的误差函数值均不小于该点的误差函数值</font></li><li><p>全局最小一定是局部极小，反之不成立</p></li></ul><p><img src="/2019/08/14/《机器学习》西瓜书学习笔记（五）-- 神经网络/figure8.PNG" alt="figure8"></p><h4 id="参数寻优"><a href="#参数寻优" class="headerlink" title="参数寻优"></a>参数寻优</h4><p><strong>参数寻优：</strong></p><ul><li>参数寻优过程中就是希望找到全局最小</li><li>参数寻优方法：<strong>梯度下降</strong></li></ul><p><strong>梯度下降法参数寻优：</strong></p><ul><li>从某些初始解出发，迭代寻找参数最优解</li><li>每次迭代中，先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向</li><li>由于梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜素最优解</li><li>若误差函数的当前梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止</li><li>如果误差函数仅有一个局部极小，则此时找到的局部极小就是全局最小</li><li>如果误差函数具有多个局部极小，则不能保证找的的解是全局最小，即<strong>参数寻优陷入了局部极小</strong></li></ul><p><strong>“跳出”局部极小，达到全局最小：</strong></p><ul><li>以多组不同参数初始化多个神经网络，选择最接近全局最小的</li><li><strong>模拟退火</strong>：每一步以一定的概率接收比当前解更差的结果</li><li><strong>随机梯度下降</strong></li></ul><hr><h3 id="其他常见的神经网络"><a href="#其他常见的神经网络" class="headerlink" title="其他常见的神经网络"></a>其他常见的神经网络</h3><h4 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h4><h4 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h4><h4 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h4><h4 id="级联相关网络"><a href="#级联相关网络" class="headerlink" title="级联相关网络"></a>级联相关网络</h4><h4 id="Elman网络"><a href="#Elman网络" class="headerlink" title="Elman网络"></a>Elman网络</h4><h4 id="Boltzmann机"><a href="#Boltzmann机" class="headerlink" title="Boltzmann机"></a>Boltzmann机</h4><hr><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（五）-神经网络&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（五）-神经网络&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（五） 神经网络&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（五） 神经网络&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（四）-- 决策树</title>
    <link href="http://sunfeng.online/2019/08/13/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89--%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://sunfeng.online/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/</id>
    <published>2019-08-13T08:43:48.000Z</published>
    <updated>2019-08-27T03:31:46.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（四）-决策树"><a href="#《机器学习》西瓜书学习笔记（四）-决策树" class="headerlink" title="《机器学习》西瓜书学习笔记（四） 决策树"></a>《机器学习》西瓜书学习笔记（四） 决策树</h2><a id="more"></a><h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><p><strong>决策树 (decision tree)：</strong></p><ul><li>一类常见的机器学习算法</li><li>决策树是基于树形结构进行决策的</li><li>决策树学习的目的是产生一棵泛化能力强，即处理未见示例能力强的决策树</li></ul><p><strong>基本流程：</strong>决策树基本流程遵循简单直观的“分而治之”策略，如下图所示：</p><p><img src="/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/figure1.PNG" alt="figure1"></p><p><strong>在决策树基本算法中，有三种情形会导致递归返回：</strong></p><ul><li>当前结点包含的样本全属于同一类别，无需划分</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分<ul><li>将当前结点标记为叶子结点</li><li>并将其类别设定为该结点所含样本最多的类别</li></ul></li><li>当前结点包含的样本集合为空，不能划分<ul><li>将当前结点标记为叶子结点</li><li>并将类别设定为其父节点所含样本最多的类别</li></ul></li></ul><hr><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><p>决策树学习的关键是 <strong>如何选择最优划分属性</strong>，<font color="#0099ff">一般而言，随着划分过程的不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。</font> </p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p><strong>信息熵 (information entropy):</strong></p><ul><li><p>信息熵是度量样本集合<strong>纯度</strong>最常用的一种指标。</p></li><li><p>假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k (k = 1, 2, …, |{\cal Y}|)$, 则 $D$ 的信息熵定义为：</p><script type="math/tex; mode=display">Ent(D) = - \sum _{k = 1}^{|{\cal Y}|}p_k\log_2p_k</script><p><strong>$Ent(D)$ 的值越小，则 $D$ 的纯度越高</strong></p></li></ul><p><strong>信息增益 (information gain):</strong></p><ul><li><p>假设离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, … , a^V\}$</p></li><li><p>使用 $a$ 对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$.</p><script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum _{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v).</script><p><strong>信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的 “纯度提升”越大</strong>。</p></li></ul><p><strong>以信息增益为准则选择划分属性：</strong></p><script type="math/tex; mode=display">a_* = \mathop{\arg\max}_{a\in A} Gain(D, a)</script><h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4><p><em>信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性。</em></p><p><strong>增益率 (gain ratio):</strong></p><script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D, a)}{IV(a)}</script><p>其中：</p><script type="math/tex; mode=display">IV(a) = - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}</script><p>称为属性 $a$ 的 “固有值”</p><h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p><em>GART 决策树使用 “基尼指数”来选择划分属性</em></p><p><strong>基尼值：</strong>用来度量数据集 $D$ 的纯度</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gini(D) &= \sum_{k=1}^{|y|}\sum_{k\prime \ne k} p_kp_{k\prime}\\&= 1 - \sum_{k=1}^{|y|}{p_k^2}\end {split}\end {equation}</script><p><strong>$Gini(D)$ 越小，则数据集 $D$ 的纯度越高</strong></p><p><strong>基尼指数 (Gini index):</strong></p><script type="math/tex; mode=display">Gini\_index(D, a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)</script><hr><h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><p><strong>剪枝 (pruning):</strong> 决策树学习算法对付“过拟合”的主要手段</p><h4 id="预剪枝-pre-pruning"><a href="#预剪枝-pre-pruning" class="headerlink" title="预剪枝 (pre-pruning)"></a>预剪枝 (pre-pruning)</h4><ul><li>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分并将当前结点标记为叶结点</li><li>具体示例详见课本 <em>P-80</em></li></ul><h4 id="后剪枝-post-pruning"><a href="#后剪枝-post-pruning" class="headerlink" title="后剪枝 (post-pruning)"></a>后剪枝 (post-pruning)</h4><ul><li>先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应子树替换成叶结点能带来决策树泛化能力的提升，则将该子树替换为叶结点</li><li>具体示例详见课本 <em>P-82</em></li></ul><hr><h3 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h3><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p><em>常用的将连续属性离散化技术是二分法策略。</em></p><p><strong>二分法 (bi-partition):</strong></p><ul><li><p>给定样本集 $D$ 和连续属性 $a$ , 假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值</p></li><li><p>将这些值从小到大进行排序，记为 $\{a^1, a^2,…,a^n\}$.</p></li><li><p>基于划分点 $t$, 将 $D$ 分为两个子集 $D_t^-$ 和 $D_t^+$ </p></li><li><p>其中 $D_t^-$ 包含那些在属性 $a$ 取值不大于 $t$ 的样本，而 $D_t^+$ 则包含那些在属性 $a$ 取值大于 $t$ 的样本</p></li><li><p>对于相邻属性取值 $a^i$ 和 $a^{i+1}$ 来说，$t$ 在区间 $[a^i, a^{i+1})$ 中取任意值所产生的划分结果相同</p></li><li><p>因此，对于连续属性 $a$, 我们可考察包含 $n-1$ 个元素的候选划分点集合：</p><script type="math/tex; mode=display">T_a = \{\frac{a^i + a^{i+1}}{2} \ |\ 1 \le i \le n-1 \}</script><p>即，以区间 $[a^i, a^{i+1})$ 的中位点 $\frac{a^i + a^{i+1}}{2}$ 作为候选划分点</p></li><li><p>选取最优划分点进行样本集合划分：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gain(D, a) &= max_{t \in T_a} Gain(D,a,t) \\&= max_{t \in Ta} Ent(D) - \sum_{\lambda \in \{-, +\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)\end {split}\end {equation}</script><p>其中 $Gain(D, a, t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益</p></li><li><p>于是，我们就可选出使  $Gain(D, a, t)$ 最大化的划分点</p></li></ul><h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><h5 id="如何在属性值缺失的情况下进行划分属性选择？"><a href="#如何在属性值缺失的情况下进行划分属性选择？" class="headerlink" title="如何在属性值缺失的情况下进行划分属性选择？"></a>如何在属性值缺失的情况下进行划分属性选择？</h5><ul><li><p>给定训练集 $D$ 和属性 $a$ , 令 $\tilde D$ 表示 $D$ 在属性 $a$ 上没有缺失值的样本子集</p></li><li><p>假定属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, … ,a^V \}$</p></li><li><p>令 $\tilde D^v$ 表示 $\tilde D$ 中在属性 $a$ 取值为 $a^v$ 的样本子集，则 $\tilde D  = \bigcup_{v=1}^{V}\tilde D^v.$</p></li><li><p>令 $\tilde D_k$ 表示 $\tilde D$ 中属于第  $k$ 类 $(k = 1,2 , … , |y|)$ 的样本子集 $\tilde D  = \bigcup_{k=1}^{|y|}\tilde D_k.$</p></li><li><p>假定为每个样本 $x$ 赋予一个权重 $w_x$, 并定义：</p><ul><li><p>$\rho$ 表示无缺失值样本所占的比例：</p><script type="math/tex; mode=display">\rho = \frac{\sum_{x\in\tilde D}w_x}{\sum_{x\in D}w_x}</script></li><li><p>$\tilde p_k$ 表示无缺失值样本中第 $k$ 类所占的比例：</p><script type="math/tex; mode=display">\tilde p_k = \frac{\sum_{x\in\tilde D_k}w_x}{\sum_{x\in\tilde D}w_x}</script><p>显然：$\sum_{k=1}^{|y|}\tilde p_k = 1$</p></li><li><p>$\tilde r_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例：</p><script type="math/tex; mode=display">\tilde r_v = \frac{\sum_{x\in\tilde D^v}w_x}{\sum_{x\in\tilde D}w_x}</script><p>显然：$\sum_{v=1}^{V}\tilde r_v = 1$</p></li></ul></li><li><p>基于上述定义，我们可以将信息增益的计算式推广为：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gain(D, a) &= \rho \times Gain(\tilde D, a)\\&= \rho \times (Ent(\tilde D) - \sum_{v =1}^{V}\tilde r_v Ent(\tilde D^v))\end {split}\end{equation}</script><p>其中：</p><script type="math/tex; mode=display">Ent(\tilde D) = -\sum _{k=1}^{|y|}{\tilde p_k \log_2 \tilde p_k}</script></li></ul><h5 id="给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？"><a href="#给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？" class="headerlink" title="给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？"></a>给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？</h5><ul><li><strong>若样本 $x$ 在划分属性 $a$ 上的取值已知：</strong> 则将 $x$ 划入与其取值对应的子结点，且样本权值在子结点中保持为 $w_x$ </li><li><strong>若样本 $x$ 在划分属性 $a$ 上的取值未知：</strong> 则将 $x$ 同时划分到所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde r^v \cdot w_x$ </li></ul><hr><h3 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h3><p><strong>多变量决策树 (multivariate decision tree)：</strong></p><ul><li>能实现 <strong>“斜划分”</strong> 甚至更复杂划分的决策树</li><li>非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试</li><li>在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（四）-决策树&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（四）-决策树&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（四） 决策树&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（四） 决策树&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（三）-- 线性模型</title>
    <link href="http://sunfeng.online/2019/08/10/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://sunfeng.online/2019/08/10/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/</id>
    <published>2019-08-10T06:54:48.000Z</published>
    <updated>2019-08-27T03:16:35.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（三）线性模型"><a href="#《机器学习》西瓜书学习笔记（三）线性模型" class="headerlink" title="《机器学习》西瓜书学习笔记（三）线性模型"></a>《机器学习》西瓜书学习笔记（三）线性模型</h2><a id="more"></a><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><h4 id="1-回顾各符号对应的概念"><a href="#1-回顾各符号对应的概念" class="headerlink" title="1. 回顾各符号对应的概念"></a>1. 回顾各符号对应的概念</h4><p><strong>示例：</strong> $\boldsymbol x = (x_1; x_2; … ; x_d)$</p><p><strong>属性：</strong> $x_i$ </p><p><strong>属性个数(维度)：</strong> $d$ </p><p><strong>数据集：</strong> $D = \{\boldsymbol x_1, \boldsymbol x_2, …, \boldsymbol x_m\}$ </p><p><strong>模型：</strong> $f$ </p><p><strong>模型在示例 $\boldsymbol x$ 上的预测输出：</strong>  $f(\boldsymbol x)$</p><h4 id="2-线性模型-linear-model"><a href="#2-线性模型-linear-model" class="headerlink" title="2. 线性模型 (linear model)"></a>2. 线性模型 (linear model)</h4><p><strong>线性模型的目的：</strong> 试图学得一个【通过属性的线性组合来进行预测】的【函数】</p><p><strong>线性模型的本质：</strong> 学得一个线性函数</p><p><strong>线性模型的特征：</strong> 通过属性的线性组合来进行预测，即：</p><script type="math/tex; mode=display">f(\boldsymbol x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b</script><p>向量形式为:</p><script type="math/tex; mode=display">f(x) = \boldsymbol w^T\boldsymbol x + b</script><p>其中 $\boldsymbol w = (w1;w2;…;wd)$ ，<font color="#0099ff">由于w直观的表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性。</font></p><hr><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="1-线性回归-linear-regression"><a href="#1-线性回归-linear-regression" class="headerlink" title="1. 线性回归 (linear regression)"></a>1. 线性回归 (linear regression)</h4><p>给定数据集 $D = {(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), … , (\boldsymbol x_m, y_m)}$ , 其中 $\boldsymbol x_i = (x_{i1}; x_{i2}; … x_{id}), y_i \in R$ .</p><p><strong>线性回归的目的：</strong> 试图学得一个【线性模型】以尽可能准确地【预测实值输出标记】</p><p><strong>线性回归的本质：</strong> 学得线性模型</p><p><strong>线性模型的作用：</strong> 预测实值输出标记</p><p><strong>线性模型的目的：</strong> 试图学得一个【通过属性的线性组合来进行预测】的【函数】</p><p><strong><font color="#0099ff">总的来讲，线性回归是一个函数，通过属性的线性组合来进行预测，尽可能准确地预测实值输出标记。</font></strong></p><p><strong>线性回归的分类：</strong></p><ul><li><font color="#0099ff">一元线性回归分析：</font>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，则称为一元线性回归分析</li><li><font color="#0099ff">多元线性回归分析：</font>回归分析中，包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析</li></ul><h4 id="2-最小二乘法-least-square-method"><a href="#2-最小二乘法-least-square-method" class="headerlink" title="2. 最小二乘法 (least square method)"></a>2. 最小二乘法 (least square method)</h4><p><strong>最小二乘法：</strong> 基于均方误差最小化来进行模型求解的方法，均方误差对应了欧氏距离</p><ul><li>所谓“二乘”，就是用平方来度量观测点与估计点的远近</li><li>所谓“最小”，是指参数的估计值要保证各个观测点与估计点的距离平方和达到最小</li></ul><p><strong>最小二乘法的目的：</strong> 在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到这条直线上的欧式距离之和最小</p><p><strong>进一步解释：</strong> <font color="#0099ff">最小二乘法是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简单的求得未知数据，并使得这些求得的数据与实际数据之间误差的平方和最小。</font></p><p><strong>在线性回归中的误差平方和：</strong></p><ul><li>如果说误差就是预测点 $f(x_i)$ 到标记点 $y_i$ 的距离</li><li><p>那么均方误差 $E(f;D) = \frac{1}{m} \sum_{i = 1}^{m} {(f(x_i)-y_i)^2}$ 则可以体现误差的平方和</p><font color="#0099ff">这就意味着，线性回归需要最小化均方误差</font></li></ul><h4 id="3-属性数目为1的简单例子-一元线性回归"><a href="#3-属性数目为1的简单例子-一元线性回归" class="headerlink" title="3. (属性数目为1的简单例子)一元线性回归"></a>3. (属性数目为1的简单例子)一元线性回归</h4><p>我们先考虑一种最简单的情形：输入属性的数目只有一个</p><p>此时数据集为：$D = \{(x_1, y_1), (x_2, y_2), … , (x_m, y_m)\}$</p><p><strong>线性回归模型试图学得：</strong></p><script type="math/tex; mode=display">f(x_i) = wx_i + b, 使得f(x_i) \approx y_i</script><p><strong>如何确定 $w$ 和 $b$：</strong> 最小二乘法，试图让均方误差最小化，即：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}(w^*, b^*) &= \mathop{\arg\min}_{(w,b)} \ E_{(w, b)}\\&= \mathop{\arg\min}_{(w,b)} \ \sum_{i = 1}^{m}{(f(x_i) - y_i)^2}\\&= \mathop{\arg\min}_{(w,b)} \ \sum_{i = 1}^{m}{(y_i - wx_i - b)^2}\\\end{split}\end{equation}</script><p><strong>参数估计：</strong> <font color="#0099ff">求解w和b使均方误差最小化的过程，称为线性回归模型的最小二乘“参数估计”（parameter estimation）</font></p><p>根据函数知识可知，一般 U形曲线的函数，如$f(x) = x^2$ ，通常都是凸函数，其最小值点一般在函数的极小值点处，也就是偏导数为0的点。</p><p>所以，我们可以将 $E(w,b)$ 分别对 $w$ 和 $b$ 求偏导，并令偏导等于0，即可得到 $w$ 和 $b$ 的最优解，具体过程如下：</p><ol><li><p>将 $E(w,b)$ 分别对 $w$ 和 $b$ 求偏导</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{\partial{E_{(w,b)}}}{\partial w} &= 2(w\sum_{i = 1}^{m}{x_i^2} - \sum_{i = 1}^{m}{(y_i - b)x_i}) \\\frac{\partial{E_{(w,b)}}}{\partial b} &= (mb - \sum_{i = 1}^{m}{(y_i-wx_i)}) \\\end{split}\end{equation}</script></li><li><p>令偏导为零, 得到得到 $w$ 和 $b$ 的最优解</p><script type="math/tex; mode=display">\begin{equation}\begin{split}w &= \frac{\sum_{i=1}^{m}{y_i(x_i - \overline x)}}{\sum_{i = 1}^{m}{x_i^2}-\frac{1}{m}{(\sum_{i=1}^{m}{x_i})^2}} \\b &= \frac{1}{m} \sum_{i=1}^{m}{(y_i - wx_i)^2} \\\end{split}\end{equation}</script><p>其中 $\overline x = \frac{1}{m} \sum_{i=1}^{m}x_i$ 为 $x$ 的均值。</p></li></ol><h4 id="4-属性数目为d的复杂例子-多元线性回归"><a href="#4-属性数目为d的复杂例子-多元线性回归" class="headerlink" title="4. (属性数目为d的复杂例子)多元线性回归"></a>4. (属性数目为d的复杂例子)多元线性回归</h4><p>现实中我们几乎碰不见属性值个数为1的例子，通常情况下，样本由d个属性描述。</p><p>此时，<strong>数据集</strong>为：$D = \{ (\boldsymbol x_1, y_1),  (\boldsymbol x_2, y_2), … , (\boldsymbol x_m, y_m)\}$ ，其中 $\boldsymbol x_i = (x_{i1}; x_{i2}; … x_{id}), y_i \in R$ </p><p><strong>线性回归模型试图学得：</strong> </p><script type="math/tex; mode=display">f(\boldsymbol x_i) = w\boldsymbol x_i + b, 使得f(\boldsymbol x_i) \approx y_i</script><p><strong>如何确定 w 和 b：</strong> 类似的，可以用最小二乘法来对 $w$ 和 $b$ 进行估计，具体过程如下：</p><ol><li><p>将$w$ 和 $b$ 吸入向量形式 $\hat w = (w; b)$ </p></li><li><p>把数据集 $D$ 表示成一个 $m \times (d + 1)$ 大小的矩阵 $\boldsymbol X$，其中每行对应一个示例，该行前个 $d$ 元素对应于示例的 $d$ 个属性值，最后一个元素恒置为1，即：</p><script type="math/tex; mode=display">\boldsymbol X = \begin{pmatrix}x_{11} & x_{12} & \ldots & x_{1d} & 1 \\x_{21} & x_{22} & \ldots & x_{2d} & 1 \\\vdots & \vdots & \ddots & \vdots & \vdots & \\x_{m1} & x_{m2} & \ldots & x_{md} & 1 \end{pmatrix} =\begin{pmatrix}\boldsymbol x_1^T & 1 \\\boldsymbol x_2^T & 1 \\\vdots & \vdots \\\boldsymbol x_m^T & 1 \end{pmatrix}</script></li><li><p>把标记写成向量形式：$y = (y_1; y_2; …;y_m)$ </p></li><li><p>求预测的误差平方和：$E_{\hat w} = (y - \boldsymbol X \hat w)^T(y - \boldsymbol X \hat w)$ </p></li><li><p>最小化均方误差：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}\hat w^* &= \mathop{\arg \min}_{\hat w} \ E_{\hat w}\\&= \mathop{\arg \min}_{\hat w} \ (y - \boldsymbol X \hat w)^T(y - \boldsymbol X \hat w)\end{split}\end {equation}</script></li><li><p>将 $E_{\hat w}$ 对 $\hat w$ 求导得到:</p><script type="math/tex; mode=display">\frac{\partial E_{\hat w}}{\partial {\hat w}} = 2 \boldsymbol X^T(\boldsymbol X \hat w - y)</script></li><li><p>然后分别考虑特殊情况和一般情况：</p><ul><li><p>特殊情况：$\boldsymbol X^T \boldsymbol X$ 为满秩矩阵或正定矩阵时，令偏导值为0，可得 $\boldsymbol {\hat w ^*} = (\boldsymbol X^T \boldsymbol X)^{-1}\boldsymbol X^T y$ , 令 $\boldsymbol {\hat x_i} = (\boldsymbol x_i; 1)$ ，则最终学得的多元线性回归模型为：</p><script type="math/tex; mode=display">f(\hat x_i) = \hat x_i^T  (\boldsymbol X^T \boldsymbol X)^{-1}\boldsymbol X^T y.</script></li><li><p>一般情况：实际上，$\boldsymbol X^T \boldsymbol X$ 一般都不是满秩矩阵，此时可以解出来多个 $\boldsymbol {\hat w}$ , 它们都能使均方误差最小化，选择哪一个作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。</p></li></ul></li></ol><h4 id="5-线性模型的丰富变化"><a href="#5-线性模型的丰富变化" class="headerlink" title="5. 线性模型的丰富变化"></a>5. 线性模型的丰富变化</h4><ul><li><p><strong>线性回归模型：</strong> </p><p>将线性模型的预测值逼近真实标记 $y$, 即：$y = w^T \boldsymbol x + b$ </p></li><li><p><strong>对数线性回归：</strong> </p><p>将线性模型的预测值逼近真实标记 $y$ 的衍生物，如对数：$\ln(y) = w^T \boldsymbol x + b$ </p></li><li><p><strong>广义线性模型：</strong> </p><p>更一般地，考虑单调可微函数 $g(.)$, 可以得到更多的真实标记 $y$ 的衍生物，令 $g(y) = w^T \boldsymbol x + b$, 即 $y = g^{-1}{(w^T \boldsymbol x + b)}$, 这样得到的模型称为广义线性模型，其中 $g(.)$ 称为 “联系函数”。</p></li></ul><hr><h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p><strong>预测：</strong> 依靠机器学习学得的模型，对新的示例进行结果判断</p><ul><li><strong>回归任务：</strong> 预测的是连续值</li><li><strong>分类任务：</strong> 预测的是离散值</li></ul><p>如果说线性模型预测连续值，只需要让预测值逼近真实标记 $y$ 或其衍生物的话，当预测离散值的时候，如何让线性模型的预测值 (连续) 和真实标记值 $y$ (离散) 联系起来呢？</p><p>其实，离散状态的真实标记 $y$, 未尝不可以有一种连续的衍生物 $z$, 这样通过单调可微的联系函数 $g(.)$ , 就可以让连续的预测值联系离散的真实标记。</p><h4 id="1-单位阶跃函数-unit-step-function"><a href="#1-单位阶跃函数-unit-step-function" class="headerlink" title="1. 单位阶跃函数 (unit-step function)"></a>1. 单位阶跃函数 (unit-step function)</h4><p>在二分类任务当中，输出标记为 $y = \{0, 1\}$ , 而线性回归模型产生的预测值：$z = \boldsymbol w^T \boldsymbol x + b$   是实值。要将连续值 $z$ 转换为离散值 0/1，最理想的是 “单位阶跃函数” </p><script type="math/tex; mode=display">\begin {equation}y = \begin {cases}0, & z < 0;\\0.5 & z = 0; \\1 & z > 0;\end {cases}\end {equation}</script><p><img src="/2019/08/10/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure.PNG" alt="Figure1"></p><h4 id="2-对数几率函数-logistic-function"><a href="#2-对数几率函数-logistic-function" class="headerlink" title="2. 对数几率函数 (logistic function)"></a>2. 对数几率函数 (logistic function)</h4><p>单位阶跃函数虽好，但是不可微，不是理想的联系函数 $g(.)$ , 我们希望找到一个在形态上趋近于单位阶跃函数的连续函数，并且是单调可微的。对数几率函数正是这样的一个函数：</p><script type="math/tex; mode=display">y = \frac {1}{1 + e^{-z}}</script><p><img src="https://pic1.zhimg.com/80/v2-433cdde5a09ff2450cb99383ad8211b8_hd.jpg" alt="img"></p><p>将 $ z = \boldsymbol w^T \boldsymbol x + b$ 带入可得 $y = \frac {1}{1 + e^{-(  \boldsymbol w^T \boldsymbol x + b)}}$ , 亦即 $\ln \frac{y}{1-y} =   \boldsymbol w^T \boldsymbol x + b$. </p><p>如果说将 $y$ 看作样本 $x$ 作为正例的可能性，那么 $1-y$ 就是其成反例的可能性。</p><p><strong>几率：</strong> 两者的比值即为 “几率”，反映了样本作为正例的相对可能性</p><script type="math/tex; mode=display">几率 = \frac {正例可能性}{反例可能性} = \frac{y}{1-y}</script><p><strong>对数几率：</strong> 对几率取对数就可得到 “对数几率”</p><script type="math/tex; mode=display">对数几率 = \ln \frac{y}{1-y}</script><p><strong>对数几率回归：</strong> 用线性模型的预测结果去逼近真实标记的对数几率，其对应的模型叫做“对数几率回归”(logistic regression，亦称逻辑回归)</p><hr><h3 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析 (LDA)"></a>线性判别分析 (LDA)</h3><p><strong>线性判别分析 (Linear Discriminant Analysis, LDA)</strong> 是一种经典的用于解决二分类问题的线性学习方法。</p><h4 id="1-二分类任务中的LDA"><a href="#1-二分类任务中的LDA" class="headerlink" title="1. 二分类任务中的LDA"></a>1. 二分类任务中的LDA</h4><p><strong>LDA大概分为三个步骤：</strong></p><ul><li>给定样例</li><li>寻找一条满足 “同类近，异类远”的投影直线</li><li>新样本的分类依靠投影后点的位置来确定</li></ul><p><img src="https://pic3.zhimg.com/80/v2-cdae5065290f57c9fc387b7d05b5b68a_hd.jpg" alt="img"></p><p><strong>第一步：给定样例：</strong></p><p>给定数据集 $D = \{(\boldsymbol x_i, y_i)\}_{i = 1}^{m},  y_i \in \{0, 1\}$, 令 $\boldsymbol X_j$、$\boldsymbol \mu_j$、$\boldsymbol \sum_j$ 分别表示第 $j \in \{0, 1\}$ 类示例的集合、均值向量、协方差矩阵。将数据投影到直线 $\boldsymbol w$ 上，则两类样本的 <strong>样本中心</strong> 在直线上的投影分别为 $\boldsymbol w^T \boldsymbol \mu_0$ 和$\boldsymbol w^T \boldsymbol \mu_1$，两类样本的协方差矩阵分别为 $\boldsymbol w^T \boldsymbol \sum_0 \boldsymbol w$ 和 $\boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$，由于直线是一维空间，因此 $\boldsymbol w^T \boldsymbol \mu_0$ 、$\boldsymbol w^T \boldsymbol \mu_1$、 $\boldsymbol w^T \boldsymbol \sum_0 \boldsymbol w$ 和、$\boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$均为实数。</p><font color="#0099ff">顺便一提，如上图，横轴坐标分别为 x1 和 x2，代表样本的两个属性，此图代表属性个数为2时张成的二维空间。但当属性个数为n时，属性空间也为n维，只不过无法在图中体现了。 </font><p><strong>第二步：寻找投影直线：</strong></p><p><strong>两个原则：</strong> </p><ul><li><strong>同类近：</strong> 欲使得同类投影点尽可能近，可以让异类样例投影点的协方差尽可能小，即 $\boldsymbol w^T \boldsymbol \sum _0 \boldsymbol w + \boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$ 尽可能小</li><li><strong>异类远：</strong> 欲使得异类投影点尽可能远离，可以让类中心之间的距离尽可能大，即 $||\boldsymbol w^T \boldsymbol \mu_0 - \boldsymbol w^T \boldsymbol \mu_1||_2^2$ 尽可能大</li></ul><p><strong>广义瑞利商：</strong> 同时考虑二者，得到最大化目标，即：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}J &= \frac{||\boldsymbol w^T \boldsymbol \mu_0 - \boldsymbol w^T \boldsymbol \mu_1||_2^2}{\boldsymbol w^T \boldsymbol \sum _0 \boldsymbol w + \boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w}\\&= \frac{\boldsymbol w^T(\boldsymbol \mu_0 - \boldsymbol \mu_1)(\boldsymbol \mu_0 - \boldsymbol \mu_1)^T\boldsymbol w}{\boldsymbol w^T (\boldsymbol \sum_0 + \boldsymbol \sum_1) \boldsymbol w}\end {split}\end {equation}</script><p><strong>类内散度矩阵：</strong> </p><script type="math/tex; mode=display">\begin {equation}\begin {split}\boldsymbol S_w &= \boldsymbol \sum _0 + \boldsymbol \sum _1 \\& = \boldsymbol \sum_{x \in X_0}(\boldsymbol x - \boldsymbol \mu_0)(\boldsymbol x - \boldsymbol \mu_0)^T + \boldsymbol \sum_{x \in X_1}(\boldsymbol x - \boldsymbol \mu_1)(\boldsymbol x - \boldsymbol \mu_1)^T\end {split}\end {equation}</script><p><strong>类间散度矩阵：</strong> </p><script type="math/tex; mode=display">\boldsymbol S_b = (\boldsymbol \mu _0 - \boldsymbol \mu_1) (\boldsymbol \mu _0 - \boldsymbol \mu_1)^T</script><p><strong>重写得：</strong></p><script type="math/tex; mode=display">J = \frac{\boldsymbol w^T \boldsymbol S_b \boldsymbol w}{\boldsymbol w^T \boldsymbol S_w \boldsymbol w}</script><p><strong>确定 $\boldsymbol  w$ :</strong></p><p>$\boldsymbol J$ 的分子分母都是关于 $\boldsymbol w$ 的二次项，因此  $\boldsymbol J$  的解与  $\boldsymbol w$  的长度无关，只与其方向有关。</p><p>故由拉格朗日乘子法，可列 $\boldsymbol S_b \boldsymbol w = \lambda \boldsymbol S_w \boldsymbol w $，又由于 $\boldsymbol S_b \boldsymbol w$ 方向恒为 $(\boldsymbol \mu _0 - \boldsymbol \mu_1) $，令 $\boldsymbol S_b \boldsymbol w=\lambda(\boldsymbol \mu _0 - \boldsymbol \mu_1) $，带入得 $\boldsymbol w = \boldsymbol S _w ^{-1}(\boldsymbol \mu _0 - \boldsymbol \mu_1) $</p><h4 id="2-多分类任务中的LDA"><a href="#2-多分类任务中的LDA" class="headerlink" title="2. 多分类任务中的LDA"></a>2. 多分类任务中的LDA</h4><hr><h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p><strong>多分类学习的两个思路：</strong> </p><ul><li><strong>一是将二分类学习方法直接推广到多分类</strong>，如LDA</li><li><strong>二是基于某些策略，利用二分类学习器来解决多分类问题</strong></li></ul><h4 id="拆解法和拆分策略"><a href="#拆解法和拆分策略" class="headerlink" title="拆解法和拆分策略"></a>拆解法和拆分策略</h4><p><strong>拆解法：</strong> 将多分类任务拆解成为若干个二分类任务求解</p><p><strong>拆解步骤：</strong> </p><ul><li>通过拆分策略对问题进行<strong>【拆分】</strong></li><li>为拆分出的每个二分类任务<strong>【训练】</strong>一个分类器</li><li>对各个分类器的结果进行<strong>【集成】</strong>，以获得多分类结果</li></ul><p><strong>拆分策略：</strong></p><ul><li>“一对一 (OvO)”</li><li>“一对其余 (OvR)”</li><li>“多对多 (MvM)”</li></ul><p>假设多分类学习有 $\boldsymbol N$ 个类别 $\boldsymbol C_1, \boldsymbol C_2, … \boldsymbol C_N$, 给定数据集 $ D = \{(\boldsymbol x_1, y_1),(\boldsymbol x_2, y_2),…,(\boldsymbol x_m, y_m)\}$, $y_i \in \{C_1, C_2, … ,C_n\}$ </p><h5 id="OvO"><a href="#OvO" class="headerlink" title="OvO:"></a>OvO:</h5><ul><li>将 $N$ 个分类分别两两配对，从而 <strong>【拆分】</strong> 成 $N(N-1)/2$ 个二分类任务</li><li><strong>【训练】</strong> 时为了区分 $C_i$ 和 $C_j$ 这两个分类，这 $N(N-1)/2$ 个分类器的一个将 $C_i$ 作为正例，$C_j$ 作为反例</li><li>测试时将新样本同时提交给所有分类器，将得到 $N(N-1)/2$ 个分类结果，<strong>【集成】</strong> 的方法是通过投票在这些结果中选出最终结果</li></ul><h5 id="OvR"><a href="#OvR" class="headerlink" title="OvR:"></a>OvR:</h5><ul><li>将 $N$ 个分类中的1个分类拿出来作为一个分类器的正例，其余均设置为反例，从而<strong>【拆分】</strong>成 $N$ 个分类任务</li><li><strong>【训练】</strong> 得到 $N$ 个分类器</li><li><strong>【集成】</strong> 的方法是考虑各被判为正例的分类器的置信度，选择置信度大的类别标记作为分类的结果</li></ul><p><img src="https://pic3.zhimg.com/80/v2-437db32129fdc1d411ac560bfa0f8842_hd.jpg" alt="img"></p><h5 id="MvM"><a href="#MvM" class="headerlink" title="MvM:"></a>MvM:</h5><p>MvM是OvO和OvR的一般形式，反过来说，OvO和OvR是MvM的特例。</p><p>MvM每次将若干个类作为正类，若干个其他类作为反类。但其构造必须有特殊的设计，不能随意选取。常用的一种MvM技术是“纠错输出码”（ECOC）技术。</p><h4 id="“纠错输出码”-ECOC-技术"><a href="#“纠错输出码”-ECOC-技术" class="headerlink" title="“纠错输出码” (ECOC) 技术"></a>“纠错输出码” (ECOC) 技术</h4><p><strong>ECOC过程主要分为两步：</strong></p><ul><li><strong>编码：</strong>对 $N$ 个类进行 $M$ 次划分，产生 $M$ 个分类器</li><li><strong>解码：</strong>$M$ 个分类器对测试样本进行预测，得到 $M$ 个预测标记。将其组成编码；这个编码与 $N$ 个类别各自的编码进行比较，返回其中距离较小的类别作为最终预测的结果</li></ul><p><strong>编码形式：</strong></p><ul><li><strong>二元码：</strong> “正类”、“反类”</li><li><strong>三元码：</strong> “正类”、“反类”、“停用类”</li></ul><p>以二元 ECOC 码为例：如下图，首先，将 $N (N = 4)$ 个类提供设计构造成 $M (M = 5)$ 个分类器 $(f_1, f_2,f_3,f_4,f_5)$, 每个分类器为每个类分别配了一个标记结果 (-1或+1)，从而，每一个类 $C_i, i\in \{1, N\}$ 都获得了一个 $M$ 位的编码，这个编码就是 <strong>【各类所对应的编码】</strong></p><p>当有一个测试示例 $A$ 时，先将 $A$ 依照次序放入 $M$ 个分类器中，得到了 $M$ 个分类标记结果 $(-1,-1,+1,-1,+1);$ 再将这 $M$ 个标记结果编成一个纠错输出码 $(-1-1+1-1+1);$ 最后去和<strong>【各类所对应的编码】</strong>进行比较海明距离或欧式距离，距离最短的编码对应的分类就是结果。</p><p><img src="https://pic1.zhimg.com/80/v2-2c27bcec29d2a98b0a4725775cb8b2c4_hd.jpg" alt="img"></p><hr><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p><strong>类别不平衡(class-imbalance)</strong> 就是指分类任务中不同类别的训练样例数目差别很大的情况</p><p>以二分类问题为例，该问题一般指的是训练集中正负样本数比例相差过大（比如正例9998个，负例2个），其一般会造成以下的一些情况：</p><ul><li>类别少的误判惩罚过低，导致有所偏袒，当样本不确定时倾向于把样本分类为多数类。</li><li>样本数量分布很不平衡时，特征的分布同样会不平衡。</li><li>传统的评价指标变得不可靠，例如准确率。</li></ul><p>而在多分类问题中，尽管原始训练集中可能不同类别训练样本数目相当，通过OvR、MvM进行拆分时也有可能会造成上述情况，所以类别不平衡问题亟待解决。</p><p><strong>再缩放：</strong> 解决类别不平衡问题一个最基本思路是 “再缩放”，即当正反例数目不同时，令 $m^+$ 表示正例数目，$m^-$ 表示反例数目，则：</p><script type="math/tex; mode=display">几率 = \frac{y\prime}{1-y\prime} = \frac{y}{1-y}\times\frac{m^-}{m^+}</script><font color="#0099ff">再缩放的思想虽然简单，但实际操作却不平凡，主要因为“训练集是真实样本总体的无偏采样”这个假设往往不成立，即我们未必能基于训练集的观察几率来推断真实几率</font><p><strong>现有技术大体上有三类做法：</strong></p><ul><li>第一类是对训练集里的反类样例进行<strong>欠采样</strong>，即去除一些反例使得正、反例数目接近</li><li>第二类是对训练集里的正类样例进行<strong>过采样</strong>，即增加一些正例使得正、反例数目接近</li><li>第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上式嵌入到其决策过程中，称为<strong>阈值移动</strong></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（三）线性模型&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（三）线性模型&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（三）线性模型&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（三）线性模型&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application</title>
    <link href="http://sunfeng.online/2019/08/08/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E5%9B%9B%E5%91%A8(Deep%20Neural%20Networks)%E2%80%94%E2%80%94Programming%20Assignments%205%E3%80%81Deep%20Neural%20Network%20Application/"/>
    <id>http://sunfeng.online/2019/08/08/课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application/</id>
    <published>2019-08-08T09:41:25.000Z</published>
    <updated>2019-08-30T08:43:46.732Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Deep-Neural-Network-for-Image-Classification-Application"><a href="#Deep-Neural-Network-for-Image-Classification-Application" class="headerlink" title="Deep Neural Network for Image Classification Application"></a>Deep Neural Network for Image Classification Application</h3><a id="more"></a><p>When you finish this, you will have finished the last programming assignment of Week 4, and also the last programming assignment of this course!</p><p>You will use the functions you had implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification. Hopefully, you will see an improvement in accuracy relative to your previous logistic regression implementation.</p><p><strong>After this assignment you will be able to:</strong></p><ul><li>Build and apply a deep neural network to supervised learning.</li></ul><p>Let’s get started!</p><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://www.numpy.org.cn/" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/dnn_app_utils_v2.py" target="_blank" rel="noopener">dnn_app_utils</a> provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage </span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>You will use the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network “  (Assignment 2). The model you had built had 70% test accuracy on classifying cats vs non-cats images. Hopefully, your new model will perform a better!</p><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- a training set of m_train images labelled <span class="keyword">as</span> cat (<span class="number">1</span>) <span class="keyword">or</span> non-cat (<span class="number">0</span>)</span><br><span class="line">- a test set of m_test images labelled <span class="keyword">as</span> cat <span class="keyword">and</span> non-cat</span><br><span class="line">- each image <span class="keyword">is</span> of shape (num_px, num_px, <span class="number">3</span>) where <span class="number">3</span> <span class="keyword">is</span> <span class="keyword">for</span> the <span class="number">3</span> channels (RGB).</span><br></pre></td></tr></table></figure><p>Let’s get more familiar with the dataset. Load the data by running the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure><p>The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">7</span></span><br><span class="line">plt.imshow(train_x_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_y[<span class="number">0</span>,index]) + <span class="string">". It's a "</span> + classes[train_y[<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">" picture."</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p>y = 1. It’s a cat picture.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128202109097-1894133180.png" alt="img"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Explore your dataset </span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>] <span class="comment"># 行数</span></span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]  <span class="comment"># 列数</span></span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]  <span class="comment"># 行数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: <span class="number">209</span></span><br><span class="line">Number of testing examples: <span class="number">50</span></span><br><span class="line">Each image <span class="keyword">is</span> of size: (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_x_orig shape: (<span class="number">209</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_y shape: (<span class="number">1</span>, <span class="number">209</span>)</span><br><span class="line">test_x_orig shape: (<span class="number">50</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">test_y shape: (<span class="number">1</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128202810081-667256240.png" alt="img"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples </span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T   <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment">#  "-1" 使得剩下的维度变为1维</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_x<span class="string">'s shape: (12288, 209)</span></span><br><span class="line"><span class="string">test_x'</span>s shape: (<span class="number">12288</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>12,288 equals 64×64×3 which is the size of one reshaped image vector.</p><p>Then, integrate the above code into the function <code>dataset_preprocess()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataset_preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementing Data Set Preprocessing</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_x: your training set features</span></span><br><span class="line"><span class="string">        train_y: your training set labels</span></span><br><span class="line"><span class="string">        test_x: your test set features</span></span><br><span class="line"><span class="string">        test_y: your test set labels</span></span><br><span class="line"><span class="string">        classes: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># load data set</span></span><br><span class="line">    train_x_orig,  train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape the  training and test examples</span></span><br><span class="line">    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T <span class="comment"># 参数-1使得剩下的维度变为1维</span></span><br><span class="line">    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Standardize data to have feature values between 0 and 1</span></span><br><span class="line">    train_x = train_x_flatten / <span class="number">255</span></span><br><span class="line">    test_x = test_x_flatten / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_x, train_y, test_x, test_y, classes</span><br></pre></td></tr></table></figure><hr><h3 id="Architecture-of-your-model"><a href="#Architecture-of-your-model" class="headerlink" title="Architecture of your model"></a>Architecture of your model</h3><p>Now that you are familiar with the dataset, it is time to build a deep neural network distinguish cat images from non-cat images.</p><p>You will build two different models:</p><ul><li>A 2-layer neural network.</li><li>An L-layer neural network.</li></ul><p>You will then compare the performance of  these models, and also try out different values for L.</p><h4 id="2-layer-neural-network"><a href="#2-layer-neural-network" class="headerlink" title="2-layer neural network"></a>2-layer neural network</h4><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128203731956-1246182495.png" alt="img"></p><p>The model can be summarized as : <strong><em>INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.</em></strong> </p><p><strong>Detailed Architecture of figure 2:</strong></p><ul><li><p>The input is a (64, 64, 3) image which  is flatten to a vector of size (12288, 1)</p></li><li><p>The corresponding vector:  ${[x_0, x_1, …… ,x_{12287} ]}^T$ is then multiplied by the weight matrix $W^{[1]}$ of size ($n^{[1]}$, 12288).</p></li><li><p>You then add a bias term and take its relu to get following vector:</p><p> ${[a_0^{[1]}, a_1^{[1]}, …… ,a_{n^{[1]}-1} ^{[1]}]}^T$</p></li><li><p>You the repeat the same process.</p></li><li><p>You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias).</p></li><li><p>Finally, you take the sigmoid of the result. If it is greater than 0.5, you classify it to be a cat.</p></li></ul><h4 id="L-layer-neural-network"><a href="#L-layer-neural-network" class="headerlink" title="L-layer neural network"></a>L-layer neural network</h4><p>It is hard to represent an L-layer deep neural network with the above representation. However, here is a simplified network representation:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128204442487-1691015986.png" alt="img"></p><p>The model can be summarized as：<strong><em>[ LINEAR -&gt; RELU ] $ \times $ (L-1) -&gt; LINEAR -&gt;SIGMOID</em></strong> </p><p><strong>Detailed Architecture of figure 3:</strong></p><ul><li><p>The input is a (64, 64, 3) image which is flattened to a vector of size (12288, 1).</p></li><li><p>The corresponding vector: ${[x_0, x_1, …… ,x_{12287} ]}^T$ is the multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[1]}$. The result is called the linear unit.</p></li><li><p>Next, you take the relu of the linear unit. This process could be repeated several times for each ($W^{[l]}, b^{[l]}$) depending on the model architecture.</p></li><li><p>Finally, you take the sigmoid of the final linear unit. If it is a greater than 0.5, you classify it to be a cat.</p></li></ul><h4 id="General-methodology"><a href="#General-methodology" class="headerlink" title="General methodology"></a>General methodology</h4><p>As usual you will follow the Deep Learning methodology to build the model:</p><ol><li>Initialize parameters / Define hyperparameters</li><li>Loop for num_iterations:<ul><li>Forward propagation</li><li>Compute cost function</li><li>Backward propagation</li><li>Update parameters</li></ul></li><li>Use trained parameters to predict labels </li></ol><hr><h3 id="Two-layer-neural-network"><a href="#Two-layer-neural-network" class="headerlink" title="Two - layer neural network"></a>Two - layer neural network</h3><p><strong>Exercise:</strong> Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: <em>LINEAR -&gt; RELU -&gt;LINEAR -&gt;SIGMOID</em>. The functions you may need and their inputs are:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Two-layer neural network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">        num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        print_cost -- If set to True, this will print the cost every 100 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one </span></span><br><span class="line">    <span class="comment"># of the functions you'd previously implemented</span></span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get W1, b1, W2, and b2</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR-&gt; SIGMOID</span></span><br><span class="line">        <span class="comment"># Inputs: "X, W1, b1"</span></span><br><span class="line">        <span class="comment"># Outputs: "A1, cache1, A2, cache2"</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, activation = <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = -(np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        <span class="comment"># Inputs: dA2, cache2, cache1</span></span><br><span class="line">        <span class="comment"># Outputs: dA1, dW2, db2, dA0(not used), dW1, db1</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = <span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Run the cell below to train your parameters. See if your model runs. The cost should be decreasing. It may take up to 5 minutes to run 2500 iterations.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693049735659989</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.6464320953428849</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.6325140647912678</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.6015024920354665</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.5601966311605748</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.515830477276473</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.4754901313943325</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.43391631512257495</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.4007977536203886</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.35807050113237987</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.3394281538366413</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.30527536361962654</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.2749137728213015</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.24681768210614827</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.1985073503746611</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.17448318112556593</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.1708076297809661</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.11306524562164737</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.09629426845937163</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.08342617959726878</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.0743907870431909</span></span><br><span class="line">Cost after iteration <span class="number">2100</span>: <span class="number">0.06630748132267938</span></span><br><span class="line">Cost after iteration <span class="number">2200</span>: <span class="number">0.05919329501038176</span></span><br><span class="line">Cost after iteration <span class="number">2300</span>: <span class="number">0.05336140348560564</span></span><br><span class="line">Cost after iteration <span class="number">2400</span>: <span class="number">0.048554785628770226</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128213017144-1851629766.png" alt="img"></p><p>Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.</p><p>Now, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 1.0</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.72</span><br></pre></td></tr></table></figure><p><strong>Note:</strong> You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting.</p><p>Congratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an LL-layer model.</p><hr><h3 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h3><p><strong>Exercise:</strong>  Use the helper functions you have implemented previously to build an LL-layer neural network with the following structure: <em>[LINEAR -&gt; RELU]×(L-1) -&gt; LINEAR -&gt; SIGMOID</em>. The functions you may need and their inputs are:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameter</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L-layer Neural Network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR-&gt;SIGMOID</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        layers_dims -- list containing the input size and each layer size, of length (number of layers + 1)</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- parameters learnt by the model. They can then be used to predict</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters initialization</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU] * (L-1) -&gt; LINEAR -&gt;SIGMOID</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125; : &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations(per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>You will now train the model as a 5-layer neural network.</p><p>Run the cell below to train your model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.771749</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.672053</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.648263</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.611507</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.567047</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.540138</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.527930</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.465477</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.369126</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.391747</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.315187</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.272700</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.237419</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.199601</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.189263</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.161189</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.148214</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.137775</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.129740</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.121225</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.113821</span></span><br><span class="line">Cost after iteration <span class="number">2100</span>: <span class="number">0.107839</span></span><br><span class="line">Cost after iteration <span class="number">2200</span>: <span class="number">0.102855</span></span><br><span class="line">Cost after iteration <span class="number">2300</span>: <span class="number">0.100897</span></span><br><span class="line">Cost after iteration <span class="number">2400</span>: <span class="number">0.092878</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128215013940-1660492466.png" alt="img"></p><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result: </strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.985645933014</span><br></pre></td></tr></table></figure><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result: </strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.8</span><br></pre></td></tr></table></figure><p>Congrats! It seems that your 5-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.</p><p>This is a good performance for this task. Nice job!</p><p>Though in the next course on “Improving deep neural networks” you will learn how to obtain even higher accuracy by systematically searching for better hyperparameters (learning_rate, layers_dims, num_iterations, and others you’ll also learn in the next course).</p><hr><h3 id="Result-Analysis"><a href="#Result-Analysis" class="headerlink" title="Result Analysis"></a>Result Analysis</h3><p>First, let’s take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128234807019-678540948.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128234830378-2132257659.png" alt="img"></p><p><strong>A few type of images the model tends to do poorly on include:</strong></p><ul><li>Cat body in an unusual position.</li><li>Cat appears against a background of  a similar color.</li><li>Unusual cat color and species.</li><li>Camera Angle.</li><li>Brightness of the picture.</li><li>Scale variation (cat is very large or small in image)</li></ul><hr><h3 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network" target="_blank" rel="noopener">Source Code</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Deep-Neural-Network-for-Image-Classification-Application&quot;&gt;&lt;a href=&quot;#Deep-Neural-Network-for-Image-Classification-Application&quot; class=&quot;headerlink&quot; title=&quot;Deep Neural Network for Image Classification Application&quot;&gt;&lt;/a&gt;Deep Neural Network for Image Classification Application&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step</title>
    <link href="http://sunfeng.online/2019/08/07/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E5%9B%9B%E5%91%A8(Deep%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%204%E3%80%81Building%20your%20Deep%20Neural%20Network,%20step%20by%20step/"/>
    <id>http://sunfeng.online/2019/08/07/课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step/</id>
    <published>2019-08-07T07:58:04.000Z</published>
    <updated>2019-08-30T08:43:26.849Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h3><a id="more"></a><p>Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a 2-layer Neural Network (with a signal hidden layer). This week, you will build a deep neural network, with as many layers as you want!</p><ul><li>In this notebook, you will implement all the functions required to build a deep neural network.</li><li>In the next assignment, you will use these functions to build a deep neural network for image classification.</li></ul><p><strong>After this assignment you will be able to:</strong></p><ul><li>Use non-linear units like ReLU to improve your model</li><li>Build a deeper neural network (with more than 1 hidden layer)</li><li>Implement an easy-to-use neural network class</li></ul><p><strong>Notation:</strong></p><ul><li>Superscript [I] denotes a quantity associated with the $1^{th}$ layer.<ul><li>Example: $a^{[L]}$is the $L^{th}$ layer activation. $W^{[L]}$ and  $b^{[L]}$ are the $L^{th}$  layer parameters.</li></ul></li><li>Superscript (i) denotes a quantity associated with the $i^{th}$ example.<ul><li>Example: $x^{(i)}$ is the $i^{th}$ training example.</li></ul></li><li>Lowerscript i denotes the $i^{th}$ entry of a vector.<ul><li>Example: $a_i^{[I]}$ denotes the $i^{th}$ entry of the $I^{th}$ layer’s activations.</li></ul></li></ul><p><strong>Let’s get started!</strong></p><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://numpy.org/" target="_blank" rel="noopener">numpy</a> is the main packages for scientific computing with Python.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/dnn_utils_v2.py" target="_blank" rel="noopener">dnn_utils</a> provides some necessary functions for this notebook.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/testCases_v2.py" target="_blank" rel="noopener">testCases</a> provides some test cases to assess the correctness of your functions.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="Outline-of-the-Assignment"><a href="#Outline-of-the-Assignment" class="headerlink" title="Outline of the Assignment"></a>Outline of the Assignment</h3><p>To build your neural network, you will be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of the assignment, you will:</p><ul><li>Initialize the parameters for a two-layer network and for an L-layer neural network.</li><li>Implement the forward propagation module (shown in purple  in the figure below).<ul><li>Complete the LINEAR part of a layer’s forward propagation step (resulting in $Z^{l}$).</li><li>We give you the ACTIVATION function (relu / sigmoid).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li><li>Stack the [LINEAR-&gt;RELU] forward function L-1 times (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li></ul></li><li>Compute the loss.</li><li>Implement the backward propagation module (denoted in red in the figure below).<ul><li>Complete the LINEAR part of a layer’s backward propagation step.</li><li>We give you the gradient of the ACTIVATION function (relu_backward / sigmoid_backward).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li><li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li></ul></li><li>Finally update the parameters.</li></ul><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127165838722-72534730.png" alt="img"></p><p><strong>Note</strong> that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backward propagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps. </p><hr><h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>You will write two helper functions that will initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one will generalize this initialization process to L layers.</p><h4 id="2-layer-Neural-Network"><a href="#2-layer-Neural-Network" class="headerlink" title="2-layer Neural Network"></a>2-layer Neural Network</h4><p><strong>Exercise:</strong> Create and initialize the parameters of the 2-layer neural network.</p><p><strong>Instructions:</strong> </p><ul><li>The model’s structure is: <em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID​.</em></li><li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) *0.01</code> with the correct shape.</li><li>Use zero initialize for the biases. Use <code>np.zeros(shape)</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 2-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize parameters for a two-layer network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @n_x: size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01624345</span> <span class="number">-0.00611756</span> <span class="number">-0.00528172</span>]</span><br><span class="line"> [<span class="number">-0.01072969</span>  <span class="number">0.00865408</span> <span class="number">-0.02301539</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[ <span class="number">0.01744812</span> <span class="number">-0.00761207</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h4 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h4><p>The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the <code>initialize_parameters_deep</code>, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units of layer l.</p><p>Thus for example if the size of our input X is (12288, 209) (with m = 209 examples) then:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200241753-1787058979.png" alt="img"></p><p>Remember that when we compute $WX + b$ in Python, it carries out broadcasting. For example, if:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200327409-197353303.png" alt="img"></p><p>The $WX + b$ will be:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200424847-714067061.png" alt="img"></p><p><strong>Exercise:</strong> Implement initialization for an L-layer Neural Network.</p><p><strong>Instructions:</strong></p><ul><li>The model’s structure is <em>[LINEAR -&gt; RELU]</em> $\times$ <em>(L-1) -&gt; LINEAR -&gt; SIGMOID.</em> It has <em>L-1</em> layers using a ReLU activation function followed by an output layer with a sigmoid activation function.</li><li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) * 0.01</code>.</li><li>We will store $n^{[l]}$, the number of units in different layers, in a variables <code>layer_dims</code>. For example, the <code>layer_dims</code> for the “Planar Data classification model” from last week would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. Thus means <code>W1</code>‘s shape was (4,2), <code>b1</code> was (4,1), <code>W2</code> was (1,4) and <code>b2</code> was (1,1). Now you will generalize this to LL layers!</li><li>Here is the implementation for L = 1 (one layer neural network). It should inspire you to implement the general case (L-layer neural network).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> L == <span class="number">1</span>:</span><br><span class="line">      parameters[<span class="string">"W"</span> + str(L)] = np.random.randn(layer_dims[<span class="number">1</span>], layer_dims[<span class="number">0</span>]) * <span class="number">0.01</span></span><br><span class="line">      parameters[<span class="string">"b"</span> + str(L)] = np.zeros((layer_dims[<span class="number">1</span>], <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. L-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initailize parameters for an L-layer neural network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @layer_dims: python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters "W1", "b1",...</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (layer_dims[1], 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)     <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># Random Initialization</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01788628</span>  <span class="number">0.0043651</span>   <span class="number">0.00096497</span> <span class="number">-0.01863493</span> <span class="number">-0.00277388</span>]</span><br><span class="line"> [<span class="number">-0.00354759</span> <span class="number">-0.00082741</span> <span class="number">-0.00627001</span> <span class="number">-0.00043818</span> <span class="number">-0.00477218</span>]</span><br><span class="line"> [<span class="number">-0.01313865</span>  <span class="number">0.00884622</span>  <span class="number">0.00881318</span>  <span class="number">0.01709573</span>  <span class="number">0.00050034</span>]</span><br><span class="line"> [<span class="number">-0.00404677</span> <span class="number">-0.0054536</span>  <span class="number">-0.01546477</span>  <span class="number">0.00982367</span> <span class="number">-0.01101068</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.01185047</span> <span class="number">-0.0020565</span>   <span class="number">0.01486148</span>  <span class="number">0.00236716</span>]</span><br><span class="line"> [<span class="number">-0.01023785</span> <span class="number">-0.00712993</span>  <span class="number">0.00625245</span> <span class="number">-0.00160513</span>]</span><br><span class="line"> [<span class="number">-0.00768836</span> <span class="number">-0.00230031</span>  <span class="number">0.00745056</span>  <span class="number">0.01976111</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><hr><h3 id="Forward-propagation-module"><a href="#Forward-propagation-module" class="headerlink" title="Forward propagation module"></a>Forward propagation module</h3><h4 id="Linear-Forward"><a href="#Linear-Forward" class="headerlink" title="Linear Forward"></a>Linear Forward</h4><p>Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:</p><ul><li>LINEAR</li><li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li><li>[LINEAR -&gt; RELU] $\times$ (L-1) -&gt; LINEAR -&gt;SIGMOID (whole model)</li></ul><p>The linear forward module (vectorized over all the examples) computes the following equations:</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}</script><p>where $A^{[0]} = X$.</p><p><strong>Exercise:</strong> Build the linear part of forward propagation.</p><p><strong>Reminder:</strong> The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. You may also find <code>np.dot()</code> useful. If your dimensions don’t match, printing <code>W.shape</code> may help.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @z: the input of activation function, also called pre-activation parameter</span></span><br><span class="line"><span class="string">        @cache: a python dictionary containing "A", "W", and "b"; stored for computing the backword pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A, W, b = linear_forward_test_case()</span><br><span class="line"></span><br><span class="line">Z, linear_cache = linear_forward(A, W, b)</span><br><span class="line">print(<span class="string">"Z = "</span> + str(Z))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = [[ <span class="number">3.26295337</span> <span class="number">-1.23429987</span>]]</span><br></pre></td></tr></table></figure><h4 id="Linear-Activation-Forward"><a href="#Linear-Activation-Forward" class="headerlink" title="Linear-Activation Forward"></a>Linear-Activation Forward</h4><p>In this notebook, you will use two activation functions:</p><ul><li><p><strong>Sigmoid:</strong> $\sigma(Z) = \sigma(WA + b) = \frac{1}{1 + e^{-(WA + b)}}$. We have provided you with the <code>sigmoid</code> function. This function returns two items: the activation value <code>&quot;a&quot;</code> and a <code>&quot;cache&quot;</code> that contains <code>&quot;Z&quot;</code> (it’s what we feed in to the corresponding backward function). To use it you could just call:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = sigmoid(Z)</span><br></pre></td></tr></table></figure></li><li><p><strong>ReLU:</strong> The mathematical formula for ReLu is $A = ReLU(Z) = max(0, Z)$. We have provided you with the <code>relu</code> function. This function returns <strong>two</strong> items: the activation value “<code>A</code>“ and a “<code>cache</code>“ that contains “<code>Z</code>“ (it’s what we will feed in to the corresponding backward function). To use it you could just call:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = relu(Z)</span><br></pre></td></tr></table></figure></li></ul><p>For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.</p><p><strong>Exercise:</strong> Implement the forward propagation of the <em>LINEAR -&gt; ACTIVATION</em> layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$ where the activation <code>&quot;g&quot;</code> can be <code>sigmoid()</code> or <code>relu()</code>. Use <code>linear_forward()</code> and the correct activation function.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A_prev: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weight matrix: numpy array of shape (size pf current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector: numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">        @activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        @A: the output of the activation function, also called the post-activation value</span></span><br><span class="line"><span class="string">        @cache: a python tuple containing "linear_cache" and "activation_cache"; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A_prev, W, b = linear_activation_forward_test_case()</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A))</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>)</span><br><span class="line">print(<span class="string">"With ReLU: A = "</span> + str(A))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">With sigmoid: A = [[ <span class="number">0.96890023</span>  <span class="number">0.11013289</span>]]</span><br><span class="line">With ReLU: A = [[ <span class="number">3.43896131</span>  <span class="number">0.</span>        ]]</span><br></pre></td></tr></table></figure><p><strong>Note:</strong> In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.</p><h4 id="L-Layer-Model"><a href="#L-Layer-Model" class="headerlink" title="L-Layer Model"></a>L-Layer Model</h4><p>For even convenience when implementing the L-layer Neural Network, you will need a function that replicates the previous one <code>linear_activation_forward</code> with ReLU L-1 times, then follows that with one <code>linear_activation_forward</code>with SIGMOID.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127213245862-62967896.png" alt="img"></p><p><strong>Exercise:</strong> Implement the forward propagation of the above model. </p><p><strong>Instruction:</strong> In the code below, the variable AL will denote </p><p>$A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]}A^{[L-1]} + b^{[L]})$.</p><p><strong>Tips:</strong> </p><ul><li>Use the functions you had previously written.</li><li>Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times</li><li>Don’t forget to keep track of the caches in the “caches” list. To add a new value <code>c</code> to a <code>list</code>, you can use <code>list.append(c)</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L-layer Model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1) -&gt; [LINEAR-&gt;SIGMOID] computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @AL -- last post-activation value</span></span><br><span class="line"><span class="string">        @caches -- list of caches containing:</span></span><br><span class="line"><span class="string">            every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">            the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU] * (L-1).</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line"></span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], activation = <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; SIGMOID]</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, parameters = L_model_forward_test_case_2hidden()</span><br><span class="line">AL, caches = L_model_forward(X, parameters)</span><br><span class="line">print(<span class="string">"AL = "</span> + str(AL))</span><br><span class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AL = [[ <span class="number">0.03921668</span>  <span class="number">0.70498921</span>  <span class="number">0.19734387</span>  <span class="number">0.04728177</span>]]</span><br><span class="line">Length of caches list = <span class="number">3</span></span><br></pre></td></tr></table></figure><p>Great! Now you have a full forward propagation that takes the input X and outputs a row vector AL containing your predictions. It also records all intermediate values in “caches”. Using AL, you can computes the cost of your predictions.</p><hr><h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.</p><p><strong>Exercise:</strong> Compute the cross-entropy cost J, using the following formula:</p><script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}</script><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function (the cross-entropy cost J)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape of (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y</span></span><br><span class="line">    cost = - (np.dot(Y, np.log(AL).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL).T)) / m</span><br><span class="line">    cost = np.squeeze(cost) <span class="comment"># To make sure your cost's shape is waht we expect</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y, AL = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = <span class="number">0.414931599615397</span></span><br></pre></td></tr></table></figure><hr><h3 id="Backward-propagation-module"><a href="#Backward-propagation-module" class="headerlink" title="Backward propagation module"></a>Backward propagation module</h3><p>Just like with forward propagation, you will implement helper functions for backward propagation. Remember that backward propagation is used to calculate the gradient of the loss function with respect to the parameters.</p><p><strong>Reminder:</strong></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127233429440-720852258.png" alt="img"></p><p><em>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</em></p><p>Now, similar to forward propagation, you are going to build the backward propagation in three steps:</p><ul><li>LINEAR backward</li><li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li><li>[LINEAR -&gt; RELU] ×× (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li></ul><h4 id="Linear-backward"><a href="#Linear-backward" class="headerlink" title="Linear backward"></a>Linear backward</h4><p>For layer l, the linear part is: $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$ (followed by an activation). </p><p>Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial J}{\partial Z^{[l]}}$ . You want to get ($dW^{[l]}$, $db^{[l]}$, $dA^{[l]}$).</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235155972-308393091.png" alt="img"></p><p>The three output ($dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$) are computed using the input $dZ^{[l]}$. Here are the formulas you need:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235506737-1259548081.png" alt="img"></p><p><strong>Exercise：</strong> Use the 3 formulas above to implement <code>linear_backward().</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA -- Gradient of the cost with respect to the activation (of the prevous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of the cost with respect to the W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of the cost with respect to the b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.sum(dZ, axis = <span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up some test inputs</span></span><br><span class="line">dZ, linear_cache = linear_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dA_prev = [[ <span class="number">0.51822968</span> <span class="number">-0.19517421</span>]</span><br><span class="line"> [<span class="number">-0.40506361</span>  <span class="number">0.15255393</span>]</span><br><span class="line"> [ <span class="number">2.37496825</span> <span class="number">-0.89445391</span>]]</span><br><span class="line">dW = [[<span class="number">-0.10076895</span>  <span class="number">1.40685096</span>  <span class="number">1.64992505</span>]]</span><br><span class="line">db = [[ <span class="number">0.50629448</span>]]</span><br></pre></td></tr></table></figure><h4 id="Linear-Activation-backward"><a href="#Linear-Activation-backward" class="headerlink" title="Linear-Activation backward"></a>Linear-Activation backward</h4><p>Next, you will create a function that merges the two helper functions: <code>linear_backward</code> and the backward step for the activation <code>linear_activation_backward</code>.</p><p>To help you implement <code>linear_activation_backward</code>, we provided two backward functions:</p><ul><li><p><strong>sigmoid_backward</strong>: Implements the backward propagation for SIGMOID unit. You can call it as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = sigmoid_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure></li><li><p><strong>relu_backward</strong>: Implements the backward propagation for RELU unit. You can call it as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = relu_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure></li></ul><p>If g(.) is the activation function, <code>sigmoid_backward</code> and <code>relu_backward</code> compute:  $dZ^{[l]} = dA^{[l]} * g^{\prime}(Z^{[l]})$ </p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dA -- post-activation gradient for current layer 1</span></span><br><span class="line"><span class="string">        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">        activation -- the activation function to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA_prev -- Gradient of cost with respect to the activation (of the previous layer l - 1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        </span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dA, linear_activation_cache = linear_activation_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sigmoid:</span><br><span class="line">dA_prev = [[ <span class="number">0.11017994</span>  <span class="number">0.01105339</span>]</span><br><span class="line"> [ <span class="number">0.09466817</span>  <span class="number">0.00949723</span>]</span><br><span class="line"> [<span class="number">-0.05743092</span> <span class="number">-0.00576154</span>]]</span><br><span class="line">dW = [[ <span class="number">0.10266786</span>  <span class="number">0.09778551</span> <span class="number">-0.01968084</span>]]</span><br><span class="line">db = [[<span class="number">-0.05729622</span>]]</span><br><span class="line"></span><br><span class="line">relu:</span><br><span class="line">dA_prev = [[ <span class="number">0.44090989</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.37883606</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.2298228</span>   <span class="number">0.</span>        ]]</span><br><span class="line">dW = [[ <span class="number">0.44513824</span>  <span class="number">0.37371418</span> <span class="number">-0.10478989</span>]]</span><br><span class="line">db = [[<span class="number">-0.20837892</span>]]</span><br></pre></td></tr></table></figure><h4 id="L-Model-Backward"><a href="#L-Model-Backward" class="headerlink" title="L-Model Backward"></a>L-Model Backward</h4><p>Now you will implement the backward function for the whole network. Recall that when you implemented the <code>L_model_forward</code> function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the <code>L_model_backward</code> function, you will iterate through all the hidden layers backward, starting from layer L. On each step, you will use the cached values for layer l to backward propagate through layer l. Figure 5 below shows the backward pass.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128142511269-248723414.png" alt="img"></p><p><strong>Initializing backward propagation:</strong>  To implement backward propagate through this network, we know that output is, $A^{[L]} = \sigma(Z^{[L]}) $ . Your code thus need to compute $dAL = \frac{\partial J}{\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don’t need in-depth knowledge of):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br></pre></td></tr></table></figure><p>You can then use this post-activation gradient <code>dAL</code> to keep going backward. As seen in Figure 5, you can now feed in <code>dAL</code> into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a <code>for</code> loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula: grads[“dW” + str(l)] = $dW^{[l]}$</p><p>For example, for l=3 this would store $dW^{[l]}$ in <code>grads[&quot;dW3&quot;]</code>.</p><p><strong>Exercise:</strong> Implement backpropagation for the <em>[LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em> model.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. L-Model Backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backword propagaiton for the [LINEAR-&gt;RELU] * (L - 1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">        caches -- list of caches containing: every cache of linear_activation_forward() with "relu" </span></span><br><span class="line"><span class="string">                  (it's caches[1], for l in range(L - 1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                  the cache of linear_activation_foreward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">                 grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment"># the number of examples</span></span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class="line">    <span class="comment"># Inputs: AL, Y, caches</span></span><br><span class="line">    <span class="comment"># Outputs: grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line"></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients</span></span><br><span class="line">        <span class="comment"># Inputs: grads["dA" + str(l + 2)], caches</span></span><br><span class="line">        <span class="comment"># outputs: grads["dA" + str(l + 1)], grads["dW" + str(l + 1)], grads["db" + str(l + 1)]</span></span><br><span class="line"></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AL, Y_assess, caches = L_model_backward_test_case()</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches)</span><br><span class="line">print_grads(grads)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ <span class="number">0.41010002</span>  <span class="number">0.07807203</span>  <span class="number">0.13798444</span>  <span class="number">0.10502167</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.05283652</span>  <span class="number">0.01005865</span>  <span class="number">0.01777766</span>  <span class="number">0.0135308</span> ]]</span><br><span class="line">db1 = [[<span class="number">-0.22007063</span>]</span><br><span class="line"> [ <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.02835349</span>]]</span><br><span class="line">dA1 = [[ <span class="number">0.12913162</span> <span class="number">-0.44014127</span>]</span><br><span class="line"> [<span class="number">-0.14175655</span>  <span class="number">0.48317296</span>]</span><br><span class="line"> [ <span class="number">0.01663708</span> <span class="number">-0.05670698</span>]]</span><br></pre></td></tr></table></figure><h4 id="Update-Parameters"><a href="#Update-Parameters" class="headerlink" title="Update Parameters"></a>Update Parameters</h4><p>In this section you will update the parameters of the model, using gradient descent:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128164243597-30974158.png" alt="img"></p><p>where $\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.</p><p><strong>Exercise:</strong> Implement <code>update_parameters()</code> to update your parameters using gradient descent.</p><p><strong>Instructions:</strong> Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for l = 1, 2, … L.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Update Parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent updatte rule</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">                      parameters["W" + str(l)] = ...</span></span><br><span class="line"><span class="string">                      parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = parameters[<span class="string">"W"</span> + str(l)] - learning_rate * grads[<span class="string">"dW"</span> + str(l)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = parameters[<span class="string">"b"</span> + str(l)] - learning_rate * grads[<span class="string">"db"</span> + str(l)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[<span class="number">-0.59562069</span> <span class="number">-0.09991781</span> <span class="number">-2.14584584</span>  <span class="number">1.82662008</span>]</span><br><span class="line"> [<span class="number">-1.76569676</span> <span class="number">-0.80627147</span>  <span class="number">0.51115557</span> <span class="number">-1.18258802</span>]</span><br><span class="line"> [<span class="number">-1.0535704</span>  <span class="number">-0.86128581</span>  <span class="number">0.68284052</span>  <span class="number">2.20374577</span>]]</span><br><span class="line">b1 = [[<span class="number">-0.04659241</span>]</span><br><span class="line"> [<span class="number">-1.28888275</span>]</span><br><span class="line"> [ <span class="number">0.53405496</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.55569196</span>  <span class="number">0.0354055</span>   <span class="number">1.32964895</span>]]</span><br><span class="line">b2 = [[<span class="number">-0.84610769</span>]]</span><br></pre></td></tr></table></figure><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Congrats on implementing all the functions required for building a deep neural network!</p><p>We know it was a long assignment but going forward it will only get better. The next part of the assignment is easier.</p><p>In the next assignment you will put all these together to build two models:</p><ul><li>A two-layer neural network</li><li>An L-layer neural network</li></ul><p>You will in fact use these models to classify cat vs non-cat images!</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Building-your-Deep-Neural-Network-Step-by-Step&quot;&gt;&lt;a href=&quot;#Building-your-Deep-Neural-Network-Step-by-Step&quot; class=&quot;headerlink&quot; title=&quot;Building your Deep Neural Network: Step by Step&quot;&gt;&lt;/a&gt;Building your Deep Neural Network: Step by Step&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（二）-- 模型评估与选择</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/</id>
    <published>2019-08-05T12:23:06.000Z</published>
    <updated>2019-08-27T02:48:37.109Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（二）模型评估与选择"><a href="#《机器学习》西瓜书学习笔记（二）模型评估与选择" class="headerlink" title="《机器学习》西瓜书学习笔记（二）模型评估与选择"></a>《机器学习》西瓜书学习笔记（二）模型评估与选择</h2><a id="more"></a><h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><ul><li><strong>错误率 (error rate)：</strong> 分类错误样本数占总样本数的比例</li><li><strong>精度 (accuracy)：</strong> 分类正确样本数占总样本数的比例</li><li><strong>误差 (error)：</strong> 学习器的实际预测输出与真实输出之间的差异</li><li><strong><font color="#0099ff">训练误差 (training error)/经验误差(empirical error)：</font></strong> 学习器在训练集上的误差</li><li><strong>泛化误差(generalization error):</strong> 学习器在新样本上的误差</li><li><strong><font color="#0099ff">过拟合(overfitting):</font></strong> 学习能力过于强大。学习器把训练样本学得太好，导致将训练样本中自身含有的特点当成所有潜在样本都会具有的一般性质，从而训练后使得泛化性能下降</li><li><strong>欠拟合(underfitting):</strong> 学习能力低下，对训练样本的一般性质尚未学好</li></ul><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure1.PNG" alt="Figure1"></p><hr><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p><strong>理想：</strong> 通过评估学习器的<strong>泛化误差</strong>，选出泛化误差最小的学习器</p><p><strong>实际：</strong> <strong>泛化误差</strong> 只能通过测试集上的 <strong>测试误差</strong>  作为近似</p><p><strong><font color="#0099ff">机器学习的目的是产生泛化能力好的模型，那么什么样的模型才是泛化能力好的模型呢? 这需要按照一定的评估方法和度量指标去衡量。</font></strong></p><p>给定一个包含m个样例的数据集 $ D = \{(x_1, y_1), (x_2, y_2), … ,(x_m, y_m)\}$ ,通过对D进行适当的处理，从中产生出训练集S和测试集T，<strong>测试集应该尽可能与训练集互斥</strong>，常见的方法有以下三种：</p><h4 id="1-留出法-hold-out"><a href="#1-留出法-hold-out" class="headerlink" title="1.留出法 (hold-out)"></a>1.留出法 (hold-out)</h4><p><strong>留出法：</strong> 直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个集合作为测试集T，即 <em>D=S</em>∪<em>T</em>，<em>S</em>∩<em>T=</em>∅. </p><p><strong>注意点：</strong></p><ul><li>要保持数据分布的一致性<ul><li>分层采样</li></ul></li><li>采用多次随机划分取均值的评估方法</li><li>训练集的比例应当适当(2/3 ~ 4/5)</li></ul><h4 id="2-交叉验证法-cross-validation"><a href="#2-交叉验证法-cross-validation" class="headerlink" title="2.交叉验证法 (cross validation)"></a>2.交叉验证法 (cross validation)</h4><p><strong>交叉验证法：</strong> 将数据集平均分成 <code>K</code> 份，并尽量保证每份数据分布一致。依次用其中 <code>K - 1</code> 份作为训练集，剩下的一份作为测试集。这样就有 <code>K</code> 组训练/测试集。从而可以进行 <code>K</code> 次训练和测试，返回K次测试结果的均值，也称为”K折交叉验证法”</p><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure2.PNG" alt="Figure2"></p><p><strong>注意点：</strong></p><ul><li>K折交叉验证要随机使用不同的划分重复p次，最终取p次K折交叉验证的均值</li></ul><p><strong>留一法：</strong> 若令K = m, 则称为“留一法 (LOO)”</p><ul><li>优点<ul><li>不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集，即每个子集只含有一个样本</li><li>被实际评估的模型与期望评估的用D训练出的模型很相似，因为使用的训练集与初始数据集相比只少了一个样本</li></ul></li><li>缺点<ul><li>当数据集比较大时，训练m个模型的计算开销比较大</li></ul></li></ul><h4 id="3-自助法-bootstrapping"><a href="#3-自助法-bootstrapping" class="headerlink" title="3.自助法 (bootstrapping)"></a>3.自助法 (bootstrapping)</h4><p><strong>自助法：</strong>  给定包含m个样本的数据集 D，从 D 中进行有放回地采样产生包含 m 个样本的数据集 <em>D’</em>，这样 D 中大概有36.8%的样本不会出现在 <em>D’</em> 中，<strong>将 <em>D’</em> 用作训练集，D - <em>D’</em>  用作测试集 (即在<em>D’</em> 中没出现的样本) </strong></p><p><strong>优点：</strong></p><ul><li>实际评估模型和期望评估模型都使用 m 个训练样本</li><li>保证了仍有数据总量约1/3的、没在训练集中出现的样本用于测试</li><li><font color="#0099ff">自助法在数据量较小，难以有效划分训练集/测试集时很有用</font></li></ul><p><strong>缺点：</strong></p><ul><li><font color="#0099ff">自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差</font></li></ul><hr><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p><strong>性能度量(performance measure)：</strong> 衡量模型泛化能力的评价标准</p><ul><li>回归任务<ul><li>均方误差</li></ul></li><li>分类任务<ul><li>错误率和精度</li><li>查准率、查全率和F1</li><li>ROC和AUC</li><li>代价敏感错误率和代价曲线</li></ul></li></ul><h4 id="1-回归任务的性能度量-——-均方误差"><a href="#1-回归任务的性能度量-——-均方误差" class="headerlink" title="1. 回归任务的性能度量 —— 均方误差"></a>1. 回归任务的性能度量 —— 均方误差</h4><p><strong>离散样本：</strong></p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}{(f(x_i)-y_i)^{2}}</script><p><strong>连续样本：</strong> 设 <strong>数据分布 D</strong> 和 <strong>概率密度 p(·) </strong></p><script type="math/tex; mode=display">E(f;D) = \int_{x\sim D}{(f(x) - y)^2p(x)dx}</script><p><strong>性能度量方法：</strong> 通常，均方误差大的模型性能差，均方误差小的模型性能好。</p><h4 id="2-分类任务的性能度量1-——-错误率与精度"><a href="#2-分类任务的性能度量1-——-错误率与精度" class="headerlink" title="2. 分类任务的性能度量1 —— 错误率与精度"></a>2. 分类任务的性能度量1 —— 错误率与精度</h4><p><strong>错误率(error rate)：</strong> 分类错误的样本占样本总数的比例</p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)\ne y_i)}</script><p><strong>精度(Accuracy)：</strong> 分类正确的样本占样本总数的比例</p><script type="math/tex; mode=display">acc(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)= y_i)}       = 1 - E(f;D)</script><p><strong>性能度量方法：</strong> 通常，错误率低精度高的模型性能好，错误率高精度低的模型性能差。</p><h4 id="3-分类任务的性能度量2-——-查准率、查全率与F1"><a href="#3-分类任务的性能度量2-——-查准率、查全率与F1" class="headerlink" title="3. 分类任务的性能度量2 —— 查准率、查全率与F1"></a>3. 分类任务的性能度量2 —— 查准率、查全率与F1</h4><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure3.PNG" alt="Figure3"></p><p><strong>查准率/准确率(Precision)：</strong> 【真正例样本数】与【预测结果是正例的样本数】的比值</p><script type="math/tex; mode=display">P = \frac{TP}{TP + FP}</script><p><strong>查全率/召回率(Recall)：</strong> 【真正例样本数】与【真实情况是正例的样本数】的比值</p><script type="math/tex; mode=display">R = \frac{TP}{TP + FN}</script><p><strong>解释：</strong></p><ul><li>查准率是在讲，挑出的好瓜里头，有多少是真正的好瓜，因此，若希望选出的瓜中好瓜的比例尽可能高，则查准率要高。</li><li>查全率是在讲，挑出的真正好瓜，占总共好瓜数的多少，因此，若希望将好瓜尽可能多的选出来，则查全率要高。</li></ul><p><strong>性能度量方法：</strong></p><ol><li><p><strong>直接观察数值</strong></p></li><li><p><strong>建立P-R图</strong></p><ul><li>“P-R曲线”是描述查准/查全率变化的曲线</li></ul></li></ol><ul><li><p>P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p> <img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure4.png" alt="Figure4"></p><ul><li><font color="#0099ff">当曲线没有交叉的时候：</font><ul><li>外侧学习器的性能优于内侧</li></ul></li><li><font color="#0099ff">当曲线有交叉的时候：</font><ul><li><strong>第一种</strong>方法是比较曲线下面积，但这个值不易估算</li><li><strong>第二种</strong>方法是比较两条曲线的平衡点，平衡点是“查准率 = 查全率”时的取值，在图中表示为曲线和对角线的交点，平衡点在外侧的曲线的学习器性能优于内侧</li><li><strong>第三种</strong>方法是F1度量和Fβ度量。F1是基于查准率与查全率的调和平均定义的，Fβ则是加权调和平均。</li></ul></li></ul></li></ul><p><strong>F1:</strong> 基于查准率和查全率的调和平均</p><script type="math/tex; mode=display">\frac{1}{F1} = \frac{1}{2} · (\frac{1}{P} + \frac{1}{R})</script><script type="math/tex; mode=display">F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}</script><p><strong>Fβ:</strong> 基于查准率和查全率的加权调和平均</p><script type="math/tex; mode=display">\frac{1}{F_\beta} = \frac{1}{1 + \beta^2} · (\frac{1}{P} + \frac{\beta^2}{R})</script><script type="math/tex; mode=display">F_\beta = \frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}</script><p><strong>说明：</strong> <strong>β &gt; 0 </strong>度量了查全率和查准率的相对重要性</p><ul><li><strong>β &gt; 1 </strong>时查全率有更大的影响</li><li><strong>β &lt; 1 </strong>时查准率有更大的影响</li></ul><p><strong><font color="#0099ff">在n个二分类混淆矩阵上综合考虑查准率和查全率：</font></strong></p><p><strong>方法一：宏(macro)：</strong>  先在各混淆矩阵上分别计算出查准率和查全率，记为 $(P_1, R_1)$, $(P_2, R_2)$, …… ,$(P_n, R_n)$, 再计算平均值，这样就得到了：</p><ul><li><p><strong>宏查准率(macro-P)</strong></p><script type="math/tex; mode=display">macro\_P = \frac{1}{n}\sum_{i = 1}^{n}{P_i}</script></li><li><p><strong>宏查全率(macro-R)</strong></p><script type="math/tex; mode=display">macro\_R = \frac{1}{n}\sum_{i = 1}^{n}{R_i}</script></li><li><p><strong>宏F1(macro-F1)</strong></p><script type="math/tex; mode=display">macro\_F1 = \frac{2 \times macro\_P \times macro\_R}{macro\_P + macro\_R}</script></li></ul><p><strong>方法二：微(micro)：</strong>  先将各混淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，分别记为 $\overline{TP}$，$\overline{FP}$，$\overline{TN}$，$\overline{FN}$，再基于这些平均值计算出：</p><ul><li><p><strong>微查准率(micro-P)</strong></p><script type="math/tex; mode=display">micro\_P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}</script></li><li><p><strong>微查全率(micro-R)</strong></p><script type="math/tex; mode=display">micro\_R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}</script></li><li><p><strong>微F1(micro-F1)</strong></p><script type="math/tex; mode=display">micro\_F1 = \frac{2 \times micro\_P \times micro\_R}{micro\_P + micro\_R}</script></li></ul><h4 id="4-分类任务的性能度量3-——-ROC与AUC"><a href="#4-分类任务的性能度量3-——-ROC与AUC" class="headerlink" title="4. 分类任务的性能度量3 —— ROC与AUC"></a>4. 分类任务的性能度量3 —— ROC与AUC</h4><p>与P-R图相同，ROC图通过对测试样本设置不同的<strong>阈值</strong>并与预测值比较，划分出正例和反例。再计算出真正例率和假正例率。P-R图逐个将样本作为正例，ROC图逐次与阈值进行比较后划分正例。本质上，都是将测试样本进行排序。</p><p><strong>真正例率(TPR):</strong> 【真正例样本数】与【真实情况是正例的样本数】的比值</p><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN}</script><p><strong>假正例率(FPR):</strong> 【假正例样本数】与【真实情况是反例的样本数】的比值</p><script type="math/tex; mode=display">FPR = \frac{FP}{TN + FP}</script><p><strong>ROC:</strong> 全称是“受试者工作特征” (Receiver Operating Characteristic)曲线，以真正例率为纵轴，以假正例率为横轴</p><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure5.png" alt="Figure5"></p><p><strong>性能度量方法：</strong> 绘制ROC曲线</p><ul><li><p><strong>当曲线没有交叉的时候：</strong> 外侧曲线的学习器性能优于内侧</p></li><li><p><strong>当曲线有交叉的时候：</strong> 比较ROC面积，如AUC</p><script type="math/tex; mode=display">AUC = \frac{1}{2}\sum_{i = 1}^{m - 1}{(x_{i+1}-x_{i})\times(y_{i}+y_{i+1})}</script></li></ul><h4 id="5-分类任务的性能度量4-——-代价敏感错误率与代价曲线"><a href="#5-分类任务的性能度量4-——-代价敏感错误率与代价曲线" class="headerlink" title="5. 分类任务的性能度量4 —— 代价敏感错误率与代价曲线"></a>5. 分类任务的性能度量4 —— 代价敏感错误率与代价曲线</h4><p>前面介绍的性能度量，大都隐式地假设了“<strong>均等代价</strong>”，而为权衡不同类型错误所造成的不同损失，应为错误赋予：“<strong>非均等代价</strong>”。</p><p>下图为二分类代价矩阵，其中 $cost_{ij}$ 表示将第i类样本预测为第j类样本的代价</p><p><img src="https://pic2.zhimg.com/80/v2-2070a8bd6c71ccf58a78a213430dee59_hd.png" alt="img"></p><p><strong>代价敏感(cost-sensitive)错误率:</strong> </p><script type="math/tex; mode=display">E(f;D;cost)=\frac{1}{m}(\sum_{x_i \in D^+} II (f(x_i) \ne y_i) \times cost_{01} + \sum_{x_i \in D^-} II(f(x^i) \ne y_i) \times cost_{10})</script><p><strong>性能度量的方法</strong>：绘制代价曲线</p><p>代价曲线的横轴是正例概率代价 $P(+)cost$，纵轴是归一化代价 $cost_{norm}$</p><script type="math/tex; mode=display">P(+)_{cost} = \frac{p\times cost_{01}}{p\times{cost_{01}} + (1-p)\times{cost_{10}}}</script><script type="math/tex; mode=display">cost_{norm} = \frac{FNR \times p\times cost_{01} + FPR \times (1 - p) \times cost_{10}}{p \times cost_{01} + (1 - p) \times cost_{10}}</script><hr><h3 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h3><p><strong><font color="#ff0000">机器学习中性能比较需要考虑的因素：</font></strong></p><ol><li>性能比较的目标是比较泛化性能，但通过实验评估方法得到的是测试集上的性能，两者的对比结果可能未必相同</li><li>测试集上的性能受测试集本身的影响较大，使用不同大小的测试集会得到不同的结果，即使相同大小的测试集，若包含的测试样例不同，则测试结果也会不同</li><li>很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会不同</li></ol><p><strong><font color="#ff0000">统计假设检验 (hypothesis test)：</font></strong></p><ul><li>为进行机器学习的性能比较提供了重要依据</li><li>基于假设检验的结果可以推断出：<strong>若在测试集上观察到学习器 A 比学习器 B 好，则 A 的泛化性能是否在统计意义上优于 B，以及这个结论的把握有多大</strong></li></ul><h4 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h4><h4 id="交叉验证-t-检验"><a href="#交叉验证-t-检验" class="headerlink" title="交叉验证 t 检验"></a>交叉验证 t 检验</h4><h4 id="McNemar-检验"><a href="#McNemar-检验" class="headerlink" title="McNemar 检验"></a>McNemar 检验</h4><h4 id="Friedman-检验与-Nemenyi-检验"><a href="#Friedman-检验与-Nemenyi-检验" class="headerlink" title="Friedman 检验与 Nemenyi 检验"></a>Friedman 检验与 Nemenyi 检验</h4><hr><h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p><strong>“偏差—方差分解”</strong> ：是解释学习器泛化性能的重要工具，在解释这个概念之前，先明确以下各变量名：</p><ul><li>测试样本：$\boldsymbol x$</li><li>测试样本 $\boldsymbol x$ 在数据集中的标记：$y_D$</li><li>测试样本 $\boldsymbol x$ 的真实标记：$y$ </li><li>训练集：$D$</li><li>从训练集 $D$ 中学得的模型：$f$</li><li>模型 $f$ 在测试样本 $\boldsymbol x$上的预测输出：$f(x; D)$</li><li><strong>数据分布：Ɗ</strong></li></ul><p><strong>1. $\overline f(x)$ (预测输出的期望) </strong>：学习算法的期望预测</p><script type="math/tex; mode=display">\overline f(x) = E_Ɗ[f(x;D)]</script><p><strong>2. $Variance$ (方差)</strong>：使用样本数相同的不同训练集产生的方差</p><script type="math/tex; mode=display">var(x) = E_Ɗ[(f(x;D) - \overline f(x))^2]</script><font color="#0099ff">方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</font><p><strong>3. $Bias$ (偏差)</strong>：期望输出与真实标记之间的差别</p><script type="math/tex; mode=display">bias^2(x) = (\overline f(x) - y)^2</script><font color="#0099ff">偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</font><p><strong>4. $\varepsilon^2$ (噪声)</strong>：数据集标记和真实标记的方差</p><script type="math/tex; mode=display">\varepsilon ^2 = E_Ɗ[(y_Ɗ - y)^2]</script><font color="#0099ff">噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</font><p>噪声数据主要来源是训练数据的错误标签的情况以及输入数据某一维不确定的情况。</p><font color="#0099ff">为了方便讨论，假定噪声期望为0，则对算法的期望泛化误差进行分解可得：</font><p><img src="https://pic2.zhimg.com/v2-ae93d046ff25fd8d5ea74467d579cb41_r.jpg" alt="preview"></p><p>故：</p><script type="math/tex; mode=display">E(f;D) = bias^2 (x) + var(x) + \varepsilon^2</script><p> 即：<strong>泛化误差 = 偏差 + 方差 + 噪声</strong></p><hr><h3 id="二项分布参数p的检验"><a href="#二项分布参数p的检验" class="headerlink" title="二项分布参数p的检验"></a>二项分布参数p的检验</h3><p>设某事件发生的概率为 <code>p</code> ， <code>p</code> 未知，作 <code>m</code> 次独立试验，每次观察该事件是否发生，以X记该事件发生的次数，则X服从二项分布 <code>B(m, p)</code>，现根据X检验如下假设：</p><script type="math/tex; mode=display">H_0: p \le p_0\\H_1: p > p_0</script><p>由二项分布本身的特性可知：p越小，X取到较小值的概率越大。因此，对于上述假设，一个直观上合理的检验为：</p><script type="math/tex; mode=display">\varphi: 当 X \le C 时接受H_0，否则拒绝H_0</script><p>其中，$C \in N$表示事件最大发生次数。此检验对应的功效函数为：</p><script type="math/tex; mode=display">\begin{align}\beta_\varphi(p) &= P(X > C)\\&= 1 - P(X \le C)\\&= 1 -\sum_{i = 0}^{C}\begin{pmatrix}m\\i\end{pmatrix}p^i(1-p)^{m-i}\\&=\sum_{i=C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}p^i(1-p)^{m-i}\\\end{align}</script><p>由于“p越小，X取到较小值的概率越大“可以等价表示为：$P(X \le C)$ 是关于p的减函数，所以$\beta_\varphi (p) = P(X &gt; C) = 1 - P(X \le C)$ 是关于p的增函数，那么当 $p \le p_0$ 时，$\beta_\varphi (p_0) $ 即为 $\beta_\varphi (p) $ 的上确界。</p><p>又因为，检验水平 $\alpha$  默认取最小可能水平，所以在给定检验水平 $\alpha$ 时，可以通过如下方程解得满足检验水平$\alpha$ 的整数C:</p><script type="math/tex; mode=display">\alpha = sup\{\beta_\varphi(p)\}</script><p>显然，当$p \le p_0$时：</p><script type="math/tex; mode=display">\begin{align}\alpha &= sup\{\beta_\varphi(p)\}\\&= \beta_\varphi(p_0)\\&= \sum_{i = C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i}\end{align}</script><p>对于此方程，通常不一定正好解得一个整数C使得方程成立，常见的情况是存在这样一个 $\overline {C}$  使得：</p><script type="math/tex; mode=display">\sum_{i = \overline C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i} < \alpha</script><script type="math/tex; mode=display">\sum_{i = \overline C}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i} > \alpha</script><p>此时，C只能取  $\overline {C}$  或者  $\overline {C} + 1$  ，若 C 取 $\overline C$ ，则相当于升高了校验水平 $\alpha$ , 若 C 取 $\overline{C} + 1$ 则相当于降低了检验水平 $\alpha$ ，具体如何取舍需要结合实际情况，但是通常为了减小犯第一类错误的概率，会倾向于令 C 取 $\overline C + 1$. </p><p>下面考虑如何求解 $\overline C$: 易证 $\beta_{\varphi}(p_0)$ 是关于 C 的减函数，所以再结合上述关于 $\overline C$ 的两个不等式，易推得：</p><script type="math/tex; mode=display">\overline C = minC \quad s.t.\ \sum_{i = C+1}^{m}{p_0^i + (1-p_0)^{m-i}} < \alpha</script><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（二）模型评估与选择&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（二）模型评估与选择&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（一）-- 绪论</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%20%E7%BB%AA%E8%AE%BA/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（一）-- 绪论/</id>
    <published>2019-08-05T10:39:04.000Z</published>
    <updated>2019-08-27T01:26:57.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（一）绪论"><a href="#《机器学习》西瓜书学习笔记（一）绪论" class="headerlink" title="《机器学习》西瓜书学习笔记（一）绪论"></a>《机器学习》西瓜书学习笔记（一）绪论</h2><a id="more"></a><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><ol><li><strong>学习算法(learning algorithm):</strong> 机器学习研究的主要内容，是关于在计算机上从数据产生“模型”的算法，即“学习算法”.</li><li><strong>学习算法的作用：</strong><ul><li>基于提供的经验数据产生模型</li><li>面对新情况，模型可提供相应的判断</li></ul></li><li><strong>模型(model):</strong> 泛指从数据中学到的结果</li><li><strong>学习器(learner):</strong> 学习算法在给定参数空间上的实例化</li></ol><hr><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><ul><li><strong>数据集(data set):</strong> 一组记录的集合</li><li><strong>示例(instance)/样本(sample)/特征向量(feature vector):</strong> 每条记录（关于一个事件或对象的描述）或空间中的每一个点（对应一个坐标向量）</li><li><strong>属性(attribute)/特征(feature):</strong> 反映事件或对象在某方面的表现或性质的事项</li><li><strong>属性值(attribute value):</strong> 属性上的取值</li><li><strong>属性空间(attribute space)/样本空间(sample space)/输入空间(input space):</strong> 属性张成的空间</li><li><strong>维数(dimensionality):</strong> 属性的个数</li><li><strong>学习(learning)/训练(training):</strong> 从数据中学得模型的过程</li><li><strong>训练数据(training data): </strong> 训练过程中使用的数据</li><li><strong>训练样本(training sample):</strong> 训练数据中的每个样本</li><li><strong>训练集(training set):</strong> 训练样本组成的集合</li><li><strong>假设(hypothesis):</strong> 学得模型对应了关于数据的某种潜在的规律</li><li><strong>真相/真实(ground-truth):</strong> 这种潜在规律的自身</li><li><strong>预测(prediction):</strong> 获得训练样本的结果信息，才能建立“预测”的模型</li><li><strong>标记(label):</strong> 关于示例结果的信息</li><li><strong>样例(example):</strong> 拥有了标记信息的示例</li><li><strong>标记空间(label space):</strong> 所有标记的集合</li><li><strong>分类(classification):</strong> 预测的离散值<ul><li>二分类(binary classification)<ul><li>正类(positive class)</li><li>负类(negative class)</li></ul></li><li>多分类(multi-class classification)</li></ul></li><li><strong>回归(regression):</strong> 预测的连续值</li><li><strong>测试(testing):</strong> 学的模型后，使用其进行预测的过程</li><li><strong>测试样本(testing sample):</strong> 被预测的样本</li><li>学习任务分类：<ul><li><font color="#0099ff">监督学习(supervised learning): 有标记</font><ul><li>分类 (classification)</li><li>回归 (regression)</li></ul></li><li><font color="#0099ff">无监督学习(unsupervised learning): 无标记</font><ul><li>聚类 (clustering)</li></ul></li></ul></li><li><strong>泛化(generalization)能力:</strong> 学得模型适用于新样本的能力</li></ul><hr><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><ul><li>科学推理:<ul><li><strong>归纳(induction):</strong> 特殊 —-&gt; 一般，泛化</li><li><strong>假设(deduction):</strong> 一般 —-&gt; 特殊，特化</li></ul></li><li><strong>归纳学习：</strong> <ul><li>广义：从样例中学习</li><li>狭义：从训练数据学得概念，概念学习、概念形成</li></ul></li></ul><hr><h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><ul><li><strong>归纳偏好:</strong> 机器学习算法在学习过程中对某种类型假设的偏好<ul><li><strong><font color="#0099ff">任何一个有效的机器学习算法必有其归纳偏好</font></strong></li></ul></li><li><strong>“奥卡姆剃刀”原则:</strong> 若有多个假设与观察一致，则选最简单的那个</li><li><strong>“没有免费的午餐”定理(NFL定理):</strong>  总误差与学习算法无关</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（一）绪论&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（一）绪论&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer</title>
    <link href="http://sunfeng.online/2019/08/03/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer/"/>
    <id>http://sunfeng.online/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/</id>
    <published>2019-08-03T09:06:43.000Z</published>
    <updated>2019-08-30T08:43:01.181Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Planar-data-classification-with-a-hidden-layer"><a href="#Planar-data-classification-with-a-hidden-layer" class="headerlink" title="Planar data classification with a hidden layer"></a>Planar data classification with a hidden layer</h3><a id="more"></a><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression.</p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a signal hidden layer</li><li>Use units with a non-linear activation function, such as tanh</li><li>Compute the cross entropy loss</li><li>Implement forward and backward propagation</li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data and analysis.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load planar dataset</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @X: your dataset features</span></span><br><span class="line"><span class="string">        @Y: your dataset labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    X, Y = load_planar_dataset()    <span class="comment"># load dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure1.png" alt="Figure1"></p><p>You have:</p><blockquote><ul><li>a numpy array (matrix) X that contains your features (x1, x2)</li><li>a numpy array (vector) Y that contains your labels (red: 0, blue: 1)</li></ul></blockquote><p>Let’s first get a better sense of what our data is like.</p><p><strong>Exercise:</strong> How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code>and <code>Y</code> ?</p><p><strong>Code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m = X.shape[<span class="number">1</span>]  <span class="comment"># training set size</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The shape of X <span class="keyword">is</span>: (<span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">The shape of Y <span class="keyword">is</span>: (<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">I have m = <span class="number">400</span> training examples!</span><br></pre></td></tr></table></figure><hr><h3 id="Simple-Logistic-Regression"><a href="#Simple-Logistic-Regression" class="headerlink" title="Simple Logistic Regression"></a>Simple Logistic Regression</h3><p>Before building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure><p>You can now plot the decision boundary  of these models. Run the code below.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure2.png" alt="Figure2"></p><p><strong>Interpretation:</strong> The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network  will do better. Let’s try this now.</p><hr><h3 id="Neural-Network-model"><a href="#Neural-Network-model" class="headerlink" title="Neural Network model"></a>Neural Network model</h3><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure3.png" alt="Figure3"></p><p><strong>Mathematically:</strong></p><p>For one example x(i):</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure4.png" alt="Figure4"></p><p>Given the predictions on all the examples, you can also compute the cost J as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Reminder:</strong> The general methodology to build a Neural Network is to:</p><blockquote><ol><li>Define the neural network structure (# of input units, # of hidden units, etc).</li><li>Initialize the model’s parameters</li><li><font color="#0099ff"><strong>Loop:</strong></font><ul><li>Implement forward propagation</li><li>Compute loss</li><li>Implement backward propagation to get the gradients</li><li>Update parameters (gradient descent)</li></ul></li></ol></blockquote><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learned the right parameters, you can make predictions on new data.</p><h4 id="Defining-the-neural-network-structure"><a href="#Defining-the-neural-network-structure" class="headerlink" title="Defining the neural network structure"></a>Defining the neural network structure</h4><p><strong>Exercise:</strong> Define three variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- n_X: the size of the input layer</span><br><span class="line">- n_h: the size of the hidden layer (set this to <span class="number">4</span>)</span><br><span class="line">- n_y: the size of the output layer</span><br></pre></td></tr></table></figure><p><strong>Hint:</strong> Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defining the neural network structure</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Defining the neural network structure</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @x: input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @y: labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @n_x: the size of input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of output layer</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]</span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</span><br><span class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</span><br><span class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The size of the input layer is: n_x = 5</span><br><span class="line">The size of the hidden layer is: n_h = 4</span><br><span class="line">The size of the output layer is: n_y = 2</span><br></pre></td></tr></table></figure><h4 id="Initialize-the-model’s-parameters"><a href="#Initialize-the-model’s-parameters" class="headerlink" title="Initialize the model’s parameters"></a>Initialize the model’s parameters</h4><p><strong>Exercise:</strong> Implement the function <code>initialize_parameters().</code></p><p><strong>Instructions:</strong></p><ul><li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if you needed.</li><li>You will initialize the weights matrices with random values.<ul><li><strong>Use:</strong> np.random.randn (a, b) * 0.01 to randomly initialize a matrix of shape (a, b).</li></ul></li><li>You will initialize the bias vectors as zeros.<ul><li><strong>Use:</strong> np.zeros((a, b)) to initialize a matrix of shape (a, b) with zeros.</li></ul></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model's parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize the model's parameters</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @n_x: the size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">2</span>)   <span class="comment"># set a seed so that the result are consisent</span></span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y = initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00416758 -0.00056267]</span><br><span class="line"> [-0.02136196  0.01640271]</span><br><span class="line"> [-0.01793436 -0.00841747]</span><br><span class="line"> [ 0.00502881 -0.01245288]]</span><br><span class="line">b1 = [[ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]]</span><br><span class="line">W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</span><br><span class="line">b2 = [[ 0.]]</span><br></pre></td></tr></table></figure><h4 id="The-Loop"><a href="#The-Loop" class="headerlink" title="The Loop"></a>The Loop</h4><p><strong>Exercise 1:</strong> Implement <code>forward_propagation()</code> .</p><p><strong>Instructions:</strong></p><ul><li>Look above at the mathematical representation of your classifier.</li><li>You can use the function <code>sigmoid()</code> . It is built-in (imported) in the notebook (<code>planar_utils.py</code>).</li><li>You can use the function <code>tanh()</code> . It is part of the numpy library.</li><li>The steps you have to implement are:<ul><li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>paramters[&quot;...&quot;]</code> .</li><li>Implement Forward Propagation. Compute <code>Z[1]</code>, <code>A[1]</code>, <code>Z[2]</code> and <code>A[2]</code>  (the vector of all your predictions on all the examples in the training set).</li></ul></li><li>Values needed in the back propagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the back propagation function.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement forward propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    implement forward propagation</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement forward propagation to calculate A2</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.262818640198 0.091999045227 -1.30766601287 0.212877681719</span><br></pre></td></tr></table></figure><p>Now that you have computed <code>A[2]</code> (in the Python variable “A2”), which contains <code>a[2](i)</code> for every examples, you can compute the cost function as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Exercise 2:</strong> Implement <code>compute_cost()</code> to compute the value of the cost J.</p><p><strong>Instructions:</strong></p><ul><li><p>There are many ways to implement the cross-entropy loss (交叉熵损失). To help you, we give you how we would have implemented:</p><script type="math/tex; mode=display">-\sum_{i = 0}^{m}{y^{(i)}log(a^{[2](i)})}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2), Y)</span><br><span class="line">cost = -np.sum(logprobs) <span class="comment"># no need to use a for loop</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement compute cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation, of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters W1, b1, W2, b2</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @cost: cross-entropy cost</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment">#number of examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span> - A2), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">-1</span> / m * np.sum(logprobs)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># make sure cost is the dimension we expect</span></span><br><span class="line">                                <span class="comment"># E.g. turns [[17]] into 17</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = 0.693058761</span><br></pre></td></tr></table></figure><p>Using the cache computed during forward propagation, you can now implement backward propagation.</p><p><strong>Exercise 3:</strong> Implement the function <code>backward_propagation().</code></p><p><strong>Instructions:</strong> Backward propagation is usually the hardest (most mathematical) part in deep learning. o help you, here again is the slide from the lecture on backward propagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure5.png" alt="Figure5"></p><p><strong>Tips:</strong></p><ul><li>To compute dZ1 you’ll need to compute g[1]′(Z[1]). Since g[1](.)is the tanh activation function, if a=g[1](z) then g[1]′(z)=1−a2. So you can compute g[1]′(Z[1]) using <code>(1 - np.power(A1, 2))</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement the function backward propagration</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the backward propagation</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing out parameters (W1, b1, W2, b2)</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2", "A2". </span></span><br><span class="line"><span class="string">        @X: input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape(1, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve W1 and W2 from the dictionary parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve A1 and A2 from dictionary "cache"</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ 0.00301023 -0.00747267]</span><br><span class="line"> [ 0.00257968 -0.00641288]</span><br><span class="line"> [-0.00156892  0.003893  ]</span><br><span class="line"> [-0.00652037  0.01618243]]</span><br><span class="line">db1 = [[ 0.00176201]</span><br><span class="line"> [ 0.00150995]</span><br><span class="line"> [-0.00091736]</span><br><span class="line"> [-0.00381422]]</span><br><span class="line">dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]</span><br><span class="line">db2 = [[-0.16655712]]</span><br></pre></td></tr></table></figure><p><strong>Exercise 4:</strong> Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p><p><strong>General gradient descent rule:</strong> θ=θ−α∂J∂θ where α is the learning rate and θ represents a parameter.</p><p><strong>Illustration:</strong> The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure6.gif" alt="Figure 6"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure7.gif" alt="Figure 7"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Update parameters using the gradient descent update rule</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradient with respect to different parameters</span></span><br><span class="line"><span class="string">        @learning_rate: the learning rate used to update parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve each grident from the dictionary "grads"</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00643025  0.01936718]</span><br><span class="line"> [-0.02410458  0.03978052]</span><br><span class="line"> [-0.01653973 -0.02096177]</span><br><span class="line"> [ 0.01046864 -0.05990141]]</span><br><span class="line">b1 = [[ -1.02420756e-06]</span><br><span class="line"> [  1.27373948e-05]</span><br><span class="line"> [  8.32996807e-07]</span><br><span class="line"> [ -3.20136836e-06]]</span><br><span class="line">W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]</span><br><span class="line">b2 = [[ 0.00010457]]</span><br></pre></td></tr></table></figure><h4 id="Integrate-part-5-1-5-2-and-5-3-in-nn-model"><a href="#Integrate-part-5-1-5-2-and-5-3-in-nn-model" class="headerlink" title="Integrate part 5.1, 5.2 and 5.3 in nn_model()"></a>Integrate part 5.1, 5.2 and 5.3 in <code>nn_model()</code></h4><p><strong>Question:</strong> Build your neural network model in <code>nn_model()</code>.</p><p><strong>Instructions: </strong> The neural network model has to use the previous functions in the right order.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all function into the nerual network model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Build your neural network in nn_model</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X: dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: labels of shape (1, number of exampless)</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @num_iterations: Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">        @print_cost: if True, print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: parameters learnt by the model. They can then be used to predict</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize paramters, then retrieve W1, b1, W2, b2.</span></span><br><span class="line">    <span class="comment"># Inputs: "n_x, n_h, n_y"</span></span><br><span class="line">    <span class="comment"># Outputs: " parameters(W1, b1, W2, b2)"</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation. </span></span><br><span class="line">        <span class="comment"># Inputs: "X, parameters". </span></span><br><span class="line">        <span class="comment"># Outputs: "A2, cache"</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost Function.</span></span><br><span class="line">        <span class="comment"># Inputs: "A2, Y, parameters"</span></span><br><span class="line">        <span class="comment"># Output: "cost"</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, cache, X, Y"</span></span><br><span class="line">        <span class="comment"># Outputs: "grads"</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters by using gradient descent.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, grads"</span></span><br><span class="line">        <span class="comment"># Outputs: "parameters"</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations:</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iterations %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.692739</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.000218</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.000107</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.000071</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.000053</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.000042</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.000035</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.000030</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.000026</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.000023</span></span><br><span class="line">W1 = [[<span class="number">-0.65848169</span>  <span class="number">1.21866811</span>]</span><br><span class="line"> [<span class="number">-0.76204273</span>  <span class="number">1.39377573</span>]</span><br><span class="line"> [ <span class="number">0.5792005</span>  <span class="number">-1.10397703</span>]</span><br><span class="line"> [ <span class="number">0.76773391</span> <span class="number">-1.41477129</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.287592</span>  ]</span><br><span class="line"> [ <span class="number">0.3511264</span> ]</span><br><span class="line"> [<span class="number">-0.2431246</span> ]</span><br><span class="line"> [<span class="number">-0.35772805</span>]]</span><br><span class="line">W2 = [[<span class="number">-2.45566237</span> <span class="number">-3.27042274</span>  <span class="number">2.00784958</span>  <span class="number">3.36773273</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.20459656</span>]]</span><br></pre></td></tr></table></figure><h4 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h4><p><strong>Question:</strong> Use your model to predict by building <code>predict()</code>. Use forward propagation to predict results.</p><p><strong>Reminder:</strong> </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure8.png" alt="Figure8"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use forward propagation to predict results</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments: </span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your paramters</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @predictions: vector of predictions of our model (red: 0 / bule: 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classfies to 0/1 using 0.5 as threshold</span></span><br><span class="line">    A2 = forward_propagation(X, parameters)[<span class="number">0</span>]</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions mean = <span class="number">0.666666666667</span></span><br></pre></td></tr></table></figure><font color="#0099">It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of `n_h` hidden units.</font><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693048</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.288083</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.254385</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.233864</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.226792</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.222644</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.219731</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.217504</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.219454</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.218607</span></span><br><span class="line">Accuracy: <span class="number">90</span>%</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure9.png" alt="Figure9"></p><p><strong>Interpretation:</strong>  Accuracy is really high compared to Logistic Regression. The model has learned the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.</p><p>Now, let’s try out several hidden layer sizes.</p><h4 id="Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="Tuning hidden layer size(optional/ungraded exercise)"></a>Tuning hidden layer size(optional/ungraded exercise)</h4><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)                                <span class="comment"># ？？？</span></span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>) </span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)  <span class="comment"># ???</span></span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Accuracy <span class="keyword">for</span> <span class="number">1</span> hidden units: <span class="number">67.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">2</span> hidden units: <span class="number">67.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">3</span> hidden units: <span class="number">90.75</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">4</span> hidden units: <span class="number">90.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">5</span> hidden units: <span class="number">91.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">20</span> hidden units: <span class="number">90.0</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">50</span> hidden units: <span class="number">90.25</span> %</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure11.png" alt="Figure11"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure12.png" alt="Figure12"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure10.png" alt="Figure10"></p><p><strong>Interpretation:</strong></p><ul><li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.</li><li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting.</li><li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting.</li></ul><p><strong><font color="#00f">You’ve learnt to:</font></strong></p><ul><li>Build a complete neural network with a hidden layer</li><li>Make a good use of a non-linear unit</li><li>Implemented forward propagation and backpropagation, and trained a neural network</li><li>See the impact of varying the hidden layer size, including overfitting.</li></ul><p>Nice work!</p><hr><h3 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment3_shallow_neural_network" target="_blank" rel="noopener">Source Code</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Planar-data-classification-with-a-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-a-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with a hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with a hidden layer&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>如何上传本地代码到github</title>
    <link href="http://sunfeng.online/2019/08/02/%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github/"/>
    <id>http://sunfeng.online/2019/08/02/如何上传本地代码到github/</id>
    <published>2019-08-02T08:34:43.000Z</published>
    <updated>2019-09-07T07:33:33.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何上传本地代码到github"><a href="#如何上传本地代码到github" class="headerlink" title="如何上传本地代码到github"></a>如何上传本地代码到github</h2><a id="more"></a><h3 id="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："><a href="#第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：" class="headerlink" title="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："></a>第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure1.jpg" alt="figure1"></p><p>点击<strong>Clone or download</strong>按钮，复制弹出的地址<strong>git@github.com:***/***.git</strong></p><p>注意要用SSH地址。</p><hr><h3 id="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"><a href="#第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令" class="headerlink" title="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"></a>第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><hr><h3 id="第三步：将项目的所有文件添加到仓库中"><a href="#第三步：将项目的所有文件添加到仓库中" class="headerlink" title="第三步：将项目的所有文件添加到仓库中"></a>第三步：将项目的所有文件添加到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><hr><h3 id="第四步：将添加的文件提交到仓库中"><a href="#第四步：将添加的文件提交到仓库中" class="headerlink" title="第四步：将添加的文件提交到仓库中"></a>第四步：将添加的文件提交到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;注释语句&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="第五步：将本地仓库关联到github上"><a href="#第五步：将本地仓库关联到github上" class="headerlink" title="第五步：将本地仓库关联到github上"></a>第五步：将本地仓库关联到github上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:***/test.git</span><br></pre></td></tr></table></figure><hr><h3 id="第六步：上传之前，先要pull一下，执行如下命令："><a href="#第六步：上传之前，先要pull一下，执行如下命令：" class="headerlink" title="第六步：上传之前，先要pull一下，执行如下命令："></a>第六步：上传之前，先要pull一下，执行如下命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><hr><h3 id="第七步：上传代码到github远程仓库"><a href="#第七步：上传代码到github远程仓库" class="headerlink" title="第七步：上传代码到github远程仓库"></a>第七步：上传代码到github远程仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><hr><h2 id="更新代码"><a href="#更新代码" class="headerlink" title="更新代码"></a>更新代码</h2><h3 id="第一步：查看当前的-git-仓库状态，执行如下命令"><a href="#第一步：查看当前的-git-仓库状态，执行如下命令" class="headerlink" title="第一步：查看当前的 git 仓库状态，执行如下命令"></a>第一步：查看当前的 git 仓库状态，执行如下命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><hr><h3 id="第二步：更新全部"><a href="#第二步：更新全部" class="headerlink" title="第二步：更新全部"></a>第二步：更新全部</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add *</span><br></pre></td></tr></table></figure><hr><h3 id="第三步：输入更新说明"><a href="#第三步：输入更新说明" class="headerlink" title="第三步：输入更新说明"></a>第三步：输入更新说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;更新说明&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="第四步：先-git-pull，拉取当前分支最新代码"><a href="#第四步：先-git-pull，拉取当前分支最新代码" class="headerlink" title="第四步：先 git pull，拉取当前分支最新代码"></a>第四步：先 git pull，拉取当前分支最新代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><hr><h3 id="第五步：push-到远程-master-分支上"><a href="#第五步：push-到远程-master-分支上" class="headerlink" title="第五步：push 到远程 master 分支上"></a>第五步：push 到远程 master 分支上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p><strong><font color="#0099ff">不出意外，打开github，可发现已经同步了</font></strong></p><hr><h2 id="如何在-VScode-上提交代码到-github"><a href="#如何在-VScode-上提交代码到-github" class="headerlink" title="如何在 VScode 上提交代码到 github"></a>如何在 VScode 上提交代码到 github</h2><h3 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure2.PNG" alt="figure2"></p><hr><h3 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure3.PNG" alt="figure3"></p><hr><h3 id="第三步："><a href="#第三步：" class="headerlink" title="第三步："></a>第三步：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure4.PNG" alt="figure4"></p><p><strong>祝你成功！</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;如何上传本地代码到github&quot;&gt;&lt;a href=&quot;#如何上传本地代码到github&quot; class=&quot;headerlink&quot; title=&quot;如何上传本地代码到github&quot;&gt;&lt;/a&gt;如何上传本地代码到github&lt;/h2&gt;
    
    </summary>
    
      <category term="Others" scheme="http://sunfeng.online/categories/Others/"/>
    
    
      <category term="github" scheme="http://sunfeng.online/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset</title>
    <link href="http://sunfeng.online/2019/08/01/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/"/>
    <id>http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/</id>
    <published>2019-08-01T08:06:21.000Z</published>
    <updated>2019-08-30T08:41:48.862Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a><strong>Logistic Regression with a Neural Network mindset</strong></h3><a id="more"></a><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li><p>Build the general architecture of a learning algorithm, including:</p><ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent)</li></ul></li><li><p>Gather all three functions above into a main function, in the right order.</p></li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li></ul><p>code ————-&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure><hr><h3 id="Overview-of-the-Problem-set"><a href="#Overview-of-the-Problem-set" class="headerlink" title="Overview of the Problem set"></a>Overview of the Problem set</h3><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><blockquote><ul><li>a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0)</li><li>a test set of m_test images labeled as cat or non-cat</li><li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)</li></ul></blockquote><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data preprocessing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_Preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load dataset and preprocess dataset</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @train_set_x: your train set features</span></span><br><span class="line"><span class="string">        @train_set_y: your train set labels</span></span><br><span class="line"><span class="string">        @test_set_x: your test set features</span></span><br><span class="line"><span class="string">        @test_set_y: your test set labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line">    <span class="comment"># load dataset from dataset files</span></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    <span class="comment"># vectorize the features of each examples</span></span><br><span class="line">    train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">    test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line">    <span class="comment"># normalize the features vector</span></span><br><span class="line">    <span class="keyword">return</span> train_set_x, train_set_y, test_set_x, test_set_y, classes</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:**</font><font color="#0099ff">Common steps for pre-processing a new dataset are:</font><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</li></ul><hr><h3 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h3><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure1.png" alt="Figure1"></p><p><strong>Mathematical expression of the algorithm:</strong></p><p>For one examples x(i):</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/Figure2.png" alt="Figure2"></p><p>The cost is the computed by summing over all training examples:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure3.png" alt="Figure3"></p><p><strong>Key steps:</strong> In this exercise, you will carry out the following steps:</p><blockquote><ul><li>Initialize the parameters of the model</li><li>Learn the parameters for the model by minimizing the cost</li><li>Use the learned parameters to make predictions (on the test set)</li><li>Analyze the results and conclude</li></ul></blockquote><hr><h3 id="Building-the-parts-of-our-algorithm"><a href="#Building-the-parts-of-our-algorithm" class="headerlink" title="Building the parts of our algorithm"></a>Building the parts of our algorithm</h3><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features)</li><li>Initialize the model’s parameters</li><li>Loop:<ul><li><strong>Calculate current loss (forward propagation)</strong></li><li><strong>Calculate current gradient (backward propagation)</strong></li><li><strong>Update parameters (gradient descent)</strong></li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call model().</p><h4 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h4><p><strong>Exercise:</strong> using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute </p><script type="math/tex; mode=display">sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}</script><p>to make predictions. Use np.exp().</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @z: A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @s: sigmoid(z)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h4 id="Initializing-parameters"><a href="#Initializing-parameters" class="headerlink" title="Initializing parameters"></a>Initializing parameters</h4><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initializing parameters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @dim: size of the w vector we want</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @w: initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">        @b: initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function propagation() that computes the cost function and its gradient.</p><p><strong>Hints(提示):</strong></p><font color="#0099ff">Forward Propagation:</font><ul><li><p>You get x</p></li><li><p>You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$</p></li><li><p>You calculate the cost function: </p><p>$J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 -  y^{(i)})log(1 - a^{(i)}) }$</p></li></ul><p>Here are the two formulas you will be using:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure4.png" alt="Figure4"></p><p><strong>code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward and Backword propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the</span></span><br><span class="line"><span class="string">    propagation explained above</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @cost: negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">        @dw: gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">        @db: gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># nx</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b))  <span class="comment"># compute activation</span></span><br><span class="line">    cost = -(np.dot(Y, np.log(A).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A).T)) / m <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m <span class="comment"># compute dw</span></span><br><span class="line">    db = np.sum(A - Y) / m      <span class="comment"># compute db</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 把shape中为1的维度去掉</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    <span class="comment"># 判断剩下的是否为空</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, </span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Optimization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @num_iterations: number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        @learning_rate: learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        @print_cost: True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">        @grads: dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">        @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">        You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        (1) Calculate the cost and the gradient for the current parameters. Use propagate()</span></span><br><span class="line"><span class="string">        (2) Update the parameters using gradient descent rule for w and b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update rule</span></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %d: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li>Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$</li><li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_Prediction</code>. </li></ol><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Graded function: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned </span></span><br><span class="line"><span class="string">    logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @Y_prediction: a numpy array (vector) containing all prediction</span></span><br><span class="line"><span class="string">                       (0 / 1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m =  X.shape[<span class="number">1</span>]     <span class="comment"># number of examples</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b)) <span class="comment"># (1, m)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0, i] to actual predictions p[0, i]</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:** You've implemented several functions that:</font><ul><li>initialize (w, b)</li><li>Optimize the loss iteratively to learn parameters (w, b):<ul><li>computing the cost and its gradient</li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w, b) to predict the labels for a given set of examples</li></ul><hr><h3 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h3><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p><blockquote><ul><li>Y_prediction for your predictions on the test set</li><li>Y_prediction_train for your predictions on the train set</li><li>w, costs, grads for the outputs of optimize()</li></ul></blockquote><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all functions into a model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function</span></span><br><span class="line"><span class="string">    you have implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">        @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">        @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">        @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test)</span></span><br><span class="line"><span class="string">        @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">        @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">        @print_cost: Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @d: dictionary containing information about the model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initailize parameters with zeros</span></span><br><span class="line">    w,b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])   <span class="comment"># num_px * num_px * 3, w: (dim, 1), b: a scalar</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples </span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span> : costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span> : Y_prediction_test,</span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">"w"</span> : w,</span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span> : num_iterations</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =<span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.376007</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.331463</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.303273</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.279880</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.260042</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.242941</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.228004</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.214820</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.203078</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.192544</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.183033</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.174399</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.166521</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.159305</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.152667</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.146542</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure><p><strong>Comment:</strong> Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. </p><font color="#0099ff">Let's also plot the cost function and the gradients:</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure5.png" alt="Figure5"></p><p><strong>Interpretation</strong>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p><hr><h3 id="Further-analysis"><a href="#Further-analysis" class="headerlink" title="Further analysis"></a>Further analysis</h3><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α.</p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow= <span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure6.png" alt="Figure6"></p><p><strong>Interpretation</strong>:</p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you:<ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li></ul></li></ul><hr><p><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment2_logistics_regression" target="_blank" rel="noopener">Source Code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic-Regression-with-a-Neural-Network-mindset&quot;&gt;&lt;a href=&quot;#Logistic-Regression-with-a-Neural-Network-mindset&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression with a Neural Network mindset&quot;&gt;&lt;/a&gt;&lt;strong&gt;Logistic Regression with a Neural Network mindset&lt;/strong&gt;&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="课后习题及编程练习" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E5%8F%8A%E7%BC%96%E7%A8%8B%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
