<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunFeng&#39;s Blog</title>
  
  <subtitle>学习，敲码，孤独终老！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunfeng.online/"/>
  <updated>2019-08-04T05:19:01.622Z</updated>
  <id>http://sunfeng.online/</id>
  
  <author>
    <name>SunFeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer</title>
    <link href="http://sunfeng.online/2019/08/03/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer/"/>
    <id>http://sunfeng.online/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/</id>
    <published>2019-08-03T09:06:43.000Z</published>
    <updated>2019-08-04T05:19:01.622Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Planar-data-classification-with-a-hidden-layer"><a href="#Planar-data-classification-with-a-hidden-layer" class="headerlink" title="Planar data classification with a hidden layer"></a>Planar data classification with a hidden layer</h3><a id="more"></a><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression.</p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a signal hidden layer</li><li>Use units with a non-linear activation function, such as tanh</li><li>Compute the cross entropy loss</li><li>Implement forward and backward propagation</li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data and analysis.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load planar dataset</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @X: your dataset features</span></span><br><span class="line"><span class="string">        @Y: your dataset labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    X, Y = load_planar_dataset()    <span class="comment"># load dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure1.png" alt="Figure1"></p><p>You have:</p><blockquote><ul><li>a numpy array (matrix) X that contains your features (x1, x2)</li><li>a numpy array (vector) Y that contains your labels (red: 0, blue: 1)</li></ul></blockquote><p>Let’s first get a better sense of what our data is like.</p><p><strong>Exercise:</strong> How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code>and <code>Y</code> ?</p><p><strong>Code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m = X.shape[<span class="number">1</span>]  <span class="comment"># training set size</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The shape of X <span class="keyword">is</span>: (<span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">The shape of Y <span class="keyword">is</span>: (<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">I have m = <span class="number">400</span> training examples!</span><br></pre></td></tr></table></figure><hr><h3 id="Simple-Logistic-Regression"><a href="#Simple-Logistic-Regression" class="headerlink" title="Simple Logistic Regression"></a>Simple Logistic Regression</h3><p>Before building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure><p>You can now plot the decision boundary  of these models. Run the code below.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure2.png" alt="Figure2"></p><p><strong>Interpretation:</strong> The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network  will do better. Let’s try this now.</p><hr><h3 id="Neural-Network-model"><a href="#Neural-Network-model" class="headerlink" title="Neural Network model"></a>Neural Network model</h3><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure3.png" alt="Figure3"></p><p><strong>Mathematically:</strong></p><p>For one example x(i):</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure4.png" alt="Figure4"></p><p>Given the predictions on all the examples, you can also compute the cost J as follows:</p><p>$ J = -\frac{1}{m}\sum_{i = 1}^{m}{(y^{(i)}log(a^{<a href="i">2</a>}) + (1 - y^{(i)})log(1 - a^{<a href="i">2</a>}))}    $                            (6)</p><p><strong>Reminder:</strong> The general methodology to build a Neural Network is to:</p><blockquote><ol><li>Define the neural network structure (# of input units, # of hidden units, etc).</li><li>Initialize the model’s parameters</li><li><font color="#0099ff">**Loop:**</font>- <font color="#0099ff">Implement forward propagation</font>- <font color="#0099ff">Compute loss</font>- <font color="#0099ff">Implement backward propagation to get the gradients</font>- <font color="#0099ff">Update parameters (gradient descent)</font></li></ol></blockquote><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learned the right parameters, you can make predictions on new data.</p><h4 id="Defining-the-neural-network-structure"><a href="#Defining-the-neural-network-structure" class="headerlink" title="Defining the neural network structure"></a>Defining the neural network structure</h4><h4 id="Initialize-the-model’s-parameters"><a href="#Initialize-the-model’s-parameters" class="headerlink" title="Initialize the model’s parameters"></a>Initialize the model’s parameters</h4><h4 id="The-Loop"><a href="#The-Loop" class="headerlink" title="The Loop"></a>The Loop</h4>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Planar-data-classification-with-a-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-a-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with a hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with a hidden layer&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera" scheme="http://sunfeng.online/tags/Coursera/"/>
    
      <category term="Neural Network" scheme="http://sunfeng.online/tags/Neural-Network/"/>
    
      <category term="Deep learning" scheme="http://sunfeng.online/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>如何上传本地代码到github</title>
    <link href="http://sunfeng.online/2019/08/02/%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github/"/>
    <id>http://sunfeng.online/2019/08/02/如何上传本地代码到github/</id>
    <published>2019-08-02T08:34:43.000Z</published>
    <updated>2019-08-02T08:57:37.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何上传本地代码到github"><a href="#如何上传本地代码到github" class="headerlink" title="如何上传本地代码到github"></a>如何上传本地代码到github</h2><a id="more"></a><h3 id="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："><a href="#第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：" class="headerlink" title="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："></a>第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure1.jpg" alt="figure1"></p><p>点击<strong>Clone or download</strong>按钮，复制弹出的地址<strong>git@github.com:***/***.git</strong></p><p>注意要用SSH地址。</p><hr><h3 id="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"><a href="#第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令" class="headerlink" title="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"></a>第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><hr><h3 id="第三步：将项目的所有文件添加到仓库中"><a href="#第三步：将项目的所有文件添加到仓库中" class="headerlink" title="第三步：将项目的所有文件添加到仓库中"></a>第三步：将项目的所有文件添加到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><hr><h3 id="第四步：将添加的文件提交到仓库中"><a href="#第四步：将添加的文件提交到仓库中" class="headerlink" title="第四步：将添加的文件提交到仓库中"></a>第四步：将添加的文件提交到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;注释语句&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="第五步：将本地仓库关联到github上"><a href="#第五步：将本地仓库关联到github上" class="headerlink" title="第五步：将本地仓库关联到github上"></a>第五步：将本地仓库关联到github上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:***/test.git</span><br></pre></td></tr></table></figure><hr><h3 id="第六步：上传之前，先要pull一下，执行如下命令："><a href="#第六步：上传之前，先要pull一下，执行如下命令：" class="headerlink" title="第六步：上传之前，先要pull一下，执行如下命令："></a>第六步：上传之前，先要pull一下，执行如下命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><hr><h3 id="第七步：上传代码到github远程仓库"><a href="#第七步：上传代码到github远程仓库" class="headerlink" title="第七步：上传代码到github远程仓库"></a>第七步：上传代码到github远程仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><hr><p><strong>祝你成功！</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;如何上传本地代码到github&quot;&gt;&lt;a href=&quot;#如何上传本地代码到github&quot; class=&quot;headerlink&quot; title=&quot;如何上传本地代码到github&quot;&gt;&lt;/a&gt;如何上传本地代码到github&lt;/h2&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="github" scheme="http://sunfeng.online/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset</title>
    <link href="http://sunfeng.online/2019/08/01/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/"/>
    <id>http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/</id>
    <published>2019-08-01T08:06:21.000Z</published>
    <updated>2019-08-02T07:54:57.378Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a><strong>Logistic Regression with a Neural Network mindset</strong></h3><a id="more"></a><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li><p>Build the general architecture of a learning algorithm, including:</p><ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent)</li></ul></li><li><p>Gather all three functions above into a main function, in the right order.</p></li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li></ul><p>code ————-&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure><hr><h3 id="Overview-of-the-Problem-set"><a href="#Overview-of-the-Problem-set" class="headerlink" title="Overview of the Problem set"></a>Overview of the Problem set</h3><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><blockquote><ul><li>a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0)</li><li>a test set of m_test images labeled as cat or non-cat</li><li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)</li></ul></blockquote><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data preprocessing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_Preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load dataset and preprocess dataset</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @train_set_x: your train set features</span></span><br><span class="line"><span class="string">        @train_set_y: your train set labels</span></span><br><span class="line"><span class="string">        @test_set_x: your test set features</span></span><br><span class="line"><span class="string">        @test_set_y: your test set labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line">    <span class="comment"># load dataset from dataset files</span></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    <span class="comment"># vectorize the features of each examples</span></span><br><span class="line">    train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">    test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line">    <span class="comment"># normalize the features vector</span></span><br><span class="line">    <span class="keyword">return</span> train_set_x, train_set_y, test_set_x, test_set_y, classes</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:**</font><font color="#0099ff">Common steps for pre-processing a new dataset are:</font><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</li></ul><hr><h3 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h3><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure1.png" alt="Figure1"></p><p><strong>Mathematical expression of the algorithm:</strong></p><p>For one examples x(i):</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/Figure2.png" alt="Figure2"></p><p>The cost is the computed by summing over all training examples:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure3.png" alt="Figure3"></p><p><strong>Key steps:</strong> In this exercise, you will carry out the following steps:</p><blockquote><ul><li>Initialize the parameters of the model</li><li>Learn the parameters for the model by minimizing the cost</li><li>Use the learned parameters to make predictions (on the test set)</li><li>Analyze the results and conclude</li></ul></blockquote><hr><h3 id="Building-the-parts-of-our-algorithm"><a href="#Building-the-parts-of-our-algorithm" class="headerlink" title="Building the parts of our algorithm"></a>Building the parts of our algorithm</h3><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features)</li><li>Initialize the model’s parameters</li><li>Loop:<ul><li><strong>Calculate current loss (forward propagation)</strong></li><li><strong>Calculate current gradient (backward propagation)</strong></li><li><strong>Update parameters (gradient descent)</strong></li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call model().</p><h4 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h4><p><strong>Exercise:</strong> using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute </p><script type="math/tex; mode=display">sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}</script><p>to make predictions. Use np.exp().</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @z: A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @s: sigmoid(z)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h4 id="Initializing-parameters"><a href="#Initializing-parameters" class="headerlink" title="Initializing parameters"></a>Initializing parameters</h4><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initializing parameters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @dim: size of the w vector we want</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @w: initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">        @b: initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function propagation() that computes the cost function and its gradient.</p><p><strong>Hints(提示):</strong></p><font color="#0099ff">Forward Propagation:</font><ul><li><p>You get x</p></li><li><p>You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$</p></li><li><p>You calculate the cost function: </p><p>$J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 -  y^{(i)})log(1 - a^{(i)}) }$</p></li></ul><p>Here are the two formulas you will be using:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure4.png" alt="Figure4"></p><p><strong>code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward and Backword propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the</span></span><br><span class="line"><span class="string">    propagation explained above</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @cost: negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">        @dw: gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">        @db: gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># nx</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b))  <span class="comment"># compute activation</span></span><br><span class="line">    cost = -(np.dot(Y, np.log(A).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A).T)) / m <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m <span class="comment"># compute dw</span></span><br><span class="line">    db = np.sum(A - Y) / m      <span class="comment"># compute db</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 把shape中为1的维度去掉</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    <span class="comment"># 判断剩下的是否为空</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, </span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Optimization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @num_iterations: number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        @learning_rate: learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        @print_cost: True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">        @grads: dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">        @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">        You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        (1) Calculate the cost and the gradient for the current parameters. Use propagate()</span></span><br><span class="line"><span class="string">        (2) Update the parameters using gradient descent rule for w and b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update rule</span></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %d: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li>Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$</li><li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_Prediction</code>. </li></ol><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Graded function: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned </span></span><br><span class="line"><span class="string">    logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @Y_prediction: a numpy array (vector) containing all prediction</span></span><br><span class="line"><span class="string">                       (0 / 1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m =  X.shape[<span class="number">1</span>]     <span class="comment"># number of examples</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b)) <span class="comment"># (1, m)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0, i] to actual predictions p[0, i]</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:** You've implemented several functions that:</font><ul><li>initialize (w, b)</li><li>Optimize the loss iteratively to learn parameters (w, b):<ul><li>computing the cost and its gradient</li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w, b) to predict the labels for a given set of examples</li></ul><hr><h3 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h3><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p><blockquote><ul><li>Y_prediction for your predictions on the test set</li><li>Y_prediction_train for your predictions on the train set</li><li>w, costs, grads for the outputs of optimize()</li></ul></blockquote><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all functions into a model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function</span></span><br><span class="line"><span class="string">    you have implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">        @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">        @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">        @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test)</span></span><br><span class="line"><span class="string">        @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">        @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">        @print_cost: Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @d: dictionary containing information about the model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initailize parameters with zeros</span></span><br><span class="line">    w,b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])   <span class="comment"># num_px * num_px * 3, w: (dim, 1), b: a scalar</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples </span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span> : costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span> : Y_prediction_test,</span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">"w"</span> : w,</span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span> : num_iterations</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =<span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.376007</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.331463</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.303273</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.279880</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.260042</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.242941</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.228004</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.214820</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.203078</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.192544</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.183033</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.174399</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.166521</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.159305</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.152667</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.146542</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure><p><strong>Comment:</strong> Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. </p><font color="#0099ff">Let's also plot the cost function and the gradients:</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure5.png" alt="Figure5"></p><p><strong>Interpretation</strong>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p><hr><h3 id="Further-analysis"><a href="#Further-analysis" class="headerlink" title="Further analysis"></a>Further analysis</h3><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α.</p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow= <span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure6.png" alt="Figure6"></p><p><strong>Interpretation</strong>:</p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you:<ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li></ul></li></ul><hr><p><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment2_logistics_regression" target="_blank" rel="noopener">Source Code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic-Regression-with-a-Neural-Network-mindset&quot;&gt;&lt;a href=&quot;#Logistic-Regression-with-a-Neural-Network-mindset&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression with a Neural Network mindset&quot;&gt;&lt;/a&gt;&lt;strong&gt;Logistic Regression with a Neural Network mindset&lt;/strong&gt;&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Neural Networks" scheme="http://sunfeng.online/tags/Neural-Networks/"/>
    
      <category term="Deep Learning" scheme="http://sunfeng.online/tags/Deep-Learning/"/>
    
      <category term="Logistic Regression" scheme="http://sunfeng.online/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Hexo教程：使用Hexo写博客</title>
    <link href="http://sunfeng.online/2019/07/31/hello-hexo-markdown/"/>
    <id>http://sunfeng.online/2019/07/31/hello-hexo-markdown/</id>
    <published>2019-07-31T12:44:47.000Z</published>
    <updated>2019-08-02T08:54:22.531Z</updated>
    
    <content type="html"><![CDATA[<p>尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。<br><a id="more"></a></p><hr><h3 id="新建文档"><a href="#新建文档" class="headerlink" title="新建文档"></a>新建文档</h3><p>假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可：</p><blockquote><p>$ hexo new “hello hexo markdown”</p></blockquote><p>上述命令的结果是在 <code>./hexo/source/_posts</code> 路径下新建了一个 <code>hello-hexo-markdown.md</code> 文件。</p><p>然后，我们就可以打开编辑器尽情地写作了。</p><hr><h3 id="文档格式"><a href="#文档格式" class="headerlink" title="文档格式"></a>文档格式</h3><p>我们使用文本编辑器打开刚刚新建的 <code>hello-hexo-markdown.md</code> 文件，会发现其中已经存在内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2019-07-31 20:44:47</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 <code>---</code> 分隔的区域。其中，</p><p><code>title</code>的值是当前文档名，也是将来在网页中显示的文章标题</p><p><code>date</code>值是我们新建文档时的当时地区时间</p><p><code>tags</code>值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>上面的配置参数为这篇文档贴上了 <code>hello</code>、<code>hexo</code>、<code>mardown</code> 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。</p><p>除了以上这些，还有很多预先定义的参数<a href="https://hexo.io/zh-cn/docs/front-matter.html" target="_blank" rel="noopener">Front_mtter</a>，我们这里选取一个常用且较为典型的配置参数<code>categories</code>讲解一下。</p><h4 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h4><p><code>categories</code> 是用来给文章分类的，它跟 <code>tags</code> 不同的是其具有顺序性和层次性。</p><p>例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，<code>categories</code> 有点儿像新建文件夹对文档进行分门别类的归置。</p><p><code>categories</code> 的用法同 <code>tags</code> 一样，只不过斗个 categories 值是分先后顺序的。</p><hr><h3 id="引用资源"><a href="#引用资源" class="headerlink" title="引用资源"></a>引用资源</h3><p>写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。</p><p>这时，有两种解决办法：</p><ol><li>使用绝对路径引用资源，在 Web 世界中就是资源的 URL</li><li>使用相对路径引用资源</li></ol><h4 id="文章资料文件夹"><a href="#文章资料文件夹" class="headerlink" title="文章资料文件夹"></a>文章资料文件夹</h4><p>如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。</p><p>使用文本编辑器打开站点根目录下的 <code>_ config.yml</code> 文件，将 <code>post_asset_folder</code> 值设置为 <code>true</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 <code>hexo new &lt;title&gt;</code> 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。</p><h4 id="相对路径引用的标签插件"><a href="#相对路径引用的标签插件" class="headerlink" title="相对路径引用的标签插件"></a>相对路径引用的标签插件</h4><p>通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>比如说：当你打开文章资源文件夹功能后，你把一个 <code>example.jpg</code> 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 <code>![](/example.jpg)</code> ，它将 <em>不会</em> 出现在首页上。（但是它会在文章中按你期待的方式工作）</p><p><strong>！！！注意：</strong> 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 <code>./资源名称</code>，不用在引用路径中添加同名文件夹目录层级。</p><p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img example.jpg This is an example image %&#125;</span><br></pre></td></tr></table></figure><p>通过这种方式，图片将会同时出现在文章和主页以及归档页中。</p><hr><h3 id="文章摘要"><a href="#文章摘要" class="headerlink" title="文章摘要"></a>文章摘要</h3><p>有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。</p><p>所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。</p><p>设置文章摘要，我们只需在想显示为摘要的内容之后添 <code>&lt;!-- more --&gt;</code> 即可。像下面这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br><span class="line">我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧</span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line">紧接着文章摘要的正文内容</span><br></pre></td></tr></table></figure><p>这样，<code>&lt;!-- more --&gt;</code> 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。</p><p>注意！文章摘要在文章详情页是正文中最前面的内容。</p><hr><h3 id="生成文件"><a href="#生成文件" class="headerlink" title="生成文件"></a>生成文件</h3><h4 id="清除缓存文件"><a href="#清除缓存文件" class="headerlink" title="清除缓存文件"></a>清除缓存文件</h4><p>为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><p>上述命令会清除在本地站点文件下的缓存文件(<code>db.json</code>)和已有的静态文件(<code>public</code>).</p><h4 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h4><p>写好MarkDown文档之后，我们就可以使用以下命令生成静态文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>然后我们就可以启动 Hexo 服务器，使用浏览器打开 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000</a> 查看效果了。</p><hr><h3 id="示范"><a href="#示范" class="headerlink" title="示范"></a>示范</h3><p>下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。</p><p><img src="/2019/07/31/hello-hexo-markdown/example.png" alt="效果截图"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="hello" scheme="http://sunfeng.online/tags/hello/"/>
    
      <category term="hexo" scheme="http://sunfeng.online/tags/hexo/"/>
    
      <category term="markdown" scheme="http://sunfeng.online/tags/markdown/"/>
    
  </entry>
  
</feed>
