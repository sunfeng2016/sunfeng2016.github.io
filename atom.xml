<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunFeng&#39;s Blog</title>
  
  <subtitle>学习，敲码，孤独终老！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunfeng.online/"/>
  <updated>2019-08-14T05:02:55.217Z</updated>
  <id>http://sunfeng.online/</id>
  
  <author>
    <name>SunFeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（四）-- 决策树</title>
    <link href="http://sunfeng.online/2019/08/13/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89--%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://sunfeng.online/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/</id>
    <published>2019-08-13T08:43:48.000Z</published>
    <updated>2019-08-14T05:02:55.217Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（四）-决策树"><a href="#《机器学习》西瓜书学习笔记（四）-决策树" class="headerlink" title="《机器学习》西瓜书学习笔记（四） 决策树"></a>《机器学习》西瓜书学习笔记（四） 决策树</h2><a id="more"></a><h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><p><strong>决策树 (decision tree)：</strong></p><ul><li>一类常见的机器学习算法</li><li>决策树是基于树形结构进行决策的</li><li>决策树学习的目的是产生一棵泛化能力强，即处理未见示例能力强的决策树</li></ul><p><strong>基本流程：</strong>决策树基本流程遵循简单直观的“分而治之”策略，如下图所示：</p><p><img src="/2019/08/13/《机器学习》西瓜书学习笔记（四）-- 决策树/figure1.PNG" alt="figure1"></p><p><strong>在决策树基本算法中，有三种情形会导致递归返回：</strong></p><ul><li>当前结点包含的样本全属于同一类别，无需划分</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分<ul><li>将当前结点标记为叶子结点</li><li>并将其类别设定为该结点所含样本最多的类别</li></ul></li><li>当前结点包含的样本集合为空，不能划分<ul><li>将当前结点标记为叶子结点</li><li>并将类别设定为其父节点所含样本最多的类别</li></ul></li></ul><hr><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><p>决策树学习的关键是 <strong>如何选择最优划分属性</strong>，<font color="#0099ff">一般而言，随着划分过程的不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。</font> </p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p><strong>信息熵 (information entropy):</strong></p><ul><li><p>信息熵是度量样本集合<strong>纯度</strong>最常用的一种指标。</p></li><li><p>假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k (k = 1, 2, …, |y|)$, 则 $D$ 的信息熵定义为：</p><script type="math/tex; mode=display">Ent(D) = - \sum _{k = 1}^{|y|}p_k\log_2p_k</script><p><strong>$Ent(D)$ 的值越小，则 $D$ 的纯度越高</strong></p></li></ul><p><strong>信息增益 (information gain):</strong></p><ul><li><p>假设离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, … , a^V\}$</p></li><li><p>使用 $a$ 对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$.</p><script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum _{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v).</script><p><strong>信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的 “纯度提升”越大</strong>。</p></li></ul><p><strong>以信息增益为准则选择划分属性：</strong></p><script type="math/tex; mode=display">a_* = arg\ max Gain(D, a)</script><h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4><p><em>信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性。</em></p><p><strong>增益率 (gain ratio):</strong></p><script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D, a)}{IV(a)}</script><p>其中：</p><script type="math/tex; mode=display">IV(a) = - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}</script><p>称为属性 $a$ 的 “固有值”</p><h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p><em>GART 决策树使用 “基尼指数”来选择划分属性</em></p><p><strong>基尼值：</strong>用来度量数据集 $D$ 的纯度</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gini(D) &= \sum_{k=1}^{|y|}\sum_{k\prime \ne k} p_kp_{k\prime}\\&= 1 - \sum_{k=1}^{|y|}{p_k^2}\end {split}\end {equation}</script><p><strong>$Gini(D)$ 越小，则数据集 $D$ 的纯度越高</strong></p><p><strong>基尼指数 (Gini index):</strong></p><script type="math/tex; mode=display">Gini\_index(D, a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)</script><hr><h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><p><strong>剪枝 (pruning):</strong> 决策树学习算法对付“过拟合”的主要手段</p><h4 id="预剪枝-pre-pruning"><a href="#预剪枝-pre-pruning" class="headerlink" title="预剪枝 (pre-pruning)"></a>预剪枝 (pre-pruning)</h4><ul><li>指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分并将当前结点标记为叶结点</li><li>具体示例详见课本 <em>P-80</em></li></ul><h4 id="后剪枝-post-pruning"><a href="#后剪枝-post-pruning" class="headerlink" title="后剪枝 (post-pruning)"></a>后剪枝 (post-pruning)</h4><ul><li>先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应子树替换成叶结点能带来决策树泛化能力的提升，则将该子树替换为叶结点</li><li>具体示例详见课本 <em>P-82</em></li></ul><hr><h3 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h3><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p><em>常用的将连续属性离散化技术是二分法策略。</em></p><p><strong>二分法 (bi-partition):</strong></p><ul><li><p>给定样本集 $D$ 和连续属性 $a$ , 假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值</p></li><li><p>将这些值从小到大进行排序，记为 $\{a^1, a^2,…,a^n\}$.</p></li><li><p>基于划分点 $t$, 将 $D$ 分为两个子集 $D_t^-$ 和 $D_t^+$ </p></li><li><p>其中 $D_t^-$ 包含那些在属性 $a$ 取值不大于 $t$ 的样本，而 $D_t^+$ 则包含那些在属性 $a$ 取值大于 $t$ 的样本</p></li><li><p>对于相邻属性取值 $a^i$ 和 $a^{i+1}$ 来说，$t$ 在区间 $[a^i, a^{i+1})$ 中取任意值所产生的划分结果相同</p></li><li><p>因此，对于连续属性 $a$, 我们可考察包含 $n-1$ 个元素的候选划分点集合：</p><script type="math/tex; mode=display">T_a = \{\frac{a^i + a^{i+1}}{2} \ |\ 1 \le i \le n-1 \}</script><p>即，以区间 $[a^i, a^{i+1})$ 的中位点 $\frac{a^i + a^{i+1}}{2}$ 作为候选划分点</p></li><li><p>选取最优划分点进行样本集合划分：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}Gain(D, a) &= max_{t \in T_a} Gain(D,a,t) \\&= max_{t \in Ta} Ent(D) - \sum_{\lambda \in \{-, +\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)\end {split}\end {equation}</script><p>其中 $Gain(D, a, t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益</p></li><li><p>于是，我们就可选出使  $Gain(D, a, t)$ 最大化的划分点</p></li></ul><h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><hr><h3 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（四）-决策树&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（四）-决策树&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（四） 决策树&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（四） 决策树&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（三）-- 线性模型</title>
    <link href="http://sunfeng.online/2019/08/10/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://sunfeng.online/2019/08/10/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/</id>
    <published>2019-08-10T06:54:48.000Z</published>
    <updated>2019-08-13T08:14:38.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（三）线性模型"><a href="#《机器学习》西瓜书学习笔记（三）线性模型" class="headerlink" title="《机器学习》西瓜书学习笔记（三）线性模型"></a>《机器学习》西瓜书学习笔记（三）线性模型</h2><a id="more"></a><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><h4 id="1-回顾各符号对应的概念"><a href="#1-回顾各符号对应的概念" class="headerlink" title="1. 回顾各符号对应的概念"></a>1. 回顾各符号对应的概念</h4><p><strong>示例：</strong> $\boldsymbol x = (x_1; x_2; … ; x_d)$</p><p><strong>属性：</strong> $x_i$ </p><p><strong>属性个数(维度)：</strong> $d$ </p><p><strong>数据集：</strong> $D = \{\boldsymbol x_1, \boldsymbol x_2, …, \boldsymbol x_m\}$ </p><p><strong>带标记的数据集：</strong> $D = \{ (\boldsymbol x_1, y_1),  (\boldsymbol x_2, y_2), … , (\boldsymbol x_m, y_m)\}$ </p><p><strong>模型：</strong> $f$ </p><p><strong>模型在示例 $x$ 上的预测输出：</strong>  $f(\boldsymbol x)$</p><h4 id="2-线性模型-linear-model"><a href="#2-线性模型-linear-model" class="headerlink" title="2. 线性模型 (linear model)"></a>2. 线性模型 (linear model)</h4><p><strong>线性模型的目的：</strong> 试图学得一个【通过属性的线性组合来进行预测】的【函数】</p><p><strong>线性模型的本质：</strong> 学得一个线性函数</p><p><strong>线性模型的特征：</strong> 通过属性的线性组合来进行预测，即：</p><script type="math/tex; mode=display">f(\boldsymbol x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b</script><p>向量形式为:</p><script type="math/tex; mode=display">f(x) = \boldsymbol w^T\boldsymbol x + b</script><p>其中 $\boldsymbol w = (w1;w2;…;wd)$ ，<font color="#0099ff">由于w直观的表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性。</font></p><hr><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="1-线性回归-linear-regression"><a href="#1-线性回归-linear-regression" class="headerlink" title="1. 线性回归 (linear regression)"></a>1. 线性回归 (linear regression)</h4><p>给定数据集 $D = {(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), … , (\boldsymbol x_m, y_m)}$ , 其中 $\boldsymbol x_i = (x_{i1}; x_{i2}; … x_{id}), y_i \in R$ .</p><p><strong>线性回归的目的：</strong> 试图学得一个【线性模型】以尽可能准确地【预测实值输出标记】</p><p><strong>线性回归的本质：</strong> 学得线性模型</p><p><strong>线性模型的作用：</strong> 预测实值输出标记</p><p><strong>线性模型的目的：</strong> 试图学得一个【通过属性的线性组合来进行预测】的【函数】</p><p><strong><font color="#0099ff">总的来讲，线性回归是一个函数，通过属性的线性组合来进行预测，尽可能准确地预测实值输出标记。</font></strong></p><p><strong>线性回归的分类：</strong></p><ul><li><font color="#0099ff">一元线性回归分析：</font>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，则称为一元线性回归分析</li><li><font color="#0099ff">多元线性回归分析：</font>回归分析中，包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析</li></ul><h4 id="2-最小二乘法-least-square-method"><a href="#2-最小二乘法-least-square-method" class="headerlink" title="2. 最小二乘法 (least square method)"></a>2. 最小二乘法 (least square method)</h4><p><strong>最小二乘法：</strong> 基于均方误差最小化来进行模型求解的方法，均方误差对应了欧氏距离</p><ul><li>所谓“二乘”，就是用平方来度量观测点与估计点的远近</li><li>所谓“最小”，是指参数的估计值要保证各个观测点与估计点的距离平方和达到最小</li></ul><p><strong>最小二乘法的目的：</strong> 在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到这条直线上的欧式距离之和最小</p><p><strong>进一步解释：</strong> <font color="#0099ff">最小二乘法是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简单的求得未知数据，并使得这些求得的数据与实际数据之间误差的平方和最小。</font></p><p><strong>在线性回归中的误差平方和：</strong></p><ul><li>如果说误差就是预测点 $f(x_i)$ 到标记点 $y_i$ 的距离</li><li><p>那么均方误差 $E(f;D) = \frac{1}{m} \sum_{i = 1}^{m} {(f(x_i)-y_i)^2}$ 则可以体现误差的平方和</p><font color="#0099ff">这就意味着，线性回归需要最小化均方误差</font></li></ul><h4 id="3-属性数目为1的简单例子-一元线性回归"><a href="#3-属性数目为1的简单例子-一元线性回归" class="headerlink" title="3. (属性数目为1的简单例子)一元线性回归"></a>3. (属性数目为1的简单例子)一元线性回归</h4><p>我们先考虑一种最简单的情形：输入属性的数目只有一个</p><p>此时数据集为：$D = \{(x_1, y_1), (x_2, y_2), … , (x_m, y_m)\}$</p><p><strong>线性回归模型试图学得：</strong></p><script type="math/tex; mode=display">f(xi) = wx_i + b, 使得f(xi) \approx y_i</script><p><strong>如何确定 $w$ 和 $b$：</strong> 最小二乘法，试图让均方误差最小化，即：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}(w^*, b^*) &= arg \ min \ E_{(w, b)}\\&= arg \ min \ \sum_{i = 1}^{m}{(f(x_i) - y_i)^2}\\&= arg \ min \ \sum_{i = 1}^{m}{(y_i - wx_i - b)^2}\\\end{split}\end{equation}</script><p><strong>参数估计：</strong> <font color="#0099ff">求解w和b使均方误差最小化的过程，称为线性回归模型的最小二乘“参数估计”（parameter estimation）</font></p><p>根据函数知识可知，一般 U形曲线的函数，如$f(x) = x^2$ ，通常都是凸函数，其最小值点一般在函数的极小值点处，也就是偏导数为0的点。</p><p>所以，我们可以将 $E(w,b)$ 分别对 $w$ 和 $b$ 求偏导，并令偏导等于0，即可得到 $w$ 和 $b$ 的最优解，具体过程如下：</p><ol><li><p>将 $E(w,b)$ 分别对 $w$ 和 $b$ 求偏导</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{\partial{E_{(w,b)}}}{\partial w} &= 2(w\sum_{i = 1}^{m}{x_i^2} - \sum_{i = 1}^{m}{(y_i - b)x_i}) \\\frac{\partial{E_{(w,b)}}}{\partial b} &= (mb - \sum_{i = 1}^{m}{(y_i-wx_i)}) \\\end{split}\end{equation}</script></li><li><p>令偏导为零, 得到得到 $w$ 和 $b$ 的最优解</p><script type="math/tex; mode=display">\begin{equation}\begin{split}w &= \frac{\sum_{i=1}^{m}{y_i(x_i - \overline x)}}{\sum_{i = 1}^{m}{x_i^2}-\frac{1}{m}{(\sum_{i=1}^{m}{x_i})^2}} \\b &= \frac{1}{m} \sum_{i=1}^{m}{(y_i - wx_i)^2} \\\end{split}\end{equation}</script><p>其中 $\overline x = \frac{1}{m} \sum_{i=1}^{m}x_i$ 为 $x$ 的均值。</p></li></ol><h4 id="4-属性数目为d的复杂例子-多元线性回归"><a href="#4-属性数目为d的复杂例子-多元线性回归" class="headerlink" title="4. (属性数目为d的复杂例子)多元线性回归"></a>4. (属性数目为d的复杂例子)多元线性回归</h4><p>现实中我们几乎碰不见属性值个数为1的例子，通常情况下，样本由d个属性描述。</p><p>此时，<strong>示例</strong>还是： $\boldsymbol x = (x_1; x_2; … ; x_d)$，<strong>数据集</strong>还是：$D = \{ (\boldsymbol x_1, y_1),  (\boldsymbol x_2, y_2), … , (\boldsymbol x_m, y_m)\}$ </p><p><strong>线性回归模型试图学得：</strong> </p><script type="math/tex; mode=display">f(\boldsymbol xi) = w\boldsymbol x_i + b, 使得f(\boldsymbol xi) \approx y_i</script><p><strong>如何确定 w 和 b：</strong> 类似的，可以用最小二乘法来对 $w$ 和 $b$ 进行估计，具体过程如下：</p><ol><li><p>将$w$ 和 $b$ 吸入向量形式 $\hat w = (w; b)$ </p></li><li><p>把数据集 $D$ 表示成一个 $m \times (d + 1)$ 大小的矩阵 $\boldsymbol X$，其中每行对应一个示例，该行前个 $d$ 元素对应于示例的 $d$ 个属性值，最后一个元素恒置为1，即：</p><script type="math/tex; mode=display">\boldsymbol X = \begin{pmatrix}x_{11} & x_{12} & \ldots & x_{1d} & 1 \\x_{21} & x_{22} & \ldots & x_{2d} & 1 \\\vdots & \vdots & \ddots & \vdots & \vdots & \\x_{m1} & x_{m2} & \ldots & x_{md} & 1 \end{pmatrix} =\begin{pmatrix}\boldsymbol x_1^T & 1 \\\boldsymbol x_2^T & 1 \\\vdots & \vdots \\\boldsymbol x_m^T & 1 \end{pmatrix}</script></li><li><p>把标记写成向量形式：$y = (y_1; y_2; …;y_m)$ </p></li><li><p>求预测的误差平方和：$E_{\hat w} = (y - \boldsymbol X \hat w)^T(y - \boldsymbol X \hat w)$ </p></li><li><p>最小化均方误差：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}\hat w^* &= arg\ min \ E_{\hat w}\\&= arg\ min \ (y - \boldsymbol X \hat w)^T(y - \boldsymbol X \hat w)\end{split}\end {equation}</script></li><li><p>将 $E_{\hat w}$ 对 $\hat w$ 求导得到:</p><script type="math/tex; mode=display">\frac{\partial E_{\hat w}}{\partial {\hat w}} = 2 \boldsymbol X^T(\boldsymbol X \hat w - y)</script></li><li><p>然后分别考虑特殊情况和一般情况：</p><ul><li><p>特殊情况：$\boldsymbol X^T \boldsymbol X$ 为满秩矩阵或正定矩阵时，令偏导值为0，可得 $\boldsymbol {\hat w ^*} = (\boldsymbol X^T \boldsymbol X)^{-1}\boldsymbol X^T y$ , 令 $\boldsymbol {\hat x_i} = (\boldsymbol x_i; 1)$ ，则最终学得的多元线性回归模型为：</p><script type="math/tex; mode=display">f(\hat x_i) = \hat x_i^T  (\boldsymbol X^T \boldsymbol X)^{-1}\boldsymbol X^T y.</script></li><li><p>一般情况：实际上，$\boldsymbol X^T \boldsymbol X$ 一般都不是满秩矩阵，此时可以解出来多个 $\boldsymbol {\hat w}$ , 它们都能使均方误差最小化，选择哪一个作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。</p></li></ul></li></ol><h4 id="5-线性模型的丰富变化"><a href="#5-线性模型的丰富变化" class="headerlink" title="5. 线性模型的丰富变化"></a>5. 线性模型的丰富变化</h4><ul><li><p><strong>线性回归模型：</strong> </p><p>将线性模型的预测值逼近真实标记 $y$, 即：$y = w^T \boldsymbol x + b$ </p></li><li><p><strong>对数线性回归：</strong> </p><p>将线性模型的预测值逼近真实标记 $y$ 的衍生物，如对数：$\ln(y) = w^T \boldsymbol x + b$ </p></li><li><p><strong>广义线性模型：</strong> </p><p>更一般地，考虑单调可微函数 $g(.)$, 可以得到更多的真实标记 $y$ 的衍生物，令 $g(y) = w^T \boldsymbol x + b$, 即 $y = g^{-1}{(w^T \boldsymbol x + b)}$, 这样得到的模型称为广义线性模型，其中 $g(.)$ 称为 “联系函数”。</p></li></ul><hr><h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p><strong>预测：</strong> 依靠机器学习学得的模型，对新的示例进行结果判断</p><ul><li><strong>回归任务：</strong> 预测的是连续值</li><li><strong>分类任务：</strong> 预测的是离散值</li></ul><p>如果说说线性模型预测连续值，只需要让预测值逼近真实标记 $y$ 或其衍生物的话，当预测离散值的时候，如何让线性模型的预测值 (连续) 和真实标记值 $y$ (离散) 联系起来呢？</p><p>其实，离散状态的真实标记 $y$, 未尝不可以有一种连续的衍生物 $z$, 这样通过单调可微的联系函数 $g(.)$ , 就可以让连续的预测值联系离散的真实标记。</p><h4 id="1-单位阶跃函数-unit-step-function"><a href="#1-单位阶跃函数-unit-step-function" class="headerlink" title="1. 单位阶跃函数 (unit-step function)"></a>1. 单位阶跃函数 (unit-step function)</h4><p>在二分类任务当中，输出标记为 $y = \{0, 1\}$ , 而线性回归模型产生的预测值：$z = \boldsymbol w^T \boldsymbol x + b$   是实值。要将连续值 $z$ 转换为离散值 0/1，最理想的是 “单位阶跃函数” </p><script type="math/tex; mode=display">\begin {equation}y = \begin {cases}0, & z < 0;\\0.5 & z = 0; \\1 & z > 0;\end {cases}\end {equation}</script><p><img src="/2019/08/10/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure.PNG" alt="Figure1"></p><h4 id="2-对数几率函数-logistic-function"><a href="#2-对数几率函数-logistic-function" class="headerlink" title="2. 对数几率函数 (logistic function)"></a>2. 对数几率函数 (logistic function)</h4><p>单位阶跃函数虽好，但是不可微，不是理想的联系函数 $g(.)$ , 我们希望找到一个在形态上趋近于单位阶跃函数的连续函数，并且是单调可微的。对数几率函数正是这样的一个函数：</p><script type="math/tex; mode=display">y = \frac {1}{1 + e^{-z}}</script><p><img src="https://pic1.zhimg.com/80/v2-433cdde5a09ff2450cb99383ad8211b8_hd.jpg" alt="img"></p><p>将 $ z = \boldsymbol w^T \boldsymbol x + b$ 带入可得 $y = \frac {1}{1 + e^{-(  \boldsymbol w^T \boldsymbol x + b)}}$ , 亦即 $\ln \frac{y}{1-y} =   \boldsymbol w^T \boldsymbol x + b$. </p><p>如果说将 $y$ 看作样本 $x$ 作为正例的可能性，那么 $1-y$ 就是其成反例的可能性。</p><p><strong>几率：</strong> 两者的比值即为 “几率”，反映了样本作为正例的相对可能性</p><script type="math/tex; mode=display">几率 = \frac {正例可能性}{反例可能性} = \frac{y}{1-y}</script><p><strong>对数几率：</strong> 对几率取对数就可得到 “对数几率”</p><script type="math/tex; mode=display">对数几率 = \ln \frac{y}{1-y}</script><p><strong>对数几率回归：</strong> 用线性模型的预测结果去逼近真实标记的对数几率，其对应的模型叫做“对数几率回归”(logistic regression，亦称逻辑回归)</p><hr><h3 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析 (LDA)"></a>线性判别分析 (LDA)</h3><p><strong>线性判别分析 (Linear Discriminant Analysis, LDA)</strong> 是一种经典的用于解决二分类问题的线性学习方法。</p><h4 id="1-二分类任务中的LDA"><a href="#1-二分类任务中的LDA" class="headerlink" title="1. 二分类任务中的LDA"></a>1. 二分类任务中的LDA</h4><p><strong>LDA大概分为三个步骤：</strong></p><ul><li>给定样例</li><li>寻找一条满足 “同类近，异类远”的投影直线</li><li>新样本的分类依靠投影后点的位置来确定</li></ul><p><img src="https://pic3.zhimg.com/80/v2-cdae5065290f57c9fc387b7d05b5b68a_hd.jpg" alt="img"></p><p><strong>第一步：给定样例：</strong></p><p>给定数据集 $D = \{(\boldsymbol x_i, y_i)\}_{i = 1}^{m},  y_i \in \{0, 1\}$, 令 $\boldsymbol X_j$、$\boldsymbol \mu_j$、$\boldsymbol \sum_j$ 分别表示第 $j \in \{0, 1\}$ 类示例的集合、均值向量、协方差矩阵。将数据投影到直线 $\boldsymbol w$ 上，则两类样本的 <strong>样本中心</strong> 在直线上的投影分别为 $\boldsymbol w^T \boldsymbol \mu_0$ 和$\boldsymbol w^T \boldsymbol \mu_1$，两类样本的协方差矩阵分别为 $\boldsymbol w^T \boldsymbol \sum_0 \boldsymbol w$ 和 $\boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$，由于直线是一维空间，因此 $\boldsymbol w^T \boldsymbol \mu_0$ 、$\boldsymbol w^T \boldsymbol \mu_1$、 $\boldsymbol w^T \boldsymbol \sum_0 \boldsymbol w$ 和、$\boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$均为实数。</p><font color="#0099ff">顺便一提，如上图，横轴坐标分别为 x1 和 x2，代表样本的两个属性，此图代表属性个数为2时张成的二维空间。但当属性个数为n时，属性空间也为n维，只不过无法在图中体现了。 </font><p><strong>第二步：寻找投影直线：</strong></p><p><strong>两个原则：</strong> </p><ul><li><strong>同类近：</strong> 欲使得同类投影点尽可能近，可以让异类样例投影点的协方差尽可能小，即 $\boldsymbol w^T \boldsymbol \sum _0 \boldsymbol w + \boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w$ 尽可能小</li><li><strong>异类远：</strong> 欲使得异类投影点尽可能远离，可以让类中心之间的距离尽可能大，即 $||\boldsymbol w^T \boldsymbol \mu_0 - \boldsymbol w^T \boldsymbol \mu_1||_2^2$ 尽可能大</li></ul><p><strong>广义瑞利商：</strong> 同时考虑二者，得到最大化目标，即：</p><script type="math/tex; mode=display">\begin {equation}\begin {split}J &= \frac{||\boldsymbol w^T \boldsymbol \mu_0 - \boldsymbol w^T \boldsymbol \mu_1||_2^2}{\boldsymbol w^T \boldsymbol \sum _0 \boldsymbol w + \boldsymbol w^T \boldsymbol \sum_1 \boldsymbol w}\\&= \frac{\boldsymbol w^T(\boldsymbol \mu_0 - \boldsymbol \mu_1)(\boldsymbol \mu_0 - \boldsymbol \mu_1)^T\boldsymbol w}{\boldsymbol w^T (\boldsymbol \sum_0 + \boldsymbol \sum_1) \boldsymbol w}\end {split}\end {equation}</script><p><strong>类内散度矩阵：</strong> </p><script type="math/tex; mode=display">\begin {equation}\begin {split}\boldsymbol S_w &= \boldsymbol \sum _0 + \boldsymbol \sum _1 \\& = \boldsymbol \sum_{x \in X_0}(\boldsymbol x - \boldsymbol \mu_0)(\boldsymbol x - \boldsymbol \mu_0)^T + \boldsymbol \sum_{x \in X_1}(\boldsymbol x - \boldsymbol \mu_1)(\boldsymbol x - \boldsymbol \mu_1)^T\end {split}\end {equation}</script><p><strong>类间散度矩阵：</strong> </p><script type="math/tex; mode=display">\boldsymbol S_b = (\boldsymbol \mu _0 - \boldsymbol \mu_1) (\boldsymbol \mu _0 - \boldsymbol \mu_1)^T</script><p><strong>重写得：</strong></p><script type="math/tex; mode=display">J = \frac{\boldsymbol w^T \boldsymbol S_b \boldsymbol w}{\boldsymbol w^T \boldsymbol S_w \boldsymbol w}</script><p><strong>确定 $\boldsymbol  w$ :</strong></p><p>$\boldsymbol J$ 的分子分母都是关于 $\boldsymbol w$ 的二次项，因此  $\boldsymbol J$  的解与  $\boldsymbol w$  的长度无关，只与其方向有关。</p><p>故由拉格朗日乘子法，可列 $\boldsymbol S_b \boldsymbol w = \lambda \boldsymbol S_w \boldsymbol w $，又由于 $\boldsymbol S_b \boldsymbol w$ 方向恒为 $(\boldsymbol \mu _0 - \boldsymbol \mu_1) $，令 $\boldsymbol S_b \boldsymbol w=\lambda(\boldsymbol \mu _0 - \boldsymbol \mu_1) $，带入得 $\boldsymbol w = \boldsymbol S _w ^{-1}(\boldsymbol \mu _0 - \boldsymbol \mu_1) $</p><h4 id="2-多分类任务中的LDA"><a href="#2-多分类任务中的LDA" class="headerlink" title="2. 多分类任务中的LDA"></a>2. 多分类任务中的LDA</h4><hr><h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><p><strong>多分类学习的两个思路：</strong> </p><ul><li><strong>一是将二分类学习方法直接推广到多分类</strong>，如LDA</li><li><strong>二是基于某些策略，利用二分类学习器来解决多分类问题</strong></li></ul><h4 id="拆解法和拆分策略"><a href="#拆解法和拆分策略" class="headerlink" title="拆解法和拆分策略"></a>拆解法和拆分策略</h4><p><strong>拆解法：</strong> 将多分类任务拆解成为若干个二分类任务求解</p><p><strong>拆解步骤：</strong> </p><ul><li>通过拆分策略对问题进行<strong>【拆分】</strong></li><li>为拆分出的每个二分类任务<strong>【训练】</strong>一个分类器</li><li>对各个分类器的结果进行<strong>【集成】</strong>，以获得多分类结果</li></ul><p><strong>拆分策略：</strong></p><ul><li>“一对一 (OvO)”</li><li>“一对其余 (OvR)”</li><li>“多对多 (MvM)”</li></ul><p>假设多分类学习有 $\boldsymbol N$ 个类别 $\boldsymbol C_1, \boldsymbol C_2, … \boldsymbol C_N$, 给定数据集 $ D = \{(\boldsymbol x_1, y_1),(\boldsymbol x_2, y_2),…,(\boldsymbol x_m, y_m)\}$, $y_i \in \{C_1, C_2, … ,C_n\}$ </p><h5 id="OvO"><a href="#OvO" class="headerlink" title="OvO:"></a>OvO:</h5><ul><li>将 $N$ 个分类分别两两配对，从而 <strong>【拆分】</strong> 成 $N(N-1)/2$ 个二分类任务</li><li><strong>【训练】</strong> 时为了区分 $C_i$ 和 $C_j$ 这两个分类，这 $N(N-1)/2$ 个分类器的一个将 $C_i$ 作为正例，$C_j$ 作为反例</li><li>测试时将新样本同时提交给所有分类器，将得到 $N(N-1)/2$ 个分类结果，<strong>【集成】</strong> 的方法是通过投票在这些结果中选出最终结果</li></ul><h5 id="OvR"><a href="#OvR" class="headerlink" title="OvR:"></a>OvR:</h5><ul><li>将 $N$ 个分类中的1个分类拿出来作为一个分类器的正例，其余均设置为反例，从而<strong>【拆分】</strong>成 $N$ 个分类任务</li><li><strong>【训练】</strong> 得到 $N$ 个分类器</li><li><strong>【集成】</strong> 的方法是考虑各被判为正例的分类器的置信度，选择置信度大的类别标记作为分类的结果</li></ul><p><img src="https://pic3.zhimg.com/80/v2-437db32129fdc1d411ac560bfa0f8842_hd.jpg" alt="img"></p><h5 id="MvM"><a href="#MvM" class="headerlink" title="MvM:"></a>MvM:</h5><p>MvM是OvO和OvR的一般形式，反过来说，OvO和OvR是MvM的特例。</p><p>MvM每次将若干个类作为正类，若干个其他类作为反类。但其构造必须有特殊的设计，不能随意选取。常用的一种MvM技术是“纠错输出码”（ECOC）技术。</p><h4 id="“纠错输出码”-ECOC-技术"><a href="#“纠错输出码”-ECOC-技术" class="headerlink" title="“纠错输出码” (ECOC) 技术"></a>“纠错输出码” (ECOC) 技术</h4><p><strong>ECOC过程主要分为两步：</strong></p><ul><li><strong>编码：</strong>对 $N$ 个类进行 $M$ 次划分，产生 $M$ 个分类器</li><li><strong>解码：</strong>$M$ 个分类器对测试样本进行预测，得到 $M$ 个预测标记。将其组成编码；这个编码与 $N$ 个类别各自的编码进行比较，返回其中距离较小的类别作为最终预测的结果</li></ul><p><strong>编码形式：</strong></p><ul><li><strong>二元码：</strong> “正类”、“反类”</li><li><strong>三元码：</strong> “正类”、“反类”、“停用类”</li></ul><p>以二元 ECOC 码为例：如下图，首先，将 $N (N = 4)$ 个类提供设计构造成 $M (M = 5)$ 个分类器 $(f_1, f_2,f_3,f_4,f_5)$, 每个分类器为每个类分别配了一个标记结果 (-1或+1)，从而，每一个类 $C_i, i\in \{1, N\}$ 都获得了一个 $M$ 位的编码，这个编码就是 <strong>【各类所对应的编码】</strong></p><p>当有一个测试示例 $A$ 时，先将 $A$ 依照次序放入 $M$ 个分类器中，得到了 $M$ 个分类标记结果 $(-1,-1,+1,-1,+1);$ 再将这 $M$ 个标记结果编成一个纠错输出码 $(-1-1+1-1+1);$ 最后去和<strong>【各类所对应的编码】</strong>进行比较海明距离或欧式距离，距离最短的编码对应的分类就是结果。</p><p><img src="https://pic1.zhimg.com/80/v2-2c27bcec29d2a98b0a4725775cb8b2c4_hd.jpg" alt="img"></p><hr><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p><strong>类别不平衡(class-imbalance)</strong> 就是指分类任务中不同类别的训练样例数目差别很大的情况</p><p>以二分类问题为例，该问题一般指的是训练集中正负样本数比例相差过大（比如正例9998个，负例2个），其一般会造成以下的一些情况：</p><ul><li>类别少的误判惩罚过低，导致有所偏袒，当样本不确定时倾向于把样本分类为多数类。</li><li>样本数量分布很不平衡时，特征的分布同样会不平衡。</li><li>传统的评价指标变得不可靠，例如准确率。</li></ul><p>而在多分类问题中，尽管原始训练集中可能不同类别训练样本数目相当，通过OvR、MvM进行拆分时也有可能会造成上述情况，所以类别不平衡问题亟待解决。</p><p><strong>再缩放：</strong> 解决类别不平衡问题一个最基本思路是 “再缩放”，即当正反例数目不同时，令 $m^+$ 表示正例数目，$m^-$ 表示反例数目，则：</p><script type="math/tex; mode=display">几率 = \frac{y\prime}{1-y\prime} = \frac{y}{1-y}\times\frac{m^-}{m^+}</script><font color="#0099ff">再缩放的思想虽然简单，但实际操作却不平凡，主要因为“训练集是真实样本总体的无偏采样”这个假设往往不成立，即我们未必能基于训练集的观察几率来推断真实几率</font><p><strong>现有技术大体上有三类做法：</strong></p><ul><li>第一类是对训练集里的反类样例进行<strong>欠采样</strong>，即去除一些反例使得正、反例数目接近</li><li>第二类是对训练集里的正类样例进行<strong>过采样</strong>，即增加一些正例使得正、反例数目接近</li><li>第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上式嵌入到其决策过程中，称为<strong>阈值移动</strong></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（三）线性模型&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（三）线性模型&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（三）线性模型&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（三）线性模型&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application</title>
    <link href="http://sunfeng.online/2019/08/08/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E5%9B%9B%E5%91%A8(Deep%20Neural%20Networks)%E2%80%94%E2%80%94Programming%20Assignments%205%E3%80%81Deep%20Neural%20Network%20Application/"/>
    <id>http://sunfeng.online/2019/08/08/课程一(Neural Networks and Deep Learning), 第四周(Deep Neural Networks)——Programming Assignments 5、Deep Neural Network Application/</id>
    <published>2019-08-08T09:41:25.000Z</published>
    <updated>2019-08-09T06:40:33.693Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Deep-Neural-Network-for-Image-Classification-Application"><a href="#Deep-Neural-Network-for-Image-Classification-Application" class="headerlink" title="Deep Neural Network for Image Classification Application"></a>Deep Neural Network for Image Classification Application</h3><a id="more"></a><p>When you finish this, you will have finished the last programming assignment of Week 4, and also the last programming assignment of this course!</p><p>You will use the functions you had implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification. Hopefully, you will see an improvement in accuracy relative to your previous logistic regression implementation.</p><p><strong>After this assignment you will be able to:</strong></p><ul><li>Build and apply a deep neural network to supervised learning.</li></ul><p>Let’s get started!</p><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://www.numpy.org.cn/" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/dnn_app_utils_v2.py" target="_blank" rel="noopener">dnn_app_utils</a> provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage </span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>You will use the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network “  (Assignment 2). The model you had built had 70% test accuracy on classifying cats vs non-cats images. Hopefully, your new model will perform a better!</p><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- a training set of m_train images labelled <span class="keyword">as</span> cat (<span class="number">1</span>) <span class="keyword">or</span> non-cat (<span class="number">0</span>)</span><br><span class="line">- a test set of m_test images labelled <span class="keyword">as</span> cat <span class="keyword">and</span> non-cat</span><br><span class="line">- each image <span class="keyword">is</span> of shape (num_px, num_px, <span class="number">3</span>) where <span class="number">3</span> <span class="keyword">is</span> <span class="keyword">for</span> the <span class="number">3</span> channels (RGB).</span><br></pre></td></tr></table></figure><p>Let’s get more familiar with the dataset. Load the data by running the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure><p>The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">7</span></span><br><span class="line">plt.imshow(train_x_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_y[<span class="number">0</span>,index]) + <span class="string">". It's a "</span> + classes[train_y[<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">" picture."</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p>y = 1. It’s a cat picture.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128202109097-1894133180.png" alt="img"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Explore your dataset </span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>] <span class="comment"># 行数</span></span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]  <span class="comment"># 列数</span></span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]  <span class="comment"># 行数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: <span class="number">209</span></span><br><span class="line">Number of testing examples: <span class="number">50</span></span><br><span class="line">Each image <span class="keyword">is</span> of size: (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_x_orig shape: (<span class="number">209</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_y shape: (<span class="number">1</span>, <span class="number">209</span>)</span><br><span class="line">test_x_orig shape: (<span class="number">50</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">test_y shape: (<span class="number">1</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128202810081-667256240.png" alt="img"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples </span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T   <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment">#  "-1" 使得剩下的维度变为1维</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_x<span class="string">'s shape: (12288, 209)</span></span><br><span class="line"><span class="string">test_x'</span>s shape: (<span class="number">12288</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>12,288 equals 64×64×3 which is the size of one reshaped image vector.</p><p>Then, integrate the above code into the function <code>dataset_preprocess()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataset_preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementing Data Set Preprocessing</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_x: your training set features</span></span><br><span class="line"><span class="string">        train_y: your training set labels</span></span><br><span class="line"><span class="string">        test_x: your test set features</span></span><br><span class="line"><span class="string">        test_y: your test set labels</span></span><br><span class="line"><span class="string">        classes: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># load data set</span></span><br><span class="line">    train_x_orig,  train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape the  training and test examples</span></span><br><span class="line">    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T <span class="comment"># 参数-1使得剩下的维度变为1维</span></span><br><span class="line">    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Standardize data to have feature values between 0 and 1</span></span><br><span class="line">    train_x = train_x_flatten / <span class="number">255</span></span><br><span class="line">    test_x = test_x_flatten / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_x, train_y, test_x, test_y, classes</span><br></pre></td></tr></table></figure><hr><h3 id="Architecture-of-your-model"><a href="#Architecture-of-your-model" class="headerlink" title="Architecture of your model"></a>Architecture of your model</h3><p>Now that you are familiar with the dataset, it is time to build a deep neural network distinguish cat images from non-cat images.</p><p>You will build two different models:</p><ul><li>A 2-layer neural network.</li><li>An L-layer neural network.</li></ul><p>You will then compare the performance of  these models, and also try out different values for L.</p><h4 id="2-layer-neural-network"><a href="#2-layer-neural-network" class="headerlink" title="2-layer neural network"></a>2-layer neural network</h4><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128203731956-1246182495.png" alt="img"></p><p>The model can be summarized as : <strong><em>INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.</em></strong> </p><p><strong>Detailed Architecture of figure 2:</strong></p><ul><li><p>The input is a (64, 64, 3) image which  is flatten to a vector of size (12288, 1)</p></li><li><p>The corresponding vector:  ${[x_0, x_1, …… ,x_{12287} ]}^T$ is then multiplied by the weight matrix $W^{[1]}$ of size ($n^{[1]}$, 12288).</p></li><li><p>You then add a bias term and take its relu to get following vector:</p><p> ${[a_0^{[1]}, a_1^{[1]}, …… ,a_{n^{[1]}-1} ^{[1]}]}^T$</p></li><li><p>You the repeat the same process.</p></li><li><p>You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias).</p></li><li><p>Finally, you take the sigmoid of the result. If it is greater than 0.5, you classify it to be a cat.</p></li></ul><h4 id="L-layer-neural-network"><a href="#L-layer-neural-network" class="headerlink" title="L-layer neural network"></a>L-layer neural network</h4><p>It is hard to represent an L-layer deep neural network with the above representation. However, here is a simplified network representation:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128204442487-1691015986.png" alt="img"></p><p>The model can be summarized as：<strong><em>[ LINEAR -&gt; RELU ] $ \times $ (L-1) -&gt; LINEAR -&gt;SIGMOID</em></strong> </p><p><strong>Detailed Architecture of figure 3:</strong></p><ul><li><p>The input is a (64, 64, 3) image which is flattened to a vector of size (12288, 1).</p></li><li><p>The corresponding vector: ${[x_0, x_1, …… ,x_{12287} ]}^T$ is the multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[1]}$. The result is called the linear unit.</p></li><li><p>Next, you take the relu of the linear unit. This process could be repeated several times for each ($W^{[l]}, b^{[l]}$) depending on the model architecture.</p></li><li><p>Finally, you take the sigmoid of the final linear unit. If it is a greater than 0.5, you classify it to be a cat.</p></li></ul><h4 id="General-methodology"><a href="#General-methodology" class="headerlink" title="General methodology"></a>General methodology</h4><p>As usual you will follow the Deep Learning methodology to build the model:</p><ol><li>Initialize parameters / Define hyperparameters</li><li>Loop for num_iterations:<ul><li>Forward propagation</li><li>Compute cost function</li><li>Backward propagation</li><li>Update parameters</li></ul></li><li>Use trained parameters to predict labels </li></ol><hr><h3 id="Two-layer-neural-network"><a href="#Two-layer-neural-network" class="headerlink" title="Two - layer neural network"></a>Two - layer neural network</h3><p><strong>Exercise:</strong> Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: <em>LINEAR -&gt; RELU -&gt;LINEAR -&gt;SIGMOID</em>. The functions you may need and their inputs are:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Two-layer neural network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">        num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        print_cost -- If set to True, this will print the cost every 100 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one </span></span><br><span class="line">    <span class="comment"># of the functions you'd previously implemented</span></span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get W1, b1, W2, and b2</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR-&gt; SIGMOID</span></span><br><span class="line">        <span class="comment"># Inputs: "X, W1, b1"</span></span><br><span class="line">        <span class="comment"># Outputs: "A1, cache1, A2, cache2"</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, activation = <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = -(np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        <span class="comment"># Inputs: dA2, cache2, cache1</span></span><br><span class="line">        <span class="comment"># Outputs: dA1, dW2, db2, dA0(not used), dW1, db1</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = <span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Run the cell below to train your parameters. See if your model runs. The cost should be decreasing. It may take up to 5 minutes to run 2500 iterations.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693049735659989</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.6464320953428849</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.6325140647912678</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.6015024920354665</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.5601966311605748</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.515830477276473</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.4754901313943325</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.43391631512257495</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.4007977536203886</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.35807050113237987</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.3394281538366413</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.30527536361962654</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.2749137728213015</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.24681768210614827</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.1985073503746611</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.17448318112556593</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.1708076297809661</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.11306524562164737</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.09629426845937163</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.08342617959726878</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.0743907870431909</span></span><br><span class="line">Cost after iteration <span class="number">2100</span>: <span class="number">0.06630748132267938</span></span><br><span class="line">Cost after iteration <span class="number">2200</span>: <span class="number">0.05919329501038176</span></span><br><span class="line">Cost after iteration <span class="number">2300</span>: <span class="number">0.05336140348560564</span></span><br><span class="line">Cost after iteration <span class="number">2400</span>: <span class="number">0.048554785628770226</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128213017144-1851629766.png" alt="img"></p><p>Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.</p><p>Now, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 1.0</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.72</span><br></pre></td></tr></table></figure><p><strong>Note:</strong> You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting.</p><p>Congratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an LL-layer model.</p><hr><h3 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h3><p><strong>Exercise:</strong>  Use the helper functions you have implemented previously to build an LL-layer neural network with the following structure: <em>[LINEAR -&gt; RELU]×(L-1) -&gt; LINEAR -&gt; SIGMOID</em>. The functions you may need and their inputs are:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameter</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L-layer Neural Network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR-&gt;SIGMOID</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        layers_dims -- list containing the input size and each layer size, of length (number of layers + 1)</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- parameters learnt by the model. They can then be used to predict</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters initialization</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU] * (L-1) -&gt; LINEAR -&gt;SIGMOID</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125; : &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations(per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>You will now train the model as a 5-layer neural network.</p><p>Run the cell below to train your model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations.</p><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.771749</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.672053</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.648263</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.611507</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.567047</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.540138</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.527930</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.465477</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.369126</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.391747</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.315187</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.272700</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.237419</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.199601</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.189263</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.161189</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.148214</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.137775</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.129740</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.121225</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.113821</span></span><br><span class="line">Cost after iteration <span class="number">2100</span>: <span class="number">0.107839</span></span><br><span class="line">Cost after iteration <span class="number">2200</span>: <span class="number">0.102855</span></span><br><span class="line">Cost after iteration <span class="number">2300</span>: <span class="number">0.100897</span></span><br><span class="line">Cost after iteration <span class="number">2400</span>: <span class="number">0.092878</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128215013940-1660492466.png" alt="img"></p><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result: </strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.985645933014</span><br></pre></td></tr></table></figure><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>Result: </strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.8</span><br></pre></td></tr></table></figure><p>Congrats! It seems that your 5-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.</p><p>This is a good performance for this task. Nice job!</p><p>Though in the next course on “Improving deep neural networks” you will learn how to obtain even higher accuracy by systematically searching for better hyperparameters (learning_rate, layers_dims, num_iterations, and others you’ll also learn in the next course).</p><hr><h3 id="Result-Analysis"><a href="#Result-Analysis" class="headerlink" title="Result Analysis"></a>Result Analysis</h3><p>First, let’s take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128234807019-678540948.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128234830378-2132257659.png" alt="img"></p><p><strong>A few type of images the model tends to do poorly on include:</strong></p><ul><li>Cat body in an unusual position.</li><li>Cat appears against a background of  a similar color.</li><li>Unusual cat color and species.</li><li>Camera Angle.</li><li>Brightness of the picture.</li><li>Scale variation (cat is very large or small in image)</li></ul><hr><h3 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network" target="_blank" rel="noopener">Source Code</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Deep-Neural-Network-for-Image-Classification-Application&quot;&gt;&lt;a href=&quot;#Deep-Neural-Network-for-Image-Classification-Application&quot; class=&quot;headerlink&quot; title=&quot;Deep Neural Network for Image Classification Application&quot;&gt;&lt;/a&gt;Deep Neural Network for Image Classification Application&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step</title>
    <link href="http://sunfeng.online/2019/08/07/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E5%9B%9B%E5%91%A8(Deep%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%204%E3%80%81Building%20your%20Deep%20Neural%20Network,%20step%20by%20step/"/>
    <id>http://sunfeng.online/2019/08/07/课程一(Neural Networks and Deep Learning), 第四周(Deep neural networks)——Programming assignment 4、Building your Deep Neural Network, step by step/</id>
    <published>2019-08-07T07:58:04.000Z</published>
    <updated>2019-08-08T03:27:53.124Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h3><a id="more"></a><p>Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a 2-layer Neural Network (with a signal hidden layer). This week, you will build a deep neural network, with as many layers as you want!</p><ul><li>In this notebook, you will implement all the functions required to build a deep neural network.</li><li>In the next assignment, you will use these functions to build a deep neural network for image classification.</li></ul><p><strong>After this assignment you will be able to:</strong></p><ul><li>Use non-linear units like ReLU to improve your model</li><li>Build a deeper neural network (with more than 1 hidden layer)</li><li>Implement an easy-to-use neural network class</li></ul><p><strong>Notation:</strong></p><ul><li>Superscript [I] denotes a quantity associated with the $1^{th}$ layer.<ul><li>Example: $a^{[L]}$is the $L^{th}$ layer activation. $W^{[L]}$ and  $b^{[L]}$ are the $L^{th}$  layer parameters.</li></ul></li><li>Superscript (i) denotes a quantity associated with the $i^{th}$ example.<ul><li>Example: $x^{(i)}$ is the $i^{th}$ training example.</li></ul></li><li>Lowerscript i denotes the $i^{th}$ entry of a vector.<ul><li>Example: $a_i^{[I]}$ denotes the $i^{th}$ entry of the $I^{th}$ layer’s activations.</li></ul></li></ul><p><strong>Let’s get started!</strong></p><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://numpy.org/" target="_blank" rel="noopener">numpy</a> is the main packages for scientific computing with Python.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/dnn_utils_v2.py" target="_blank" rel="noopener">dnn_utils</a> provides some necessary functions for this notebook.</li><li><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/blob/master/course1_deep_learning_and_neural_network/assignment4_deep_neural_network/testCases_v2.py" target="_blank" rel="noopener">testCases</a> provides some test cases to assess the correctness of your functions.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="Outline-of-the-Assignment"><a href="#Outline-of-the-Assignment" class="headerlink" title="Outline of the Assignment"></a>Outline of the Assignment</h3><p>To build your neural network, you will be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of the assignment, you will:</p><ul><li>Initialize the parameters for a two-layer network and for an L-layer neural network.</li><li>Implement the forward propagation module (shown in purple  in the figure below).<ul><li>Complete the LINEAR part of a layer’s forward propagation step (resulting in $Z^{l}$).</li><li>We give you the ACTIVATION function (relu / sigmoid).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li><li>Stack the [LINEAR-&gt;RELU] forward function L-1 times (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li></ul></li><li>Compute the loss.</li><li>Implement the backward propagation module (denoted in red in the figure below).<ul><li>Complete the LINEAR part of a layer’s backward propagation step.</li><li>We give you the gradient of the ACTIVATION function (relu_backward / sigmoid_backward).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li><li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li></ul></li><li>Finally update the parameters.</li></ul><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127165838722-72534730.png" alt="img"></p><p><strong>Note</strong> that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backward propagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps. </p><hr><h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>You will write two helper functions that will initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one will generalize this initialization process to L layers.</p><h4 id="2-layer-Neural-Network"><a href="#2-layer-Neural-Network" class="headerlink" title="2-layer Neural Network"></a>2-layer Neural Network</h4><p><strong>Exercise:</strong> Create and initialize the parameters of the 2-layer neural network.</p><p><strong>Instructions:</strong> </p><ul><li>The model’s structure is: <em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID​.</em></li><li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) *0.01</code> with the correct shape.</li><li>Use zero initialize for the biases. Use <code>np.zeros(shape)</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 2-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize parameters for a two-layer network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @n_x: size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01624345</span> <span class="number">-0.00611756</span> <span class="number">-0.00528172</span>]</span><br><span class="line"> [<span class="number">-0.01072969</span>  <span class="number">0.00865408</span> <span class="number">-0.02301539</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[ <span class="number">0.01744812</span> <span class="number">-0.00761207</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h4 id="L-layer-Neural-Network"><a href="#L-layer-Neural-Network" class="headerlink" title="L-layer Neural Network"></a>L-layer Neural Network</h4><p>The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the <code>initialize_parameters_deep</code>, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units of layer l.</p><p>Thus for example if the size of our input X is (12288, 209) (with m = 209 examples) then:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200241753-1787058979.png" alt="img"></p><p>Remember that when we compute $WX + b$ in Python, it carries out broadcasting. For example, if:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200327409-197353303.png" alt="img"></p><p>The $WX + b$ will be:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127200424847-714067061.png" alt="img"></p><p><strong>Exercise:</strong> Implement initialization for an L-layer Neural Network.</p><p><strong>Instructions:</strong></p><ul><li>The model’s structure is <em>[LINEAR -&gt; RELU]</em> $\times$ <em>(L-1) -&gt; LINEAR -&gt; SIGMOID.</em> It has <em>L-1</em> layers using a ReLU activation function followed by an output layer with a sigmoid activation function.</li><li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape) * 0.01</code>.</li><li>We will store $n^{[l]}$, the number of units in different layers, in a variables <code>layer_dims</code>. For example, the <code>layer_dims</code> for the “Planar Data classification model” from last week would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. Thus means <code>W1</code>‘s shape was (4,2), <code>b1</code> was (4,1), <code>W2</code> was (1,4) and <code>b2</code> was (1,1). Now you will generalize this to LL layers!</li><li>Here is the implementation for L = 1 (one layer neural network). It should inspire you to implement the general case (L-layer neural network).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> L == <span class="number">1</span>:</span><br><span class="line">      parameters[<span class="string">"W"</span> + str(L)] = np.random.randn(layer_dims[<span class="number">1</span>], layer_dims[<span class="number">0</span>]) * <span class="number">0.01</span></span><br><span class="line">      parameters[<span class="string">"b"</span> + str(L)] = np.zeros((layer_dims[<span class="number">1</span>], <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. L-layer Neural Network</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initailize parameters for an L-layer neural network</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @layer_dims: python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters "W1", "b1",...</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (layer_dims[1], 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)     <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># Random Initialization</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ <span class="number">0.01788628</span>  <span class="number">0.0043651</span>   <span class="number">0.00096497</span> <span class="number">-0.01863493</span> <span class="number">-0.00277388</span>]</span><br><span class="line"> [<span class="number">-0.00354759</span> <span class="number">-0.00082741</span> <span class="number">-0.00627001</span> <span class="number">-0.00043818</span> <span class="number">-0.00477218</span>]</span><br><span class="line"> [<span class="number">-0.01313865</span>  <span class="number">0.00884622</span>  <span class="number">0.00881318</span>  <span class="number">0.01709573</span>  <span class="number">0.00050034</span>]</span><br><span class="line"> [<span class="number">-0.00404677</span> <span class="number">-0.0054536</span>  <span class="number">-0.01546477</span>  <span class="number">0.00982367</span> <span class="number">-0.01101068</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.01185047</span> <span class="number">-0.0020565</span>   <span class="number">0.01486148</span>  <span class="number">0.00236716</span>]</span><br><span class="line"> [<span class="number">-0.01023785</span> <span class="number">-0.00712993</span>  <span class="number">0.00625245</span> <span class="number">-0.00160513</span>]</span><br><span class="line"> [<span class="number">-0.00768836</span> <span class="number">-0.00230031</span>  <span class="number">0.00745056</span>  <span class="number">0.01976111</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><hr><h3 id="Forward-propagation-module"><a href="#Forward-propagation-module" class="headerlink" title="Forward propagation module"></a>Forward propagation module</h3><h4 id="Linear-Forward"><a href="#Linear-Forward" class="headerlink" title="Linear Forward"></a>Linear Forward</h4><p>Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:</p><ul><li>LINEAR</li><li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li><li>[LINEAR -&gt; RELU] $\times$ (L-1) -&gt; LINEAR -&gt;SIGMOID (whole model)</li></ul><p>The linear forward module (vectorized over all the examples) computes the following equations:</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}</script><p>where $A^{[0]} = X$.</p><p><strong>Exercise:</strong> Build the linear part of forward propagation.</p><p><strong>Reminder:</strong> The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. You may also find <code>np.dot()</code> useful. If your dimensions don’t match, printing <code>W.shape</code> may help.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @z: the input of activation function, also called pre-activation parameter</span></span><br><span class="line"><span class="string">        @cache: a python dictionary containing "A", "W", and "b"; stored for computing the backword pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A, W, b = linear_forward_test_case()</span><br><span class="line"></span><br><span class="line">Z, linear_cache = linear_forward(A, W, b)</span><br><span class="line">print(<span class="string">"Z = "</span> + str(Z))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = [[ <span class="number">3.26295337</span> <span class="number">-1.23429987</span>]]</span><br></pre></td></tr></table></figure><h4 id="Linear-Activation-Forward"><a href="#Linear-Activation-Forward" class="headerlink" title="Linear-Activation Forward"></a>Linear-Activation Forward</h4><p>In this notebook, you will use two activation functions:</p><ul><li><p><strong>Sigmoid:</strong> $\sigma(Z) = \sigma(WA + b) = \frac{1}{1 + e^{-(WA + b)}}$. We have provided you with the <code>sigmoid</code> function. This function returns two items: the activation value <code>&quot;a&quot;</code> and a <code>&quot;cache&quot;</code> that contains <code>&quot;Z&quot;</code> (it’s what we feed in to the corresponding backward function). To use it you could just call:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = sigmoid(Z)</span><br></pre></td></tr></table></figure></li><li><p><strong>ReLU:</strong> The mathematical formula for ReLu is $A = ReLU(Z) = max(0, Z)$. We have provided you with the <code>relu</code> function. This function returns <strong>two</strong> items: the activation value “<code>A</code>“ and a “<code>cache</code>“ that contains “<code>Z</code>“ (it’s what we will feed in to the corresponding backward function). To use it you could just call:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = relu(Z)</span><br></pre></td></tr></table></figure></li></ul><p>For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.</p><p><strong>Exercise:</strong> Implement the forward propagation of the <em>LINEAR -&gt; ACTIVATION</em> layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$ where the activation <code>&quot;g&quot;</code> can be <code>sigmoid()</code> or <code>relu()</code>. Use <code>linear_forward()</code> and the correct activation function.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation Forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A_prev: activations from previous layer (or input data) of shape (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">        @W: weight matrix: numpy array of shape (size pf current layer, size of previous layer)</span></span><br><span class="line"><span class="string">        @b: bias vector: numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">        @activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        @A: the output of the activation function, also called the post-activation value</span></span><br><span class="line"><span class="string">        @cache: a python tuple containing "linear_cache" and "activation_cache"; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: A_prev, W, b</span></span><br><span class="line">        <span class="comment"># Outputs: A, activation_cache</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A_prev, W, b = linear_activation_forward_test_case()</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A))</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>)</span><br><span class="line">print(<span class="string">"With ReLU: A = "</span> + str(A))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">With sigmoid: A = [[ <span class="number">0.96890023</span>  <span class="number">0.11013289</span>]]</span><br><span class="line">With ReLU: A = [[ <span class="number">3.43896131</span>  <span class="number">0.</span>        ]]</span><br></pre></td></tr></table></figure><p><strong>Note:</strong> In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.</p><h4 id="L-Layer-Model"><a href="#L-Layer-Model" class="headerlink" title="L-Layer Model"></a>L-Layer Model</h4><p>For even convenience when implementing the L-layer Neural Network, you will need a function that replicates the previous one <code>linear_activation_forward</code> with ReLU L-1 times, then follows that with one <code>linear_activation_forward</code>with SIGMOID.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127213245862-62967896.png" alt="img"></p><p><strong>Exercise:</strong> Implement the forward propagation of the above model. </p><p><strong>Instruction:</strong> In the code below, the variable AL will denote </p><p>$A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]}A^{[L-1]} + b^{[L]})$.</p><p><strong>Tips:</strong> </p><ul><li>Use the functions you had previously written.</li><li>Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times</li><li>Don’t forget to keep track of the caches in the “caches” list. To add a new value <code>c</code> to a <code>list</code>, you can use <code>list.append(c)</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L-layer Model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1) -&gt; [LINEAR-&gt;SIGMOID] computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @AL -- last post-activation value</span></span><br><span class="line"><span class="string">        @caches -- list of caches containing:</span></span><br><span class="line"><span class="string">            every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">            the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU] * (L-1).</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line"></span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], activation = <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; SIGMOID]</span></span><br><span class="line">    <span class="comment"># Add "cache" to the "caches" list</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, parameters = L_model_forward_test_case_2hidden()</span><br><span class="line">AL, caches = L_model_forward(X, parameters)</span><br><span class="line">print(<span class="string">"AL = "</span> + str(AL))</span><br><span class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AL = [[ <span class="number">0.03921668</span>  <span class="number">0.70498921</span>  <span class="number">0.19734387</span>  <span class="number">0.04728177</span>]]</span><br><span class="line">Length of caches list = <span class="number">3</span></span><br></pre></td></tr></table></figure><p>Great! Now you have a full forward propagation that takes the input X and outputs a row vector AL containing your predictions. It also records all intermediate values in “caches”. Using AL, you can computes the cost of your predictions.</p><hr><h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.</p><p><strong>Exercise:</strong> Compute the cross-entropy cost J, using the following formula:</p><script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}</script><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function (the cross-entropy cost J)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape of (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y</span></span><br><span class="line">    cost = - (np.dot(Y, np.log(AL).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL).T)) / m</span><br><span class="line">    cost = np.squeeze(cost) <span class="comment"># To make sure your cost's shape is waht we expect</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><strong>Test:</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y, AL = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = <span class="number">0.414931599615397</span></span><br></pre></td></tr></table></figure><hr><h3 id="Backward-propagation-module"><a href="#Backward-propagation-module" class="headerlink" title="Backward propagation module"></a>Backward propagation module</h3><p>Just like with forward propagation, you will implement helper functions for backward propagation. Remember that backward propagation is used to calculate the gradient of the loss function with respect to the parameters.</p><p><strong>Reminder:</strong></p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127233429440-720852258.png" alt="img"></p><p><em>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</em></p><p>Now, similar to forward propagation, you are going to build the backward propagation in three steps:</p><ul><li>LINEAR backward</li><li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li><li>[LINEAR -&gt; RELU] ×× (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li></ul><h4 id="Linear-backward"><a href="#Linear-backward" class="headerlink" title="Linear backward"></a>Linear backward</h4><p>For layer l, the linear part is: $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$ (followed by an activation). </p><p>Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial J}{\partial Z^{[l]}}$ . You want to get ($dW^{[l]}$, $db^{[l]}$, $dA^{[l]}$).</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235155972-308393091.png" alt="img"></p><p>The three output ($dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$) are computed using the input $dZ^{[l]}$. Here are the formulas you need:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171127235506737-1259548081.png" alt="img"></p><p><strong>Exercise：</strong> Use the 3 formulas above to implement <code>linear_backward().</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Linear backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA -- Gradient of the cost with respect to the activation (of the prevous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of the cost with respect to the W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of the cost with respect to the b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.sum(dZ, axis = <span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up some test inputs</span></span><br><span class="line">dZ, linear_cache = linear_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dA_prev = [[ <span class="number">0.51822968</span> <span class="number">-0.19517421</span>]</span><br><span class="line"> [<span class="number">-0.40506361</span>  <span class="number">0.15255393</span>]</span><br><span class="line"> [ <span class="number">2.37496825</span> <span class="number">-0.89445391</span>]]</span><br><span class="line">dW = [[<span class="number">-0.10076895</span>  <span class="number">1.40685096</span>  <span class="number">1.64992505</span>]]</span><br><span class="line">db = [[ <span class="number">0.50629448</span>]]</span><br></pre></td></tr></table></figure><h4 id="Linear-Activation-backward"><a href="#Linear-Activation-backward" class="headerlink" title="Linear-Activation backward"></a>Linear-Activation backward</h4><p>Next, you will create a function that merges the two helper functions: <code>linear_backward</code> and the backward step for the activation <code>linear_activation_backward</code>.</p><p>To help you implement <code>linear_activation_backward</code>, we provided two backward functions:</p><ul><li><p><strong>sigmoid_backward</strong>: Implements the backward propagation for SIGMOID unit. You can call it as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = sigmoid_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure></li><li><p><strong>relu_backward</strong>: Implements the backward propagation for RELU unit. You can call it as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = relu_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure></li></ul><p>If g(.) is the activation function, <code>sigmoid_backward</code> and <code>relu_backward</code> compute:  $dZ^{[l]} = dA^{[l]} * g^{\prime}(Z^{[l]})$ </p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Linear-Activation backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dA -- post-activation gradient for current layer 1</span></span><br><span class="line"><span class="string">        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">        activation -- the activation function to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dA_prev -- Gradient of cost with respect to the activation (of the previous layer l - 1), same shape as A_prev</span></span><br><span class="line"><span class="string">        dW -- Gradient of cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">        db -- Gradient of cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        </span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dA, linear_activation_cache = linear_activation_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sigmoid:</span><br><span class="line">dA_prev = [[ <span class="number">0.11017994</span>  <span class="number">0.01105339</span>]</span><br><span class="line"> [ <span class="number">0.09466817</span>  <span class="number">0.00949723</span>]</span><br><span class="line"> [<span class="number">-0.05743092</span> <span class="number">-0.00576154</span>]]</span><br><span class="line">dW = [[ <span class="number">0.10266786</span>  <span class="number">0.09778551</span> <span class="number">-0.01968084</span>]]</span><br><span class="line">db = [[<span class="number">-0.05729622</span>]]</span><br><span class="line"></span><br><span class="line">relu:</span><br><span class="line">dA_prev = [[ <span class="number">0.44090989</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.37883606</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.2298228</span>   <span class="number">0.</span>        ]]</span><br><span class="line">dW = [[ <span class="number">0.44513824</span>  <span class="number">0.37371418</span> <span class="number">-0.10478989</span>]]</span><br><span class="line">db = [[<span class="number">-0.20837892</span>]]</span><br></pre></td></tr></table></figure><h4 id="L-Model-Backward"><a href="#L-Model-Backward" class="headerlink" title="L-Model Backward"></a>L-Model Backward</h4><p>Now you will implement the backward function for the whole network. Recall that when you implemented the <code>L_model_forward</code> function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the <code>L_model_backward</code> function, you will iterate through all the hidden layers backward, starting from layer L. On each step, you will use the cached values for layer l to backward propagate through layer l. Figure 5 below shows the backward pass.</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128142511269-248723414.png" alt="img"></p><p><strong>Initializing backward propagation:</strong>  To implement backward propagate through this network, we know that output is, $A^{[L]} = \sigma(Z^{[L]}) $ . Your code thus need to compute $dAL = \frac{\partial J}{\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don’t need in-depth knowledge of):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br></pre></td></tr></table></figure><p>You can then use this post-activation gradient <code>dAL</code> to keep going backward. As seen in Figure 5, you can now feed in <code>dAL</code> into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a <code>for</code> loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula: grads[“dW” + str(l)] = $dW^{[l]}$</p><p>For example, for l=3 this would store $dW^{[l]}$ in <code>grads[&quot;dW3&quot;]</code>.</p><p><strong>Exercise:</strong> Implement backpropagation for the <em>[LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em> model.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. L-Model Backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backword propagaiton for the [LINEAR-&gt;RELU] * (L - 1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">        Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">        caches -- list of caches containing: every cache of linear_activation_forward() with "relu" </span></span><br><span class="line"><span class="string">                  (it's caches[1], for l in range(L - 1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                  the cache of linear_activation_foreward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">                 grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                 grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment"># the number of examples</span></span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class="line">    <span class="comment"># Inputs: AL, Y, caches</span></span><br><span class="line">    <span class="comment"># Outputs: grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line"></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients</span></span><br><span class="line">        <span class="comment"># Inputs: grads["dA" + str(l + 2)], caches</span></span><br><span class="line">        <span class="comment"># outputs: grads["dA" + str(l + 1)], grads["dW" + str(l + 1)], grads["db" + str(l + 1)]</span></span><br><span class="line"></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AL, Y_assess, caches = L_model_backward_test_case()</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches)</span><br><span class="line">print_grads(grads)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ <span class="number">0.41010002</span>  <span class="number">0.07807203</span>  <span class="number">0.13798444</span>  <span class="number">0.10502167</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.05283652</span>  <span class="number">0.01005865</span>  <span class="number">0.01777766</span>  <span class="number">0.0135308</span> ]]</span><br><span class="line">db1 = [[<span class="number">-0.22007063</span>]</span><br><span class="line"> [ <span class="number">0.</span>        ]</span><br><span class="line"> [<span class="number">-0.02835349</span>]]</span><br><span class="line">dA1 = [[ <span class="number">0.12913162</span> <span class="number">-0.44014127</span>]</span><br><span class="line"> [<span class="number">-0.14175655</span>  <span class="number">0.48317296</span>]</span><br><span class="line"> [ <span class="number">0.01663708</span> <span class="number">-0.05670698</span>]]</span><br></pre></td></tr></table></figure><h4 id="Update-Parameters"><a href="#Update-Parameters" class="headerlink" title="Update Parameters"></a>Update Parameters</h4><p>In this section you will update the parameters of the model, using gradient descent:</p><p><img src="https://images2018.cnblogs.com/blog/937910/201711/937910-20171128164243597-30974158.png" alt="img"></p><p>where $\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.</p><p><strong>Exercise:</strong> Implement <code>update_parameters()</code> to update your parameters using gradient descent.</p><p><strong>Instructions:</strong> Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for l = 1, 2, … L.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Update Parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">        learning_rate -- learning rate of the gradient descent updatte rule</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">                      parameters["W" + str(l)] = ...</span></span><br><span class="line"><span class="string">                      parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = parameters[<span class="string">"W"</span> + str(l)] - learning_rate * grads[<span class="string">"dW"</span> + str(l)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = parameters[<span class="string">"b"</span> + str(l)] - learning_rate * grads[<span class="string">"db"</span> + str(l)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[<span class="number">-0.59562069</span> <span class="number">-0.09991781</span> <span class="number">-2.14584584</span>  <span class="number">1.82662008</span>]</span><br><span class="line"> [<span class="number">-1.76569676</span> <span class="number">-0.80627147</span>  <span class="number">0.51115557</span> <span class="number">-1.18258802</span>]</span><br><span class="line"> [<span class="number">-1.0535704</span>  <span class="number">-0.86128581</span>  <span class="number">0.68284052</span>  <span class="number">2.20374577</span>]]</span><br><span class="line">b1 = [[<span class="number">-0.04659241</span>]</span><br><span class="line"> [<span class="number">-1.28888275</span>]</span><br><span class="line"> [ <span class="number">0.53405496</span>]]</span><br><span class="line">W2 = [[<span class="number">-0.55569196</span>  <span class="number">0.0354055</span>   <span class="number">1.32964895</span>]]</span><br><span class="line">b2 = [[<span class="number">-0.84610769</span>]]</span><br></pre></td></tr></table></figure><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Congrats on implementing all the functions required for building a deep neural network!</p><p>We know it was a long assignment but going forward it will only get better. The next part of the assignment is easier.</p><p>In the next assignment you will put all these together to build two models:</p><ul><li>A two-layer neural network</li><li>An L-layer neural network</li></ul><p>You will in fact use these models to classify cat vs non-cat images!</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Building-your-Deep-Neural-Network-Step-by-Step&quot;&gt;&lt;a href=&quot;#Building-your-Deep-Neural-Network-Step-by-Step&quot; class=&quot;headerlink&quot; title=&quot;Building your Deep Neural Network: Step by Step&quot;&gt;&lt;/a&gt;Building your Deep Neural Network: Step by Step&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（二）-- 模型评估与选择</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/</id>
    <published>2019-08-05T12:23:06.000Z</published>
    <updated>2019-08-10T06:51:57.924Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（二）模型评估与选择"><a href="#《机器学习》西瓜书学习笔记（二）模型评估与选择" class="headerlink" title="《机器学习》西瓜书学习笔记（二）模型评估与选择"></a>《机器学习》西瓜书学习笔记（二）模型评估与选择</h2><a id="more"></a><h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><ul><li><strong>错误率 (error rate)：</strong> 分类错误样本数占总样本数的比例</li><li><strong>准确率 (accuracy)：</strong> 分类正确样本数占总样本数的比例</li><li><strong>误差 (error)：</strong> 学习器的实际预测输出与真实输出之间的差异</li><li><strong><font color="#0099ff">训练误差 (training error)/经验误差(empirical error)：</font></strong> 学习器在训练集上的误差</li><li><strong>泛化误差(generalization error):</strong> 学习器在新样本上的误差</li><li><strong><font color="#0099ff">过拟合(overfitting):</font></strong> 学习能力过于强大。学习器把训练样本学得太好，导致将训练样本中自身含有的特点当成所有潜在样本都会具有的一般性质，从而训练后使得泛化性能下降</li><li><strong>欠拟合(underfitting):</strong> 学习能力低下，对训练样本的一般性质尚未学好</li></ul><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure1.PNG" alt="Figure1"></p><hr><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p><strong>理想：</strong> 通过评估学习器的<strong>泛化误差</strong>，选出泛化误差最小的学习器</p><p><strong>实际：</strong> <strong>泛化误差</strong> 只能通过测试集上的 <strong>测试误差</strong>  作为近似</p><p><strong><font color="#0099ff">机器学习的目的是产生泛化能力好的模型，那么什么样的模型才是泛化能力好的模型呢? 这需要按照一定的评估方法和度量指标去衡量。</font></strong></p><p>给定一个包含m个样例的数据集 $ D = \{(x_1, y_1), (x_2, y_2), … ,(x_m, y_m)\}$ ,通过对D进行适当的处理，从中产生出训练集S和测试集T，<strong>测试集应该尽可能与训练集互斥</strong>，常见的方法有以下三种：</p><h4 id="1-留出法-hold-out"><a href="#1-留出法-hold-out" class="headerlink" title="1.留出法 (hold-out)"></a>1.留出法 (hold-out)</h4><p><strong>留出法：</strong> 直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个集合作为测试集T，即 <em>D=S</em>∪<em>T</em>，<em>S</em>∩<em>T=</em>∅. </p><p><strong>注意点：</strong></p><ul><li>要保持数据分布的一致性<ul><li>分层采样</li></ul></li><li>采用多次随机划分取均值的评估方法</li><li>训练集的比例应当适当(2/3 ~ 4/5)</li></ul><h4 id="2-交叉验证法-cross-validation"><a href="#2-交叉验证法-cross-validation" class="headerlink" title="2.交叉验证法 (cross validation)"></a>2.交叉验证法 (cross validation)</h4><p><strong>交叉验证法：</strong> 将数据集平均分成 <code>K</code> 份，并尽量保证每份数据分布一致。依次用其中 <code>K - 1</code> 份作为训练集，剩下的一份作为测试集。这样就有 <code>K</code> 组训练/测试集。从而可以进行 <code>K</code> 次训练和测试，返回K次测试结果的均值，也称为”K折交叉验证法”</p><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure2.PNG" alt="Figure2"></p><p><strong>注意点：</strong></p><ul><li>K折交叉验证要随机使用不同的划分重复p次，最终取p次K折交叉验证的均值</li></ul><p><strong>留一法：</strong> 若令K = m, 则称为“留一法 (LOO)”</p><ul><li>优点<ul><li>不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集，即每个子集只含有一个样本</li><li>被实际评估的模型与期望评估的用D训练出的模型很相似，因为使用的训练集与初始数据集相比只少了一个样本</li></ul></li><li>缺点<ul><li>当数据集比较大时，训练m个模型的计算开销比较大</li></ul></li></ul><h4 id="3-自助法-bootstrapping"><a href="#3-自助法-bootstrapping" class="headerlink" title="3.自助法 (bootstrapping)"></a>3.自助法 (bootstrapping)</h4><p><strong>自助法：</strong>  给定包含m个样本的数据集 D，从 D 中进行有放回地采样产生包含 m 个样本的数据集 <em>D’</em>，这样 D 中大概有36.8%的样本不会出现在 <em>D’</em> 中，<strong>将 <em>D’</em> 用作训练集，D - <em>D’</em>  用作测试集 (即在<em>D’</em> 中没出现的样本) </strong></p><p><strong>优点：</strong></p><ul><li>实际评估模型和期望评估模型都使用 m 个训练样本</li><li>保证了仍有数据总量约1/3的、没在训练集中出现的样本用于测试</li><li><font color="#0099ff">自助法在数据量较小，难以有效划分训练集/测试集时很有用</font></li></ul><p><strong>缺点：</strong></p><ul><li><font color="#0099ff">自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差</font></li></ul><hr><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p><strong>性能度量(performance measure)：</strong> 衡量模型泛化能力的评价标准</p><ul><li>回归任务<ul><li>均方误差</li></ul></li><li>分类任务<ul><li>错误率和精度</li><li>查准率、查全率和F1</li><li>ROC和AUC</li><li>代价敏感错误率和代价曲线</li></ul></li></ul><h4 id="1-回归任务的性能度量-——-均方误差"><a href="#1-回归任务的性能度量-——-均方误差" class="headerlink" title="1. 回归任务的性能度量 —— 均方误差"></a>1. 回归任务的性能度量 —— 均方误差</h4><p><strong>离散样本：</strong></p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}{(f(x_i)-y_i)^{2}}</script><p><strong>连续样本：</strong> 设 <strong>数据分布 D</strong> 和 <strong>概率密度 p(·) </strong></p><script type="math/tex; mode=display">E(f;D) = \int_{x~D}{(f(x) - y)^2p(x)dx}</script><p><strong>性能度量方法：</strong> 通常，均方误差大的模型性能差，均方误差小的模型性能好。</p><h4 id="2-分类任务的性能度量1-——-错误率与精度"><a href="#2-分类任务的性能度量1-——-错误率与精度" class="headerlink" title="2. 分类任务的性能度量1 —— 错误率与精度"></a>2. 分类任务的性能度量1 —— 错误率与精度</h4><p><strong>错误率(error rate)：</strong> 分类错误的样本占样本总数的比例</p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)\ne y_i)}</script><p><strong>精度(Accuracy)：</strong> 分类正确的样本占样本总数的比例</p><script type="math/tex; mode=display">acc(f;D) = \frac{1}{m}\sum_{i = 1}^{m}II{(f(x_i)= y_i)}       = 1 - E(f;D)</script><p><strong>性能度量方法：</strong> 通常，错误率低精度高的模型性能好，错误率高精度低的模型性能差。</p><h4 id="3-分类任务的性能度量2-——-查准率、查全率与F1"><a href="#3-分类任务的性能度量2-——-查准率、查全率与F1" class="headerlink" title="3. 分类任务的性能度量2 —— 查准率、查全率与F1"></a>3. 分类任务的性能度量2 —— 查准率、查全率与F1</h4><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure3.PNG" alt="Figure3"></p><p><strong>查准率/准确率(Precision)：</strong> 【真正例样本数】与【预测结果是正例的样本数】的比值</p><script type="math/tex; mode=display">P = \frac{TP}{TP + FP}</script><p><strong>查全率/召回率(Recall)：</strong> 【真正例样本数】与【真实情况是正例的样本数】的比值</p><script type="math/tex; mode=display">R = \frac{TP}{TP + FN}</script><p><strong>解释：</strong></p><ul><li>查准率是在讲，挑出的好瓜里头，有多少是真正的好瓜，因此，若希望选出的瓜中好瓜的比例尽可能高，则查准率要高。</li><li>查全率是在讲，挑出的真正好瓜，占总共好瓜数的多少，因此，若希望将好瓜尽可能多的选出来，则查全率要高。</li></ul><p><strong>性能度量方法：</strong></p><ol><li><p><strong>直接观察数值</strong></p></li><li><p><strong>建立P-R图</strong></p><ul><li>“P-R曲线”是描述查准/查全率变化的曲线</li></ul></li></ol><ul><li><p>P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p> <img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure4.png" alt="Figure4"></p><ul><li><font color="#0099ff">当曲线没有交叉的时候：</font><ul><li>外侧学习器的性能优于内侧</li></ul></li><li><font color="#0099ff">当曲线有交叉的时候：</font><ul><li><strong>第一种</strong>方法是比较曲线下面积，但这个值不易估算</li><li><strong>第二种</strong>方法是比较两条曲线的平衡点，平衡点是“查准率 = 查全率”时的取值，在图中表示为曲线和对角线的交点，平衡点在外侧的曲线的学习器性能优于内侧</li><li><strong>第三种</strong>方法是F1度量和Fβ度量。F1是基于查准率与查全率的调和平均定义的，Fβ则是加权调和平均。</li></ul></li></ul></li></ul><p><strong>F1:</strong> 基于查准率和查全率的调和平均</p><script type="math/tex; mode=display">\frac{1}{F1} = \frac{1}{2} · (\frac{1}{P} + \frac{1}{R})</script><script type="math/tex; mode=display">F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}</script><p><strong>Fβ:</strong> 基于查准率和查全率的加权调和平均</p><script type="math/tex; mode=display">\frac{1}{F_\beta} = \frac{1}{1 + \beta^2} · (\frac{1}{P} + \frac{\beta^2}{R})</script><script type="math/tex; mode=display">F_\beta = \frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}</script><p><strong>说明：</strong> <strong>β &gt; 0 </strong>度量了查全率和查准率的相对重要性</p><ul><li><strong>β &gt; 1 </strong>时查全率有更大的影响</li><li><strong>β &lt; 1 </strong>时查准率有更大的影响</li></ul><p><strong><font color="#0099ff">在n个二分类混淆矩阵上综合考虑查准率和查全率：</font></strong></p><p><strong>方法一：宏(macro)：</strong>  先在各混淆矩阵上分别计算出查准率和查全率，记为 $(P_1, R_1)$, $(P_2, R_2)$, …… ,$(P_n, R_n)$, 再计算平均值，这样就得到了：</p><ul><li><p><strong>宏查准率(macro-P)</strong></p><script type="math/tex; mode=display">macro\_P = \frac{1}{n}\sum_{i = 1}^{n}{P_i}</script></li><li><p><strong>宏查全率(macro-R)</strong></p><script type="math/tex; mode=display">macro\_R = \frac{1}{n}\sum_{i = 1}^{n}{R_i}</script></li><li><p><strong>宏F1(macro-F1)</strong></p><script type="math/tex; mode=display">macro\_F1 = \frac{2 \times macro\_P \times macro\_R}{macro\_P + macro\_R}</script></li></ul><p><strong>方法二：微(micro)：</strong>  先将各混淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，分别记为 $\overline{TP}$，$\overline{FP}$，$\overline{TN}$，$\overline{FN}$，再基于这些平均值计算出：</p><ul><li><p><strong>微查准率(micro-P)</strong></p><script type="math/tex; mode=display">micro\_P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}</script></li><li><p><strong>微查全率(micro-R)</strong></p><script type="math/tex; mode=display">micro\_R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}</script></li><li><p><strong>微F1(micro-F1)</strong></p><script type="math/tex; mode=display">micro\_F1 = \frac{2 \times micro\_P \times micro\_R}{micro\_P + micro\_R}</script></li></ul><h4 id="4-分类任务的性能度量3-——-ROC与AUC"><a href="#4-分类任务的性能度量3-——-ROC与AUC" class="headerlink" title="4. 分类任务的性能度量3 —— ROC与AUC"></a>4. 分类任务的性能度量3 —— ROC与AUC</h4><p>与P-R图相同，ROC图通过对测试样本设置不同的<strong>阈值</strong>并与预测值比较，划分出正例和反例。再计算出真正例率和假正例率。P-R图逐个将样本作为正例，ROC图逐次与阈值进行比较后划分正例。本质上，都是将测试样本进行排序。</p><p><strong>真正例率(TPR):</strong> 【真正例样本数】与【真实情况是正例的样本数】的比值</p><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN}</script><p><strong>假正例率(FPR):</strong> 【假正例样本数】与【真实情况是反例的样本数】的比值</p><script type="math/tex; mode=display">FPR = \frac{FP}{TN + FP}</script><p><strong>ROC:</strong> 全称是“受试者工作特征” (Receiver Operating Characteristic)曲线，以真正例率为纵轴，以假正例率为横轴</p><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure5.png" alt="Figure5"></p><p><strong>性能度量方法：</strong> 绘制ROC曲线</p><ul><li><p><strong>当曲线没有交叉的时候：</strong> 外侧曲线的学习器性能优于内侧</p></li><li><p><strong>当曲线有交叉的时候：</strong> 比较ROC面积，如AUC</p><script type="math/tex; mode=display">AUC = \frac{1}{2}\sum_{i = 1}^{m - 1}{(x_{i+1}-x_{i})\times(y_{i}+y_{i+1})}</script></li></ul><h4 id="5-分类任务的性能度量4-——-代价敏感错误率与代价曲线"><a href="#5-分类任务的性能度量4-——-代价敏感错误率与代价曲线" class="headerlink" title="5. 分类任务的性能度量4 —— 代价敏感错误率与代价曲线"></a>5. 分类任务的性能度量4 —— 代价敏感错误率与代价曲线</h4><p>前面介绍的性能度量，大都隐式地假设了“<strong>均等代价</strong>”，而为权衡不同类型错误所造成的不同损失，应为错误赋予：“<strong>非均等代价</strong>”。</p><p>下图为二分类代价矩阵，其中 $cost_{ij}$ 表示将第i类样本预测为第j类样本的代价</p><p><img src="https://pic2.zhimg.com/80/v2-2070a8bd6c71ccf58a78a213430dee59_hd.png" alt="img"></p><p><strong>代价敏感(cost-sensitive)错误率:</strong> </p><script type="math/tex; mode=display">E(f;D;cost)=\frac{1}{m}(\sum_{x_i \in D^+} II (f(x_i) \ne y_i) \times cost_{01} + \sum_{x_i \in D^-} II(f(x^i) \ne y_i) \times cost_{10})</script><p><strong>性能度量的方法</strong>：绘制代价曲线</p><p>代价曲线的横轴是正例概率代价 $P(+)cost$，纵轴是归一化代价 $cost_{norm}$</p><script type="math/tex; mode=display">P(+)_{cost} = \frac{p\times cost_{01}}{p\times{cost_{01}} + (1-p)\times{cost_{10}}}</script><script type="math/tex; mode=display">cost_{norm} = \frac{FNR \times p\times cost_{01} + FPR \times (1 - p) \times cost_{10}}{p \times cost_{01} + (1 - p) \times cost_{10}}</script><hr><h3 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h3><script type="math/tex; mode=display">\begin{pmatrix}m\\m^\prime\end{pmatrix}</script><hr><h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p><strong>“偏差—方差分解”</strong> ：是解释学习器泛化性能的重要工具，在解释这个概念之前，先明确以下各变量名：</p><ul><li>测试样本：<strong>x</strong></li><li>测试样本<strong>x</strong>在数据集中的标记：$y_D$</li><li>测试样本<strong>x</strong>的真实标记：$y$</li><li>训练集：$D$</li><li>从训练集$D$中学得的模型：$f$</li><li>模型$f$在测试样本<strong>x</strong>上的真实输出：$f(x; D)$</li><li><strong>数据分布：Ɗ</strong></li></ul><p><strong>1. $\overline f(x)$ (预测输出的期望) </strong>：学习算法的期望预测</p><script type="math/tex; mode=display">\overline f(x) = E_Ɗ[f(x;D)]</script><p><strong>2. $Variance$ (方差)</strong>：使用样本数相同的不同训练集产生的方差</p><script type="math/tex; mode=display">var(x) = E_Ɗ[(f(x;D) - \overline f(x))^2]</script><font color="#0099ff">方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</font><p><strong>3. $Bias$ (偏差)</strong>：期望输出与真实标记之间的差别</p><script type="math/tex; mode=display">bias^2(x) = (\overline f(x) - y)^2</script><font color="#0099ff">偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</font><p><strong>4. $\varepsilon^2$ (噪声)</strong>：数据集标记和真实标记的方差</p><script type="math/tex; mode=display">\varepsilon ^2 = E_Ɗ[(y_Ɗ - y)^2]</script><font color="#0099ff">噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</font><p>噪声数据主要来源是训练数据的错误标签的情况以及输入数据某一维不确定的情况。</p><font color="#0099ff">为了方便讨论，假定噪声期望为0，则对算法的期望泛化误差进行分解可得：</font><p><img src="https://pic2.zhimg.com/v2-ae93d046ff25fd8d5ea74467d579cb41_r.jpg" alt="preview"></p><p>故：</p><script type="math/tex; mode=display">E(f;D) = bias^2 (x) + var(x) + \varepsilon^2</script><p> 即：<strong>泛化误差 = 偏差 + 方差 + 噪声</strong></p><p><strong><font color="#099ff">总结：</font></strong></p><ul><li><strong>偏差：</strong> </li><li><strong>方差：</strong> </li><li><strong>噪声：</strong> </li></ul><hr><h3 id="二项分布参数p的检验"><a href="#二项分布参数p的检验" class="headerlink" title="二项分布参数p的检验"></a>二项分布参数p的检验</h3><p>设某事件发生的概率为 <code>p</code> ， <code>p</code> 未知，作 <code>m</code> 次独立试验，每次观察该事件是否发生，以X记该事件发生的次数，则X服从二项分布 <code>B(m, p)</code>，现根据X检验如下假设：</p><script type="math/tex; mode=display">H_0: p \le p_0\\H_1: p > p_0</script><p>由二项分布本身的特性可知：p越小，X取到较小值的概率越大。因此，对于上述假设，一个直观上合理的检验为：</p><script type="math/tex; mode=display">\varphi: 当 X \le C 时接受H_0，否则拒绝H_0</script><p>其中，$C \in N$表示事件最大发生次数。此检验对应的功效函数为：</p><script type="math/tex; mode=display">\begin{align}\beta_\varphi(p) &= P(X > C)\\&= 1 - P(X \le C)\\&= 1 -\sum_{i = 0}^{C}\begin{pmatrix}m\\i\end{pmatrix}p^i(1-p)^{m-i}\\&=\sum_{i=C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}p^i(1-p)^{m-i}\\\end{align}</script><p>由于“p越小，X取到较小值的概率越大“可以等价表示为：$P(X \le C)$ 是关于p的减函数，所以$\beta_\varphi (p) = P(X &gt; C) = 1 - P(X \le C)$ 是关于p的增函数，那么当 $p \le p_0$ 时，$\beta_\varphi (p_0) $ 即为 $\beta_\varphi (p) $ 的上确界。</p><p>又因为，检验水平 $\alpha$  默认取最小可能水平，所以在给定检验水平 $\alpha$ 时，可以通过如下方程解得满足检验水平$\alpha$ 的整数C:</p><script type="math/tex; mode=display">\alpha = sup\{\beta_\varphi(p)\}</script><p>显然，当$p \le p_0$时：</p><script type="math/tex; mode=display">\begin{align}\alpha &= sup\{\beta_\varphi(p)\}\\&= \beta_\varphi(p_0)\\&= \sum_{i = C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i}\end{align}</script><p>对于此方程，通常不一定正好解得一个整数C使得方程成立，常见的情况是存在这样一个 $\overline {C}$  使得：</p><script type="math/tex; mode=display">\sum_{i = \overline C+1}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i} < \alpha</script><script type="math/tex; mode=display">\sum_{i = \overline C}^{m}\begin{pmatrix}m\\i\end{pmatrix}(p_0)^i(1 - p_0)^{m-i} > \alpha</script><p>此时，C只能取  $\overline {C}$  或者  $\overline {C} + 1$  ，若 C 取 $\overline C$ ，则相当于升高了校验水平 $\alpha$ , 若 C 取 $\overline{C} + 1$ 则相当于降低了检验水平 $\alpha$ ，具体如何取舍需要结合实际情况，但是通常为了减小犯第一类错误的概率，会倾向于令 C 取 $\overline C + 1$. </p><p>下面考虑如何求解 $\overline C$: 易证 $\beta_{\varphi}(p_0)$ 是关于 C 的减函数，所以再结合上述关于 $\overline C$ 的两个不等式，易推得：</p><script type="math/tex; mode=display">\overline C = minC \quad s.t.\ \sum_{i = C+1}^{m}{p_0^i + (1-p_0)^{m-i}} < \alpha</script><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（二）模型评估与选择&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（二）模型评估与选择&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（一）-- 绪论</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%20%E7%BB%AA%E8%AE%BA/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（一）-- 绪论/</id>
    <published>2019-08-05T10:39:04.000Z</published>
    <updated>2019-08-10T06:04:07.976Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（一）绪论"><a href="#《机器学习》西瓜书学习笔记（一）绪论" class="headerlink" title="《机器学习》西瓜书学习笔记（一）绪论"></a>《机器学习》西瓜书学习笔记（一）绪论</h2><a id="more"></a><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><ol><li><strong>学习算法(learning algorithm):</strong> 机器学习研究的主要内容，是关于在计算机上从数据产生“模型”的算法，即“学习算法”.</li><li><strong>学习算法的作用：</strong><ul><li>基于提供的经验数据产生模型</li><li>面对新情况，模型可提供相应的判断</li></ul></li><li><strong>模型(model):</strong> 泛指从数据中学到的结果</li><li><strong>学习器(learner):</strong> 学习算法在给定参数空间上的实例化</li></ol><hr><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><ul><li><strong>数据集(data set):</strong> 一组记录的集合</li><li><strong>示例(instance)/样本(sample)/特征向量(feature vector):</strong> 每条记录（关于一个事件或对象的描述）或空间中的每一个点（对应一个坐标向量）</li><li><strong>属性(attribute)/特征(feature):</strong> 反映事件或对象在某方面的表现或性质的事项</li><li><strong>属性值(attribute value):</strong> 属性上的取值</li><li><strong>属性空间(attribute space)/样本空间(sample space)/输入空间(input space):</strong> 属性张成的空间</li><li><strong>维数(dimensionality):</strong> 属性的个数</li><li><strong>学习(learning)/训练(training):</strong> 从数据中学得模型的过程</li><li><strong>训练数据(training data): </strong> 训练过程中使用的数据</li><li><strong>训练样本(training sample):</strong> 训练数据中的每个样本</li><li><strong>训练集(training set):</strong> 训练样本组成的集合</li><li><strong>假设(hypothesis):</strong> 学得模型对应了关于数据的某种潜在的规律</li><li><strong>真相/真实(ground-truth):</strong> 这种潜在规律的自身</li><li><strong>预测(prediction):</strong> 获得训练样本的结果信息，才能建立“预测”的模型</li><li><strong>标记(label):</strong> 关于示例结果的信息</li><li><strong>样例(example):</strong> 拥有了标记信息的示例</li><li><strong>标记空间(label space):</strong> 所有标记的集合</li><li><strong>分类(classification):</strong> 预测的离散值<ul><li>二分类(binary classification)<ul><li>正类(positive class)</li><li>负类(negative class)</li></ul></li><li>多分类(multi-class classification)</li></ul></li><li><strong>回归(regression):</strong> 预测的连续值</li><li><strong>测试(testing):</strong> 学的模型后，使用其进行预测的过程</li><li><strong>测试样本(testing sample):</strong> 被预测的样本</li><li>学习任务分类：<ul><li><font color="#0099ff">监督学习(supervised learning): 有标记</font><ul><li>分类</li><li>回归</li></ul></li><li><font color="#0099ff">无监督学习(unsupervised learning): 无标记</font><ul><li>聚类(clustering)</li></ul></li></ul></li><li><strong>泛化(generalization)能力:</strong> 学得模型适用于新样本的能力</li></ul><hr><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><ul><li>科学推理:<ul><li><strong>归纳(induction):</strong> 特殊 —-&gt; 一般，泛化</li><li><strong>假设(deduction):</strong> 一般 —-&gt; 特殊，特化</li></ul></li><li><strong>归纳学习：</strong> <ul><li>广义：从样例中学习</li><li>狭义：从训练数据学得概念，概念学习、概念形成</li></ul></li></ul><hr><h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><ul><li><strong>归纳偏好:</strong> 机器学习算法在学习过程中对某种类型假设的偏好<ul><li><strong><font color="#0099ff">任何一个有效的机器学习算法必有其归纳偏好</font></strong></li></ul></li><li><strong>“奥卡姆剃刀”原则:</strong> 若有多个假设与观察一致，则选最简单的那个</li><li><strong>“没有免费的午餐”定理(NFL定理):</strong>  总误差与学习算法无关</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（一）绪论&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（一）绪论&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer</title>
    <link href="http://sunfeng.online/2019/08/03/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer/"/>
    <id>http://sunfeng.online/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/</id>
    <published>2019-08-03T09:06:43.000Z</published>
    <updated>2019-08-08T03:28:10.718Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Planar-data-classification-with-a-hidden-layer"><a href="#Planar-data-classification-with-a-hidden-layer" class="headerlink" title="Planar data classification with a hidden layer"></a>Planar data classification with a hidden layer</h3><a id="more"></a><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression.</p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a signal hidden layer</li><li>Use units with a non-linear activation function, such as tanh</li><li>Compute the cross entropy loss</li><li>Implement forward and backward propagation</li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data and analysis.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load planar dataset</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @X: your dataset features</span></span><br><span class="line"><span class="string">        @Y: your dataset labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    X, Y = load_planar_dataset()    <span class="comment"># load dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure1.png" alt="Figure1"></p><p>You have:</p><blockquote><ul><li>a numpy array (matrix) X that contains your features (x1, x2)</li><li>a numpy array (vector) Y that contains your labels (red: 0, blue: 1)</li></ul></blockquote><p>Let’s first get a better sense of what our data is like.</p><p><strong>Exercise:</strong> How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code>and <code>Y</code> ?</p><p><strong>Code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m = X.shape[<span class="number">1</span>]  <span class="comment"># training set size</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The shape of X <span class="keyword">is</span>: (<span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">The shape of Y <span class="keyword">is</span>: (<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">I have m = <span class="number">400</span> training examples!</span><br></pre></td></tr></table></figure><hr><h3 id="Simple-Logistic-Regression"><a href="#Simple-Logistic-Regression" class="headerlink" title="Simple Logistic Regression"></a>Simple Logistic Regression</h3><p>Before building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure><p>You can now plot the decision boundary  of these models. Run the code below.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure2.png" alt="Figure2"></p><p><strong>Interpretation:</strong> The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network  will do better. Let’s try this now.</p><hr><h3 id="Neural-Network-model"><a href="#Neural-Network-model" class="headerlink" title="Neural Network model"></a>Neural Network model</h3><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure3.png" alt="Figure3"></p><p><strong>Mathematically:</strong></p><p>For one example x(i):</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure4.png" alt="Figure4"></p><p>Given the predictions on all the examples, you can also compute the cost J as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Reminder:</strong> The general methodology to build a Neural Network is to:</p><blockquote><ol><li>Define the neural network structure (# of input units, # of hidden units, etc).</li><li>Initialize the model’s parameters</li><li><font color="#0099ff"><strong>Loop:</strong></font><ul><li>Implement forward propagation</li><li>Compute loss</li><li>Implement backward propagation to get the gradients</li><li>Update parameters (gradient descent)</li></ul></li></ol></blockquote><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learned the right parameters, you can make predictions on new data.</p><h4 id="Defining-the-neural-network-structure"><a href="#Defining-the-neural-network-structure" class="headerlink" title="Defining the neural network structure"></a>Defining the neural network structure</h4><p><strong>Exercise:</strong> Define three variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- n_X: the size of the input layer</span><br><span class="line">- n_h: the size of the hidden layer (set this to <span class="number">4</span>)</span><br><span class="line">- n_y: the size of the output layer</span><br></pre></td></tr></table></figure><p><strong>Hint:</strong> Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defining the neural network structure</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Defining the neural network structure</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @x: input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @y: labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @n_x: the size of input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of output layer</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]</span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</span><br><span class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</span><br><span class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The size of the input layer is: n_x = 5</span><br><span class="line">The size of the hidden layer is: n_h = 4</span><br><span class="line">The size of the output layer is: n_y = 2</span><br></pre></td></tr></table></figure><h4 id="Initialize-the-model’s-parameters"><a href="#Initialize-the-model’s-parameters" class="headerlink" title="Initialize the model’s parameters"></a>Initialize the model’s parameters</h4><p><strong>Exercise:</strong> Implement the function <code>initialize_parameters().</code></p><p><strong>Instructions:</strong></p><ul><li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if you needed.</li><li>You will initialize the weights matrices with random values.<ul><li><strong>Use:</strong> np.random.randn (a, b) * 0.01 to randomly initialize a matrix of shape (a, b).</li></ul></li><li>You will initialize the bias vectors as zeros.<ul><li><strong>Use:</strong> np.zeros((a, b)) to initialize a matrix of shape (a, b) with zeros.</li></ul></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model's parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize the model's parameters</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @n_x: the size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">2</span>)   <span class="comment"># set a seed so that the result are consisent</span></span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y = initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00416758 -0.00056267]</span><br><span class="line"> [-0.02136196  0.01640271]</span><br><span class="line"> [-0.01793436 -0.00841747]</span><br><span class="line"> [ 0.00502881 -0.01245288]]</span><br><span class="line">b1 = [[ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]]</span><br><span class="line">W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</span><br><span class="line">b2 = [[ 0.]]</span><br></pre></td></tr></table></figure><h4 id="The-Loop"><a href="#The-Loop" class="headerlink" title="The Loop"></a>The Loop</h4><p><strong>Exercise 1:</strong> Implement <code>forward_propagation()</code> .</p><p><strong>Instructions:</strong></p><ul><li>Look above at the mathematical representation of your classifier.</li><li>You can use the function <code>sigmoid()</code> . It is built-in (imported) in the notebook (<code>planar_utils.py</code>).</li><li>You can use the function <code>tanh()</code> . It is part of the numpy library.</li><li>The steps you have to implement are:<ul><li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>paramters[&quot;...&quot;]</code> .</li><li>Implement Forward Propagation. Compute <code>Z[1]</code>, <code>A[1]</code>, <code>Z[2]</code> and <code>A[2]</code>  (the vector of all your predictions on all the examples in the training set).</li></ul></li><li>Values needed in the back propagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the back propagation function.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement forward propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    implement forward propagation</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement forward propagation to calculate A2</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.262818640198 0.091999045227 -1.30766601287 0.212877681719</span><br></pre></td></tr></table></figure><p>Now that you have computed <code>A[2]</code> (in the Python variable “A2”), which contains <code>a[2](i)</code> for every examples, you can compute the cost function as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Exercise 2:</strong> Implement <code>compute_cost()</code> to compute the value of the cost J.</p><p><strong>Instructions:</strong></p><ul><li><p>There are many ways to implement the cross-entropy loss (交叉熵损失). To help you, we give you how we would have implemented:</p><script type="math/tex; mode=display">-\sum_{i = 0}^{m}{y^{(i)}log(a^{[2](i)})}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2), Y)</span><br><span class="line">cost = -np.sum(logprobs) <span class="comment"># no need to use a for loop</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement compute cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation, of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters W1, b1, W2, b2</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @cost: cross-entropy cost</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment">#number of examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span> - A2), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">-1</span> / m * np.sum(logprobs)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># make sure cost is the dimension we expect</span></span><br><span class="line">                                <span class="comment"># E.g. turns [[17]] into 17</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = 0.693058761</span><br></pre></td></tr></table></figure><p>Using the cache computed during forward propagation, you can now implement backward propagation.</p><p><strong>Exercise 3:</strong> Implement the function <code>backward_propagation().</code></p><p><strong>Instructions:</strong> Backward propagation is usually the hardest (most mathematical) part in deep learning. o help you, here again is the slide from the lecture on backward propagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure5.png" alt="Figure5"></p><p><strong>Tips:</strong></p><ul><li>To compute dZ1 you’ll need to compute g[1]′(Z[1]). Since g[1](.)is the tanh activation function, if a=g[1](z) then g[1]′(z)=1−a2. So you can compute g[1]′(Z[1]) using <code>(1 - np.power(A1, 2))</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement the function backward propagration</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the backward propagation</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing out parameters (W1, b1, W2, b2)</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2", "A2". </span></span><br><span class="line"><span class="string">        @X: input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape(1, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve W1 and W2 from the dictionary parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve A1 and A2 from dictionary "cache"</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ 0.00301023 -0.00747267]</span><br><span class="line"> [ 0.00257968 -0.00641288]</span><br><span class="line"> [-0.00156892  0.003893  ]</span><br><span class="line"> [-0.00652037  0.01618243]]</span><br><span class="line">db1 = [[ 0.00176201]</span><br><span class="line"> [ 0.00150995]</span><br><span class="line"> [-0.00091736]</span><br><span class="line"> [-0.00381422]]</span><br><span class="line">dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]</span><br><span class="line">db2 = [[-0.16655712]]</span><br></pre></td></tr></table></figure><p><strong>Exercise 4:</strong> Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p><p><strong>General gradient descent rule:</strong> θ=θ−α∂J∂θ where α is the learning rate and θ represents a parameter.</p><p><strong>Illustration:</strong> The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure6.gif" alt="Figure 6"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure7.gif" alt="Figure 7"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Update parameters using the gradient descent update rule</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradient with respect to different parameters</span></span><br><span class="line"><span class="string">        @learning_rate: the learning rate used to update parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve each grident from the dictionary "grads"</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00643025  0.01936718]</span><br><span class="line"> [-0.02410458  0.03978052]</span><br><span class="line"> [-0.01653973 -0.02096177]</span><br><span class="line"> [ 0.01046864 -0.05990141]]</span><br><span class="line">b1 = [[ -1.02420756e-06]</span><br><span class="line"> [  1.27373948e-05]</span><br><span class="line"> [  8.32996807e-07]</span><br><span class="line"> [ -3.20136836e-06]]</span><br><span class="line">W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]</span><br><span class="line">b2 = [[ 0.00010457]]</span><br></pre></td></tr></table></figure><h4 id="Integrate-part-5-1-5-2-and-5-3-in-nn-model"><a href="#Integrate-part-5-1-5-2-and-5-3-in-nn-model" class="headerlink" title="Integrate part 5.1, 5.2 and 5.3 in nn_model()"></a>Integrate part 5.1, 5.2 and 5.3 in <code>nn_model()</code></h4><p><strong>Question:</strong> Build your neural network model in <code>nn_model()</code>.</p><p><strong>Instructions: </strong> The neural network model has to use the previous functions in the right order.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all function into the nerual network model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Build your neural network in nn_model</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X: dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: labels of shape (1, number of exampless)</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @num_iterations: Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">        @print_cost: if True, print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: parameters learnt by the model. They can then be used to predict</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize paramters, then retrieve W1, b1, W2, b2.</span></span><br><span class="line">    <span class="comment"># Inputs: "n_x, n_h, n_y"</span></span><br><span class="line">    <span class="comment"># Outputs: " parameters(W1, b1, W2, b2)"</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation. </span></span><br><span class="line">        <span class="comment"># Inputs: "X, parameters". </span></span><br><span class="line">        <span class="comment"># Outputs: "A2, cache"</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost Function.</span></span><br><span class="line">        <span class="comment"># Inputs: "A2, Y, parameters"</span></span><br><span class="line">        <span class="comment"># Output: "cost"</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, cache, X, Y"</span></span><br><span class="line">        <span class="comment"># Outputs: "grads"</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters by using gradient descent.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, grads"</span></span><br><span class="line">        <span class="comment"># Outputs: "parameters"</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations:</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iterations %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.692739</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.000218</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.000107</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.000071</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.000053</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.000042</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.000035</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.000030</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.000026</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.000023</span></span><br><span class="line">W1 = [[<span class="number">-0.65848169</span>  <span class="number">1.21866811</span>]</span><br><span class="line"> [<span class="number">-0.76204273</span>  <span class="number">1.39377573</span>]</span><br><span class="line"> [ <span class="number">0.5792005</span>  <span class="number">-1.10397703</span>]</span><br><span class="line"> [ <span class="number">0.76773391</span> <span class="number">-1.41477129</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.287592</span>  ]</span><br><span class="line"> [ <span class="number">0.3511264</span> ]</span><br><span class="line"> [<span class="number">-0.2431246</span> ]</span><br><span class="line"> [<span class="number">-0.35772805</span>]]</span><br><span class="line">W2 = [[<span class="number">-2.45566237</span> <span class="number">-3.27042274</span>  <span class="number">2.00784958</span>  <span class="number">3.36773273</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.20459656</span>]]</span><br></pre></td></tr></table></figure><h4 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h4><p><strong>Question:</strong> Use your model to predict by building <code>predict()</code>. Use forward propagation to predict results.</p><p><strong>Reminder:</strong> </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure8.png" alt="Figure8"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use forward propagation to predict results</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments: </span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your paramters</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @predictions: vector of predictions of our model (red: 0 / bule: 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classfies to 0/1 using 0.5 as threshold</span></span><br><span class="line">    A2 = forward_propagation(X, parameters)[<span class="number">0</span>]</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions mean = <span class="number">0.666666666667</span></span><br></pre></td></tr></table></figure><font color="#0099">It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of `n_h` hidden units.</font><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693048</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.288083</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.254385</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.233864</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.226792</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.222644</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.219731</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.217504</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.219454</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.218607</span></span><br><span class="line">Accuracy: <span class="number">90</span>%</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure9.png" alt="Figure9"></p><p><strong>Interpretation:</strong>  Accuracy is really high compared to Logistic Regression. The model has learned the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.</p><p>Now, let’s try out several hidden layer sizes.</p><h4 id="Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="Tuning hidden layer size(optional/ungraded exercise)"></a>Tuning hidden layer size(optional/ungraded exercise)</h4><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)                                <span class="comment"># ？？？</span></span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>) </span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)  <span class="comment"># ???</span></span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Accuracy <span class="keyword">for</span> <span class="number">1</span> hidden units: <span class="number">67.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">2</span> hidden units: <span class="number">67.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">3</span> hidden units: <span class="number">90.75</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">4</span> hidden units: <span class="number">90.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">5</span> hidden units: <span class="number">91.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">20</span> hidden units: <span class="number">90.0</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">50</span> hidden units: <span class="number">90.25</span> %</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure11.png" alt="Figure11"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure12.png" alt="Figure12"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure10.png" alt="Figure10"></p><p><strong>Interpretation:</strong></p><ul><li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.</li><li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting.</li><li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting.</li></ul><p><strong><font color="#00f">You’ve learnt to:</font></strong></p><ul><li>Build a complete neural network with a hidden layer</li><li>Make a good use of a non-linear unit</li><li>Implemented forward propagation and backpropagation, and trained a neural network</li><li>See the impact of varying the hidden layer size, including overfitting.</li></ul><p>Nice work!</p><hr><h3 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment3_shallow_neural_network" target="_blank" rel="noopener">Source Code</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Planar-data-classification-with-a-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-a-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with a hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with a hidden layer&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>如何上传本地代码到github</title>
    <link href="http://sunfeng.online/2019/08/02/%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github/"/>
    <id>http://sunfeng.online/2019/08/02/如何上传本地代码到github/</id>
    <published>2019-08-02T08:34:43.000Z</published>
    <updated>2019-08-02T08:57:37.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何上传本地代码到github"><a href="#如何上传本地代码到github" class="headerlink" title="如何上传本地代码到github"></a>如何上传本地代码到github</h2><a id="more"></a><h3 id="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："><a href="#第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：" class="headerlink" title="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："></a>第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure1.jpg" alt="figure1"></p><p>点击<strong>Clone or download</strong>按钮，复制弹出的地址<strong>git@github.com:***/***.git</strong></p><p>注意要用SSH地址。</p><hr><h3 id="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"><a href="#第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令" class="headerlink" title="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"></a>第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><hr><h3 id="第三步：将项目的所有文件添加到仓库中"><a href="#第三步：将项目的所有文件添加到仓库中" class="headerlink" title="第三步：将项目的所有文件添加到仓库中"></a>第三步：将项目的所有文件添加到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><hr><h3 id="第四步：将添加的文件提交到仓库中"><a href="#第四步：将添加的文件提交到仓库中" class="headerlink" title="第四步：将添加的文件提交到仓库中"></a>第四步：将添加的文件提交到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;注释语句&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="第五步：将本地仓库关联到github上"><a href="#第五步：将本地仓库关联到github上" class="headerlink" title="第五步：将本地仓库关联到github上"></a>第五步：将本地仓库关联到github上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:***/test.git</span><br></pre></td></tr></table></figure><hr><h3 id="第六步：上传之前，先要pull一下，执行如下命令："><a href="#第六步：上传之前，先要pull一下，执行如下命令：" class="headerlink" title="第六步：上传之前，先要pull一下，执行如下命令："></a>第六步：上传之前，先要pull一下，执行如下命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><hr><h3 id="第七步：上传代码到github远程仓库"><a href="#第七步：上传代码到github远程仓库" class="headerlink" title="第七步：上传代码到github远程仓库"></a>第七步：上传代码到github远程仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><hr><p><strong>祝你成功！</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;如何上传本地代码到github&quot;&gt;&lt;a href=&quot;#如何上传本地代码到github&quot; class=&quot;headerlink&quot; title=&quot;如何上传本地代码到github&quot;&gt;&lt;/a&gt;如何上传本地代码到github&lt;/h2&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="github" scheme="http://sunfeng.online/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset</title>
    <link href="http://sunfeng.online/2019/08/01/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/"/>
    <id>http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/</id>
    <published>2019-08-01T08:06:21.000Z</published>
    <updated>2019-08-08T03:28:35.916Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a><strong>Logistic Regression with a Neural Network mindset</strong></h3><a id="more"></a><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li><p>Build the general architecture of a learning algorithm, including:</p><ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent)</li></ul></li><li><p>Gather all three functions above into a main function, in the right order.</p></li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li></ul><p>code ————-&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure><hr><h3 id="Overview-of-the-Problem-set"><a href="#Overview-of-the-Problem-set" class="headerlink" title="Overview of the Problem set"></a>Overview of the Problem set</h3><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><blockquote><ul><li>a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0)</li><li>a test set of m_test images labeled as cat or non-cat</li><li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)</li></ul></blockquote><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data preprocessing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_Preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load dataset and preprocess dataset</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @train_set_x: your train set features</span></span><br><span class="line"><span class="string">        @train_set_y: your train set labels</span></span><br><span class="line"><span class="string">        @test_set_x: your test set features</span></span><br><span class="line"><span class="string">        @test_set_y: your test set labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line">    <span class="comment"># load dataset from dataset files</span></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    <span class="comment"># vectorize the features of each examples</span></span><br><span class="line">    train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">    test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line">    <span class="comment"># normalize the features vector</span></span><br><span class="line">    <span class="keyword">return</span> train_set_x, train_set_y, test_set_x, test_set_y, classes</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:**</font><font color="#0099ff">Common steps for pre-processing a new dataset are:</font><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</li></ul><hr><h3 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h3><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure1.png" alt="Figure1"></p><p><strong>Mathematical expression of the algorithm:</strong></p><p>For one examples x(i):</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/Figure2.png" alt="Figure2"></p><p>The cost is the computed by summing over all training examples:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure3.png" alt="Figure3"></p><p><strong>Key steps:</strong> In this exercise, you will carry out the following steps:</p><blockquote><ul><li>Initialize the parameters of the model</li><li>Learn the parameters for the model by minimizing the cost</li><li>Use the learned parameters to make predictions (on the test set)</li><li>Analyze the results and conclude</li></ul></blockquote><hr><h3 id="Building-the-parts-of-our-algorithm"><a href="#Building-the-parts-of-our-algorithm" class="headerlink" title="Building the parts of our algorithm"></a>Building the parts of our algorithm</h3><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features)</li><li>Initialize the model’s parameters</li><li>Loop:<ul><li><strong>Calculate current loss (forward propagation)</strong></li><li><strong>Calculate current gradient (backward propagation)</strong></li><li><strong>Update parameters (gradient descent)</strong></li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call model().</p><h4 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h4><p><strong>Exercise:</strong> using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute </p><script type="math/tex; mode=display">sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}</script><p>to make predictions. Use np.exp().</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @z: A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @s: sigmoid(z)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h4 id="Initializing-parameters"><a href="#Initializing-parameters" class="headerlink" title="Initializing parameters"></a>Initializing parameters</h4><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initializing parameters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @dim: size of the w vector we want</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @w: initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">        @b: initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function propagation() that computes the cost function and its gradient.</p><p><strong>Hints(提示):</strong></p><font color="#0099ff">Forward Propagation:</font><ul><li><p>You get x</p></li><li><p>You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$</p></li><li><p>You calculate the cost function: </p><p>$J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 -  y^{(i)})log(1 - a^{(i)}) }$</p></li></ul><p>Here are the two formulas you will be using:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure4.png" alt="Figure4"></p><p><strong>code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward and Backword propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the</span></span><br><span class="line"><span class="string">    propagation explained above</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @cost: negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">        @dw: gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">        @db: gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># nx</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b))  <span class="comment"># compute activation</span></span><br><span class="line">    cost = -(np.dot(Y, np.log(A).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A).T)) / m <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m <span class="comment"># compute dw</span></span><br><span class="line">    db = np.sum(A - Y) / m      <span class="comment"># compute db</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 把shape中为1的维度去掉</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    <span class="comment"># 判断剩下的是否为空</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, </span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Optimization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @num_iterations: number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        @learning_rate: learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        @print_cost: True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">        @grads: dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">        @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">        You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        (1) Calculate the cost and the gradient for the current parameters. Use propagate()</span></span><br><span class="line"><span class="string">        (2) Update the parameters using gradient descent rule for w and b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update rule</span></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %d: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li>Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$</li><li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_Prediction</code>. </li></ol><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Graded function: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned </span></span><br><span class="line"><span class="string">    logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @Y_prediction: a numpy array (vector) containing all prediction</span></span><br><span class="line"><span class="string">                       (0 / 1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m =  X.shape[<span class="number">1</span>]     <span class="comment"># number of examples</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b)) <span class="comment"># (1, m)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0, i] to actual predictions p[0, i]</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:** You've implemented several functions that:</font><ul><li>initialize (w, b)</li><li>Optimize the loss iteratively to learn parameters (w, b):<ul><li>computing the cost and its gradient</li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w, b) to predict the labels for a given set of examples</li></ul><hr><h3 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h3><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p><blockquote><ul><li>Y_prediction for your predictions on the test set</li><li>Y_prediction_train for your predictions on the train set</li><li>w, costs, grads for the outputs of optimize()</li></ul></blockquote><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all functions into a model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function</span></span><br><span class="line"><span class="string">    you have implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">        @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">        @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">        @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test)</span></span><br><span class="line"><span class="string">        @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">        @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">        @print_cost: Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @d: dictionary containing information about the model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initailize parameters with zeros</span></span><br><span class="line">    w,b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])   <span class="comment"># num_px * num_px * 3, w: (dim, 1), b: a scalar</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples </span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span> : costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span> : Y_prediction_test,</span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">"w"</span> : w,</span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span> : num_iterations</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =<span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.376007</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.331463</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.303273</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.279880</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.260042</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.242941</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.228004</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.214820</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.203078</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.192544</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.183033</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.174399</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.166521</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.159305</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.152667</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.146542</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure><p><strong>Comment:</strong> Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. </p><font color="#0099ff">Let's also plot the cost function and the gradients:</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure5.png" alt="Figure5"></p><p><strong>Interpretation</strong>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p><hr><h3 id="Further-analysis"><a href="#Further-analysis" class="headerlink" title="Further analysis"></a>Further analysis</h3><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α.</p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow= <span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure6.png" alt="Figure6"></p><p><strong>Interpretation</strong>:</p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you:<ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li></ul></li></ul><hr><p><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment2_logistics_regression" target="_blank" rel="noopener">Source Code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic-Regression-with-a-Neural-Network-mindset&quot;&gt;&lt;a href=&quot;#Logistic-Regression-with-a-Neural-Network-mindset&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression with a Neural Network mindset&quot;&gt;&lt;/a&gt;&lt;strong&gt;Logistic Regression with a Neural Network mindset&lt;/strong&gt;&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/tags/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hexo教程：使用Hexo写博客</title>
    <link href="http://sunfeng.online/2019/07/31/hello-hexo-markdown/"/>
    <id>http://sunfeng.online/2019/07/31/hello-hexo-markdown/</id>
    <published>2019-07-31T12:44:47.000Z</published>
    <updated>2019-08-02T08:54:22.531Z</updated>
    
    <content type="html"><![CDATA[<p>尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。<br><a id="more"></a></p><hr><h3 id="新建文档"><a href="#新建文档" class="headerlink" title="新建文档"></a>新建文档</h3><p>假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可：</p><blockquote><p>$ hexo new “hello hexo markdown”</p></blockquote><p>上述命令的结果是在 <code>./hexo/source/_posts</code> 路径下新建了一个 <code>hello-hexo-markdown.md</code> 文件。</p><p>然后，我们就可以打开编辑器尽情地写作了。</p><hr><h3 id="文档格式"><a href="#文档格式" class="headerlink" title="文档格式"></a>文档格式</h3><p>我们使用文本编辑器打开刚刚新建的 <code>hello-hexo-markdown.md</code> 文件，会发现其中已经存在内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2019-07-31 20:44:47</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 <code>---</code> 分隔的区域。其中，</p><p><code>title</code>的值是当前文档名，也是将来在网页中显示的文章标题</p><p><code>date</code>值是我们新建文档时的当时地区时间</p><p><code>tags</code>值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>上面的配置参数为这篇文档贴上了 <code>hello</code>、<code>hexo</code>、<code>mardown</code> 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。</p><p>除了以上这些，还有很多预先定义的参数<a href="https://hexo.io/zh-cn/docs/front-matter.html" target="_blank" rel="noopener">Front_mtter</a>，我们这里选取一个常用且较为典型的配置参数<code>categories</code>讲解一下。</p><h4 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h4><p><code>categories</code> 是用来给文章分类的，它跟 <code>tags</code> 不同的是其具有顺序性和层次性。</p><p>例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，<code>categories</code> 有点儿像新建文件夹对文档进行分门别类的归置。</p><p><code>categories</code> 的用法同 <code>tags</code> 一样，只不过斗个 categories 值是分先后顺序的。</p><hr><h3 id="引用资源"><a href="#引用资源" class="headerlink" title="引用资源"></a>引用资源</h3><p>写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。</p><p>这时，有两种解决办法：</p><ol><li>使用绝对路径引用资源，在 Web 世界中就是资源的 URL</li><li>使用相对路径引用资源</li></ol><h4 id="文章资料文件夹"><a href="#文章资料文件夹" class="headerlink" title="文章资料文件夹"></a>文章资料文件夹</h4><p>如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。</p><p>使用文本编辑器打开站点根目录下的 <code>_ config.yml</code> 文件，将 <code>post_asset_folder</code> 值设置为 <code>true</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 <code>hexo new &lt;title&gt;</code> 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。</p><h4 id="相对路径引用的标签插件"><a href="#相对路径引用的标签插件" class="headerlink" title="相对路径引用的标签插件"></a>相对路径引用的标签插件</h4><p>通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>比如说：当你打开文章资源文件夹功能后，你把一个 <code>example.jpg</code> 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 <code>![](/example.jpg)</code> ，它将 <em>不会</em> 出现在首页上。（但是它会在文章中按你期待的方式工作）</p><p><strong>！！！注意：</strong> 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 <code>./资源名称</code>，不用在引用路径中添加同名文件夹目录层级。</p><p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img example.jpg This is an example image %&#125;</span><br></pre></td></tr></table></figure><p>通过这种方式，图片将会同时出现在文章和主页以及归档页中。</p><hr><h3 id="文章摘要"><a href="#文章摘要" class="headerlink" title="文章摘要"></a>文章摘要</h3><p>有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。</p><p>所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。</p><p>设置文章摘要，我们只需在想显示为摘要的内容之后添 <code>&lt;!-- more --&gt;</code> 即可。像下面这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br><span class="line">我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧</span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line">紧接着文章摘要的正文内容</span><br></pre></td></tr></table></figure><p>这样，<code>&lt;!-- more --&gt;</code> 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。</p><p>注意！文章摘要在文章详情页是正文中最前面的内容。</p><hr><h3 id="生成文件"><a href="#生成文件" class="headerlink" title="生成文件"></a>生成文件</h3><h4 id="清除缓存文件"><a href="#清除缓存文件" class="headerlink" title="清除缓存文件"></a>清除缓存文件</h4><p>为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><p>上述命令会清除在本地站点文件下的缓存文件(<code>db.json</code>)和已有的静态文件(<code>public</code>).</p><h4 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h4><p>写好MarkDown文档之后，我们就可以使用以下命令生成静态文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>然后我们就可以启动 Hexo 服务器，使用浏览器打开 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000</a> 查看效果了。</p><hr><h3 id="示范"><a href="#示范" class="headerlink" title="示范"></a>示范</h3><p>下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。</p><p><img src="/2019/07/31/hello-hexo-markdown/example.png" alt="效果截图"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="hello" scheme="http://sunfeng.online/tags/hello/"/>
    
      <category term="hexo" scheme="http://sunfeng.online/tags/hexo/"/>
    
      <category term="markdown" scheme="http://sunfeng.online/tags/markdown/"/>
    
  </entry>
  
</feed>
