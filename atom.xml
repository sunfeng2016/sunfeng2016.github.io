<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunFeng&#39;s Blog</title>
  
  <subtitle>学习，敲码，孤独终老！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunfeng.online/"/>
  <updated>2019-08-06T02:32:12.403Z</updated>
  <id>http://sunfeng.online/</id>
  
  <author>
    <name>SunFeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（二）-- 模型评估与选择</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/</id>
    <published>2019-08-05T12:23:06.000Z</published>
    <updated>2019-08-06T02:32:12.403Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（二）模型评估与选择"><a href="#《机器学习》西瓜书学习笔记（二）模型评估与选择" class="headerlink" title="《机器学习》西瓜书学习笔记（二）模型评估与选择"></a>《机器学习》西瓜书学习笔记（二）模型评估与选择</h2><a id="more"></a><h3 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h3><ul><li><strong>错误率 (error rate)：</strong> 分类错误样本数占总样本数的比例</li><li><strong>准确率 (accuracy)：</strong> 分类正确样本数占总样本数的比例</li><li><strong>误差 (error)：</strong> 学习器的实际预测输出与真实输出之间的差异</li><li><strong><font color="#0099ff">训练误差 (training error)/经验误差(empirical error)：</font></strong> 学习器在训练集上的误差</li><li><strong>泛化误差(generalization error):</strong> 学习器在新样本上的误差</li><li><strong><font color="#0099ff">过拟合(overfitting):</font></strong> 学习能力过于强大。学习器把训练样本学得太好，导致将训练样本中自身含有的特点当成所有潜在样本都会具有的一般性质，从而训练后使得泛化性能下降</li><li><strong>欠拟合(underfitting):</strong> 学习能力底下。对训练样本的一般性质尚未学好</li></ul><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure1.PNG" alt="Figure1"></p><hr><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p><strong>理想：</strong> 通过评估学习器的<strong>泛化误差</strong>，选出泛化误差最小的学习器</p><p><strong>实际：</strong> <strong>泛化误差</strong> 只能通过测试集上的 <strong>测试误差</strong>  作为近似</p><p><strong><font color="#0099ff">机器学习的目的是产生泛化能力好的模型，那么什么样的模型才是泛化能力好的模型呢? 这需要按照一定的评估方法和度量指标去衡量。</font></strong></p><p>给定一个包含m个样例的数据集 $ D = \{(x_1, y_1), (x_2, y_2), … ,(x_m, y_m)\}$ ,通过对D进行适当的处理，从中产生出训练集S和测试集T，<strong>测试集应该尽可能与训练集互斥</strong>，常见的方法有以下三种：</p><h4 id="1-留出法"><a href="#1-留出法" class="headerlink" title="1.留出法"></a>1.留出法</h4><p><strong>留出法：</strong> 直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个集合作为测试集T，即 <em>D=S</em>∪<em>T</em>，<em>S</em>∩<em>T=</em>∅. </p><p><strong>注意点：</strong></p><ul><li>要保持数据分布的一致性<ul><li>分层采样</li></ul></li><li>采用多次随机划分取均值的评估方法</li><li>测试集的比例应当适当(2/3 ~ 4/5)</li></ul><h4 id="2-交叉验证法"><a href="#2-交叉验证法" class="headerlink" title="2.交叉验证法"></a>2.交叉验证法</h4><p><strong>交叉验证法：</strong> 将数据集平均分成 <code>K</code> 份，并尽量保证每份数据分布一致。依次用其中 <code>K - 1</code> 份作为训练集，剩下的一份作为测试集。这样就有 <code>K</code> 组训练/测试集。从而可以进行 <code>K</code> 次训练和测试，返回K测测试结果的均值，也称为”K折交叉验证法”</p><p><img src="/2019/08/05/《机器学习》西瓜书学习笔记（二）-- 模型评估与选择/Figure2.PNG" alt="Figure2"></p><p><strong>注意点：</strong></p><ul><li>K折交叉验证要随机使用不同的划分重复p次，最终取p次K折交叉验证的均值</li></ul><p><strong>留一法：</strong> 若令K = m, 则称为“留一法 (LOO)”</p><ul><li>优点<ul><li>不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集，即每个子集只含有一个样本</li><li>被实际评估的模型与期望评估的用D训练出的模型很相似，因为使用的训练集与初始数据集相比至少一个样本</li></ul></li><li>缺点<ul><li>当数据集比较大时，训练m个模型的计算开销比较大</li></ul></li></ul><h4 id="3-自助法"><a href="#3-自助法" class="headerlink" title="3.自助法"></a>3.自助法</h4><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（二）模型评估与选择&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（二）模型评估与选择&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（二）模型评估与选择&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》西瓜书学习笔记（一）-- 绪论</title>
    <link href="http://sunfeng.online/2019/08/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%20%E7%BB%AA%E8%AE%BA/"/>
    <id>http://sunfeng.online/2019/08/05/《机器学习》西瓜书学习笔记（一）-- 绪论/</id>
    <published>2019-08-05T10:39:04.000Z</published>
    <updated>2019-08-05T12:25:53.164Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《机器学习》西瓜书学习笔记（一）绪论"><a href="#《机器学习》西瓜书学习笔记（一）绪论" class="headerlink" title="《机器学习》西瓜书学习笔记（一）绪论"></a>《机器学习》西瓜书学习笔记（一）绪论</h2><a id="more"></a><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><ol><li><strong>学习算法(learning algorithm):</strong> 机器学习研究的主要内容，是关于在计算机上从数据产生“模型”的算法，即“学习算法”.</li><li><strong>学习算法的作用：</strong><ul><li>基于提供的经验数据产生模型</li><li>面对新情况，模型可提供相应的判断</li></ul></li><li><strong>模型(model):</strong> 泛指从数据中学到的结果</li><li><strong>学习器(learner):</strong> 学习算法在给定参数空间上的实例化</li></ol><hr><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><ul><li><strong>数据集(data set):</strong> 一组记录的集合</li><li><strong>示例(instance)/样本(sample)/特征向量(feature vector):</strong> 每条记录（关于一个事件或对象的描述）或空间中的每一个点（对应一个坐标向量）</li><li><strong>属性(attribute)/特征(feature):</strong> 反映事件或对象在某方面的表现或性质的事项</li><li><strong>属性值(attribute value):</strong> 属性上的取值</li><li><strong>属性空间(attribute space)/样本空间(sample space)/输入空间(input space):</strong> 属性张成的空间</li><li><strong>维数(dimensionality):</strong> 属性的个数</li><li><strong>学习(learning)/训练(training):</strong> 从数据中学的模型的过程</li><li><strong>训练数据(training data): </strong> 训练过程中使用的数据</li><li><strong>训练样本(training sample):</strong> 训练数据中的每个样本</li><li><strong>训练集(training set):</strong> 训练样本组成的集合</li><li><strong>假设(hypothesis):</strong> 学得模型对应了关于数据的某种潜在的规律</li><li><strong>真相/真实(ground-truth):</strong> 这种潜在规律的自身</li><li><strong>预测(prediction):</strong> 获得训练样本的结果信息，才能建立“预测”的模型</li><li><strong>标记(label):</strong> 关于示例结果的信息</li><li><strong>样例(example):</strong> 拥有了标记信息的示例</li><li><strong>标记空间(label space):</strong> 所有标记的集合</li><li><strong>分类(classification):</strong> 预测的离散值<ul><li>二分类(binary classification)<ul><li>正类(positive class)</li><li>负类(negative class)</li></ul></li><li>多分类(multi-class classification)</li></ul></li><li><strong>回归(regression):</strong> 预测的连续值</li><li><strong>测试(testing):</strong> 学的模型后，使用其进行预测的过程</li><li><strong>测试样本(testing sample):</strong> 被预测的样本</li><li>学习任务分类：<ul><li><font color="#0099ff">监督学习(supervised learning): 有标记</font><ul><li>分类</li><li>回归</li></ul></li><li><font color="#0099ff">无监督学习(unsupervised learning): 无标记</font><ul><li>聚类(clustering)</li></ul></li></ul></li><li><strong>泛化(generalization)能力:</strong> 学得模型适用于新样本的能力</li></ul><hr><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><ul><li>科学推理:<ul><li><strong>归纳(induction):</strong> 特殊 —-&gt; 一般，泛化</li><li><strong>假设(deduction):</strong> 一般 —-&gt; 特殊，特化</li></ul></li><li><strong>归纳学习：</strong> <ul><li>广义：从样例中学习</li><li>狭义：从训练数据学得概念，概念学习、概念形成</li></ul></li></ul><hr><h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><ul><li><strong>归纳偏好:</strong> 机器学习算法在学习过程中对某种类型假设的偏好<ul><li><strong><font color="#0099ff">任何一个有效的机器学习算法必有其归纳偏好</font></strong></li></ul></li><li><strong>“奥卡姆剃刀”原则:</strong> 若有多个假设与观察一致，则选最简单的那个</li><li><strong>“没有免费的午餐”定理(NFL定理):</strong>  总误差与学习算法无关</li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;a href=&quot;#《机器学习》西瓜书学习笔记（一）绪论&quot; class=&quot;headerlink&quot; title=&quot;《机器学习》西瓜书学习笔记（一）绪论&quot;&gt;&lt;/a&gt;《机器学习》西瓜书学习笔记（一）绪论&lt;/h2&gt;
    
    </summary>
    
      <category term="《机器学习》西瓜书学习笔记" scheme="http://sunfeng.online/categories/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="machine learning" scheme="http://sunfeng.online/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer</title>
    <link href="http://sunfeng.online/2019/08/03/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning),%20%E7%AC%AC%E4%B8%89%E5%91%A8(Shallow%20neural%20networks)%E2%80%94%E2%80%94Programming%20assignment%203%E3%80%81Planar%20data%20classification%20with%20a%20hidden%20layer/"/>
    <id>http://sunfeng.online/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/</id>
    <published>2019-08-03T09:06:43.000Z</published>
    <updated>2019-08-04T12:35:08.617Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Planar-data-classification-with-a-hidden-layer"><a href="#Planar-data-classification-with-a-hidden-layer" class="headerlink" title="Planar data classification with a hidden layer"></a>Planar data classification with a hidden layer</h3><a id="more"></a><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression.</p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a signal hidden layer</li><li>Use units with a non-linear activation function, such as tanh</li><li>Compute the cross entropy loss</li><li>Implement forward and backward propagation</li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental packages for scientific computing with Python.</li><li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data and analysis.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure><hr><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load planar dataset</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @X: your dataset features</span></span><br><span class="line"><span class="string">        @Y: your dataset labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    X, Y = load_planar_dataset()    <span class="comment"># load dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure1.png" alt="Figure1"></p><p>You have:</p><blockquote><ul><li>a numpy array (matrix) X that contains your features (x1, x2)</li><li>a numpy array (vector) Y that contains your labels (red: 0, blue: 1)</li></ul></blockquote><p>Let’s first get a better sense of what our data is like.</p><p><strong>Exercise:</strong> How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code>and <code>Y</code> ?</p><p><strong>Code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m = X.shape[<span class="number">1</span>]  <span class="comment"># training set size</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The shape of X <span class="keyword">is</span>: (<span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">The shape of Y <span class="keyword">is</span>: (<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">I have m = <span class="number">400</span> training examples!</span><br></pre></td></tr></table></figure><hr><h3 id="Simple-Logistic-Regression"><a href="#Simple-Logistic-Regression" class="headerlink" title="Simple Logistic Regression"></a>Simple Logistic Regression</h3><p>Before building a full neural network, let’s first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure><p>You can now plot the decision boundary  of these models. Run the code below.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure2.png" alt="Figure2"></p><p><strong>Interpretation:</strong> The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network  will do better. Let’s try this now.</p><hr><h3 id="Neural-Network-model"><a href="#Neural-Network-model" class="headerlink" title="Neural Network model"></a>Neural Network model</h3><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model:</strong></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure3.png" alt="Figure3"></p><p><strong>Mathematically:</strong></p><p>For one example x(i):</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure4.png" alt="Figure4"></p><p>Given the predictions on all the examples, you can also compute the cost J as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Reminder:</strong> The general methodology to build a Neural Network is to:</p><blockquote><ol><li>Define the neural network structure (# of input units, # of hidden units, etc).</li><li>Initialize the model’s parameters</li><li><font color="#0099ff"><strong>Loop:</strong></font><ul><li>Implement forward propagation</li><li>Compute loss</li><li>Implement backward propagation to get the gradients</li><li>Update parameters (gradient descent)</li></ul></li></ol></blockquote><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learned the right parameters, you can make predictions on new data.</p><h4 id="Defining-the-neural-network-structure"><a href="#Defining-the-neural-network-structure" class="headerlink" title="Defining the neural network structure"></a>Defining the neural network structure</h4><p><strong>Exercise:</strong> Define three variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- n_X: the size of the input layer</span><br><span class="line">- n_h: the size of the hidden layer (set this to <span class="number">4</span>)</span><br><span class="line">- n_y: the size of the output layer</span><br></pre></td></tr></table></figure><p><strong>Hint:</strong> Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defining the neural network structure</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Defining the neural network structure</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @x: input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">        @y: labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @n_x: the size of input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of output layer</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]</span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</span><br><span class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</span><br><span class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The size of the input layer is: n_x = 5</span><br><span class="line">The size of the hidden layer is: n_h = 4</span><br><span class="line">The size of the output layer is: n_y = 2</span><br></pre></td></tr></table></figure><h4 id="Initialize-the-model’s-parameters"><a href="#Initialize-the-model’s-parameters" class="headerlink" title="Initialize the model’s parameters"></a>Initialize the model’s parameters</h4><p><strong>Exercise:</strong> Implement the function <code>initialize_parameters().</code></p><p><strong>Instructions:</strong></p><ul><li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if you needed.</li><li>You will initialize the weights matrices with random values.<ul><li><strong>Use:</strong> np.random.randn (a, b) * 0.01 to randomly initialize a matrix of shape (a, b).</li></ul></li><li>You will initialize the bias vectors as zeros.<ul><li><strong>Use:</strong> np.zeros((a, b)) to initialize a matrix of shape (a, b) with zeros.</li></ul></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model's parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Initialize the model's parameters</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @n_x: the size of the input layer</span></span><br><span class="line"><span class="string">        @n_h: the size of the hidden layer</span></span><br><span class="line"><span class="string">        @n_y: the size of the output layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">            @W1: weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">            @b1: bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">            @W2: weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">            @b2: bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">2</span>)   <span class="comment"># set a seed so that the result are consisent</span></span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y = initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00416758 -0.00056267]</span><br><span class="line"> [-0.02136196  0.01640271]</span><br><span class="line"> [-0.01793436 -0.00841747]</span><br><span class="line"> [ 0.00502881 -0.01245288]]</span><br><span class="line">b1 = [[ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]</span><br><span class="line"> [ 0.]]</span><br><span class="line">W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</span><br><span class="line">b2 = [[ 0.]]</span><br></pre></td></tr></table></figure><h4 id="The-Loop"><a href="#The-Loop" class="headerlink" title="The Loop"></a>The Loop</h4><p><strong>Exercise 1:</strong> Implement <code>forward_propagation()</code> .</p><p><strong>Instructions:</strong></p><ul><li>Look above at the mathematical representation of your classifier.</li><li>You can use the function <code>sigmoid()</code> . It is built-in (imported) in the notebook (<code>planar_utils.py</code>).</li><li>You can use the function <code>tanh()</code> . It is part of the numpy library.</li><li>The steps you have to implement are:<ul><li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>paramters[&quot;...&quot;]</code> .</li><li>Implement Forward Propagation. Compute <code>Z[1]</code>, <code>A[1]</code>, <code>Z[2]</code> and <code>A[2]</code>  (the vector of all your predictions on all the examples in the training set).</li></ul></li><li>Values needed in the back propagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the back propagation function.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement forward propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    implement forward propagation</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement forward propagation to calculate A2</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.262818640198 0.091999045227 -1.30766601287 0.212877681719</span><br></pre></td></tr></table></figure><p>Now that you have computed <code>A[2]</code> (in the Python variable “A2”), which contains <code>a[2](i)</code> for every examples, you can compute the cost function as follows:</p><script type="math/tex; mode=display">J = -\frac{1}{m}\sum_{i = 0}^{m}{(y^{(i)}{log(a^{[2](i)})} + (1 - y^{(i)}){log(1 - a^{[2](i)}))}}</script><p><strong>Exercise 2:</strong> Implement <code>compute_cost()</code> to compute the value of the cost J.</p><p><strong>Instructions:</strong></p><ul><li><p>There are many ways to implement the cross-entropy loss (交叉熵损失). To help you, we give you how we would have implemented:</p><script type="math/tex; mode=display">-\sum_{i = 0}^{m}{y^{(i)}log(a^{[2](i)})}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2), Y)</span><br><span class="line">cost = -np.sum(logprobs) <span class="comment"># no need to use a for loop</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement compute cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @A2: The sigmoid output of the second activation, of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters W1, b1, W2, b2</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @cost: cross-entropy cost</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment">#number of examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span> - A2), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">-1</span> / m * np.sum(logprobs)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># make sure cost is the dimension we expect</span></span><br><span class="line">                                <span class="comment"># E.g. turns [[17]] into 17</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cost = 0.693058761</span><br></pre></td></tr></table></figure><p>Using the cache computed during forward propagation, you can now implement backward propagation.</p><p><strong>Exercise 3:</strong> Implement the function <code>backward_propagation().</code></p><p><strong>Instructions:</strong> Backward propagation is usually the hardest (most mathematical) part in deep learning. o help you, here again is the slide from the lecture on backward propagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure5.png" alt="Figure5"></p><p><strong>Tips:</strong></p><ul><li>To compute dZ1 you’ll need to compute g[1]′(Z[1]). Since g[1](.)is the tanh activation function, if a=g[1](z) then g[1]′(z)=1−a2. So you can compute g[1]′(Z[1]) using <code>(1 - np.power(A1, 2))</code>.</li></ul><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement the function backward propagration</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the backward propagation</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing out parameters (W1, b1, W2, b2)</span></span><br><span class="line"><span class="string">        @cache: a dictionary containing "Z1", "A1", "Z2", "A2". </span></span><br><span class="line"><span class="string">        @X: input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: "true" labels vector of shape(1, number of examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve W1 and W2 from the dictionary parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve A1 and A2 from dictionary "cache"</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dW1 = [[ 0.00301023 -0.00747267]</span><br><span class="line"> [ 0.00257968 -0.00641288]</span><br><span class="line"> [-0.00156892  0.003893  ]</span><br><span class="line"> [-0.00652037  0.01618243]]</span><br><span class="line">db1 = [[ 0.00176201]</span><br><span class="line"> [ 0.00150995]</span><br><span class="line"> [-0.00091736]</span><br><span class="line"> [-0.00381422]]</span><br><span class="line">dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]</span><br><span class="line">db2 = [[-0.16655712]]</span><br></pre></td></tr></table></figure><p><strong>Exercise 4:</strong> Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p><p><strong>General gradient descent rule:</strong> θ=θ−α∂J∂θ where α is the learning rate and θ represents a parameter.</p><p><strong>Illustration:</strong> The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure6.gif" alt="Figure 6"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure7.gif" alt="Figure 7"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Update parameters using the gradient descent update rule</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your parameters</span></span><br><span class="line"><span class="string">        @grads: python dictionary containing your gradient with respect to different parameters</span></span><br><span class="line"><span class="string">        @learning_rate: the learning rate used to update parameters</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your updated parameters</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve each grident from the dictionary "grads"</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[-0.00643025  0.01936718]</span><br><span class="line"> [-0.02410458  0.03978052]</span><br><span class="line"> [-0.01653973 -0.02096177]</span><br><span class="line"> [ 0.01046864 -0.05990141]]</span><br><span class="line">b1 = [[ -1.02420756e-06]</span><br><span class="line"> [  1.27373948e-05]</span><br><span class="line"> [  8.32996807e-07]</span><br><span class="line"> [ -3.20136836e-06]]</span><br><span class="line">W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]</span><br><span class="line">b2 = [[ 0.00010457]]</span><br></pre></td></tr></table></figure><h4 id="Integrate-part-5-1-5-2-and-5-3-in-nn-model"><a href="#Integrate-part-5-1-5-2-and-5-3-in-nn-model" class="headerlink" title="Integrate part 5.1, 5.2 and 5.3 in nn_model()"></a>Integrate part 5.1, 5.2 and 5.3 in <code>nn_model()</code></h4><p><strong>Question:</strong> Build your neural network model in <code>nn_model()</code>.</p><p><strong>Instructions: </strong> The neural network model has to use the previous functions in the right order.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all function into the nerual network model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Build your neural network in nn_model</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X: dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">        @Y: labels of shape (1, number of exampless)</span></span><br><span class="line"><span class="string">        @n_h: size of the hidden layer</span></span><br><span class="line"><span class="string">        @num_iterations: Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">        @print_cost: if True, print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @parameters: parameters learnt by the model. They can then be used to predict</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize paramters, then retrieve W1, b1, W2, b2.</span></span><br><span class="line">    <span class="comment"># Inputs: "n_x, n_h, n_y"</span></span><br><span class="line">    <span class="comment"># Outputs: " parameters(W1, b1, W2, b2)"</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation. </span></span><br><span class="line">        <span class="comment"># Inputs: "X, parameters". </span></span><br><span class="line">        <span class="comment"># Outputs: "A2, cache"</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost Function.</span></span><br><span class="line">        <span class="comment"># Inputs: "A2, Y, parameters"</span></span><br><span class="line">        <span class="comment"># Output: "cost"</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, cache, X, Y"</span></span><br><span class="line">        <span class="comment"># Outputs: "grads"</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters by using gradient descent.</span></span><br><span class="line">        <span class="comment"># Inputs: "parameters, grads"</span></span><br><span class="line">        <span class="comment"># Outputs: "parameters"</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations:</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iterations %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.692739</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.000218</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.000107</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.000071</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.000053</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.000042</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.000035</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.000030</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.000026</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.000023</span></span><br><span class="line">W1 = [[<span class="number">-0.65848169</span>  <span class="number">1.21866811</span>]</span><br><span class="line"> [<span class="number">-0.76204273</span>  <span class="number">1.39377573</span>]</span><br><span class="line"> [ <span class="number">0.5792005</span>  <span class="number">-1.10397703</span>]</span><br><span class="line"> [ <span class="number">0.76773391</span> <span class="number">-1.41477129</span>]]</span><br><span class="line">b1 = [[ <span class="number">0.287592</span>  ]</span><br><span class="line"> [ <span class="number">0.3511264</span> ]</span><br><span class="line"> [<span class="number">-0.2431246</span> ]</span><br><span class="line"> [<span class="number">-0.35772805</span>]]</span><br><span class="line">W2 = [[<span class="number">-2.45566237</span> <span class="number">-3.27042274</span>  <span class="number">2.00784958</span>  <span class="number">3.36773273</span>]]</span><br><span class="line">b2 = [[ <span class="number">0.20459656</span>]]</span><br></pre></td></tr></table></figure><h4 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h4><p><strong>Question:</strong> Use your model to predict by building <code>predict()</code>. Use forward propagation to predict results.</p><p><strong>Reminder:</strong> </p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure8.png" alt="Figure8"></p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use forward propagation to predict results</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments: </span></span><br><span class="line"><span class="string">        @parameters: python dictionary containing your paramters</span></span><br><span class="line"><span class="string">        @X: input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @predictions: vector of predictions of our model (red: 0 / bule: 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classfies to 0/1 using 0.5 as threshold</span></span><br><span class="line">    A2 = forward_propagation(X, parameters)[<span class="number">0</span>]</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p><strong>Test:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions mean = <span class="number">0.666666666667</span></span><br></pre></td></tr></table></figure><font color="#0099">It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of `n_h` hidden units.</font><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693048</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.288083</span></span><br><span class="line">Cost after iteration <span class="number">2000</span>: <span class="number">0.254385</span></span><br><span class="line">Cost after iteration <span class="number">3000</span>: <span class="number">0.233864</span></span><br><span class="line">Cost after iteration <span class="number">4000</span>: <span class="number">0.226792</span></span><br><span class="line">Cost after iteration <span class="number">5000</span>: <span class="number">0.222644</span></span><br><span class="line">Cost after iteration <span class="number">6000</span>: <span class="number">0.219731</span></span><br><span class="line">Cost after iteration <span class="number">7000</span>: <span class="number">0.217504</span></span><br><span class="line">Cost after iteration <span class="number">8000</span>: <span class="number">0.219454</span></span><br><span class="line">Cost after iteration <span class="number">9000</span>: <span class="number">0.218607</span></span><br><span class="line">Accuracy: <span class="number">90</span>%</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure9.png" alt="Figure9"></p><p><strong>Interpretation:</strong>  Accuracy is really high compared to Logistic Regression. The model has learned the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.</p><p>Now, let’s try out several hidden layer sizes.</p><h4 id="Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="Tuning hidden layer size(optional/ungraded exercise)"></a>Tuning hidden layer size(optional/ungraded exercise)</h4><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)                                <span class="comment"># ？？？</span></span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>) </span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)  <span class="comment"># ???</span></span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Accuracy <span class="keyword">for</span> <span class="number">1</span> hidden units: <span class="number">67.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">2</span> hidden units: <span class="number">67.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">3</span> hidden units: <span class="number">90.75</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">4</span> hidden units: <span class="number">90.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">5</span> hidden units: <span class="number">91.25</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">20</span> hidden units: <span class="number">90.0</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="number">50</span> hidden units: <span class="number">90.25</span> %</span><br></pre></td></tr></table></figure><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure11.png" alt="Figure11"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure12.png" alt="Figure12"></p><p><img src="/2019/08/03/课程一(Neural Networks and Deep Learning), 第三周(Shallow neural networks)——Programming assignment 3、Planar data classification with a hidden layer/Figure10.png" alt="Figure10"></p><p><strong>Interpretation:</strong></p><ul><li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.</li><li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting.</li><li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting.</li></ul><p><strong><font color="#00f">You’ve learnt to:</font></strong></p><ul><li>Build a complete neural network with a hidden layer</li><li>Make a good use of a non-linear unit</li><li>Implemented forward propagation and backpropagation, and trained a neural network</li><li>See the impact of varying the hidden layer size, including overfitting.</li></ul><p>Nice work!</p><hr><h3 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment3_shallow_neural_network" target="_blank" rel="noopener">Source Code</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Planar-data-classification-with-a-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-a-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with a hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with a hidden layer&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Coursera" scheme="http://sunfeng.online/tags/Coursera/"/>
    
      <category term="Neural Network" scheme="http://sunfeng.online/tags/Neural-Network/"/>
    
      <category term="Deep learning" scheme="http://sunfeng.online/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>如何上传本地代码到github</title>
    <link href="http://sunfeng.online/2019/08/02/%E5%A6%82%E4%BD%95%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%88%B0github/"/>
    <id>http://sunfeng.online/2019/08/02/如何上传本地代码到github/</id>
    <published>2019-08-02T08:34:43.000Z</published>
    <updated>2019-08-02T08:57:37.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何上传本地代码到github"><a href="#如何上传本地代码到github" class="headerlink" title="如何上传本地代码到github"></a>如何上传本地代码到github</h2><a id="more"></a><h3 id="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："><a href="#第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：" class="headerlink" title="第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示："></a>第一步：去github上创建自己的仓库Repository，创建后的页面如下图所示：</h3><p><img src="/2019/08/02/如何上传本地代码到github/figure1.jpg" alt="figure1"></p><p>点击<strong>Clone or download</strong>按钮，复制弹出的地址<strong>git@github.com:***/***.git</strong></p><p>注意要用SSH地址。</p><hr><h3 id="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"><a href="#第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令" class="headerlink" title="第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令"></a>第二步：建立git仓库，cd到你的本地项目根目录下，执行git命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><hr><h3 id="第三步：将项目的所有文件添加到仓库中"><a href="#第三步：将项目的所有文件添加到仓库中" class="headerlink" title="第三步：将项目的所有文件添加到仓库中"></a>第三步：将项目的所有文件添加到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><hr><h3 id="第四步：将添加的文件提交到仓库中"><a href="#第四步：将添加的文件提交到仓库中" class="headerlink" title="第四步：将添加的文件提交到仓库中"></a>第四步：将添加的文件提交到仓库中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;注释语句&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="第五步：将本地仓库关联到github上"><a href="#第五步：将本地仓库关联到github上" class="headerlink" title="第五步：将本地仓库关联到github上"></a>第五步：将本地仓库关联到github上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:***/test.git</span><br></pre></td></tr></table></figure><hr><h3 id="第六步：上传之前，先要pull一下，执行如下命令："><a href="#第六步：上传之前，先要pull一下，执行如下命令：" class="headerlink" title="第六步：上传之前，先要pull一下，执行如下命令："></a>第六步：上传之前，先要pull一下，执行如下命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure><hr><h3 id="第七步：上传代码到github远程仓库"><a href="#第七步：上传代码到github远程仓库" class="headerlink" title="第七步：上传代码到github远程仓库"></a>第七步：上传代码到github远程仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><hr><p><strong>祝你成功！</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;如何上传本地代码到github&quot;&gt;&lt;a href=&quot;#如何上传本地代码到github&quot; class=&quot;headerlink&quot; title=&quot;如何上传本地代码到github&quot;&gt;&lt;/a&gt;如何上传本地代码到github&lt;/h2&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="github" scheme="http://sunfeng.online/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset</title>
    <link href="http://sunfeng.online/2019/08/01/%E8%AF%BE%E7%A8%8B%E4%B8%80(Neural%20Networks%20and%20Deep%20Learning)%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%88Basics%20of%20Neural%20Network%20programming%EF%BC%89%E2%80%94%E2%80%94%20Programming%20assignment%202%E3%80%81Logistic%20Regression%20with%20a%20Neural%20Network%20mindset/"/>
    <id>http://sunfeng.online/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/</id>
    <published>2019-08-01T08:06:21.000Z</published>
    <updated>2019-08-02T07:54:57.378Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a><strong>Logistic Regression with a Neural Network mindset</strong></h3><a id="more"></a><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li><p>Build the general architecture of a learning algorithm, including:</p><ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent)</li></ul></li><li><p>Gather all three functions above into a main function, in the right order.</p></li></ul><hr><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p><ul><li><a href="https://hub.coursera-notebooks.org/user/rdzflaokljifhqibzgygqq/notebooks/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/www.numpy.org" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li></ul><p>code ————-&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure><hr><h3 id="Overview-of-the-Problem-set"><a href="#Overview-of-the-Problem-set" class="headerlink" title="Overview of the Problem set"></a>Overview of the Problem set</h3><p><strong>Problem Statement:</strong> You are given a dataset (“data.h5”) containing:</p><blockquote><ul><li>a training set of m_train images labeled as cat (y = 1) or non-cat (y = 0)</li><li>a test set of m_test images labeled as cat or non-cat</li><li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)</li></ul></blockquote><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data preprocessing</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_Preprocess</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load dataset and preprocess dataset</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @train_set_x: your train set features</span></span><br><span class="line"><span class="string">        @train_set_y: your train set labels</span></span><br><span class="line"><span class="string">        @test_set_x: your test set features</span></span><br><span class="line"><span class="string">        @test_set_y: your test set labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line">    <span class="comment"># load dataset from dataset files</span></span><br><span class="line">    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">    <span class="comment"># vectorize the features of each examples</span></span><br><span class="line">    train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">    test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line">    <span class="comment"># normalize the features vector</span></span><br><span class="line">    <span class="keyword">return</span> train_set_x, train_set_y, test_set_x, test_set_y, classes</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:**</font><font color="#0099ff">Common steps for pre-processing a new dataset are:</font><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each examples is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</li></ul><hr><h3 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h3><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure1.png" alt="Figure1"></p><p><strong>Mathematical expression of the algorithm:</strong></p><p>For one examples x(i):</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/Figure2.png" alt="Figure2"></p><p>The cost is the computed by summing over all training examples:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure3.png" alt="Figure3"></p><p><strong>Key steps:</strong> In this exercise, you will carry out the following steps:</p><blockquote><ul><li>Initialize the parameters of the model</li><li>Learn the parameters for the model by minimizing the cost</li><li>Use the learned parameters to make predictions (on the test set)</li><li>Analyze the results and conclude</li></ul></blockquote><hr><h3 id="Building-the-parts-of-our-algorithm"><a href="#Building-the-parts-of-our-algorithm" class="headerlink" title="Building the parts of our algorithm"></a>Building the parts of our algorithm</h3><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features)</li><li>Initialize the model’s parameters</li><li>Loop:<ul><li><strong>Calculate current loss (forward propagation)</strong></li><li><strong>Calculate current gradient (backward propagation)</strong></li><li><strong>Update parameters (gradient descent)</strong></li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call model().</p><h4 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h4><p><strong>Exercise:</strong> using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute </p><script type="math/tex; mode=display">sigmoid(w^T + b) = \frac{1}{1 + e^{-(w^T + b)}}</script><p>to make predictions. Use np.exp().</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @z: A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @s: sigmoid(z)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h4 id="Initializing-parameters"><a href="#Initializing-parameters" class="headerlink" title="Initializing parameters"></a>Initializing parameters</h4><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You will initialize w as a vector of zeros. If you don’t know what numpy function to use, loop up np.zeros() in the Numpy library’s documentation.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initializing parameters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">        @dim: size of the w vector we want</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @w: initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">        @b: initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function propagation() that computes the cost function and its gradient.</p><p><strong>Hints(提示):</strong></p><font color="#0099ff">Forward Propagation:</font><ul><li><p>You get x</p></li><li><p>You compute $ A = \sigma(w^TX + b) = (a^{(0)},a^{(1)},…a^{(m-1)},a^{(m)})$</p></li><li><p>You calculate the cost function: </p><p>$J = - \frac{1}{m}\sum_{i = 1}^{m}{y^{(i)}log(a^{(i)}) +(1 -  y^{(i)})log(1 - a^{(i)}) }$</p></li></ul><p>Here are the two formulas you will be using:</p><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure4.png" alt="Figure4"></p><p><strong>code: </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward and Backword propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the</span></span><br><span class="line"><span class="string">    propagation explained above</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: true "label" vector(containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        @cost: negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">        @dw: gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">        @db: gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># nx</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b))  <span class="comment"># compute activation</span></span><br><span class="line">    cost = -(np.dot(Y, np.log(A).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A).T)) / m <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m <span class="comment"># compute dw</span></span><br><span class="line">    db = np.sum(A - Y) / m      <span class="comment"># compute db</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 把shape中为1的维度去掉</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    <span class="comment"># 判断剩下的是否为空</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, </span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and b by minimizing the cost function J. For a parameter θ, the update rule is θ = θ - α dθ, where α is the learning rate.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Optimization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">        @Y: ture "label" vector (contaning 0 if non-cat, 1 if cat), of shape(1, number of examples)</span></span><br><span class="line"><span class="string">        @num_iterations: number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">        @learning_rate: learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">        @print_cost: True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @params: dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">        @grads: dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">        @costs: list of all the costs computed during the optimization, this will be used to plot the learning curve</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">        You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        (1) Calculate the cost and the gradient for the current parameters. Use propagate()</span></span><br><span class="line"><span class="string">        (2) Update the parameters using gradient descent rule for w and b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation</span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update rule</span></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %d: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li>Calculate: $ \hat{Y} = A = \sigma(w^TX + b)$</li><li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_Prediction</code>. </li></ol><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Graded function: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned </span></span><br><span class="line"><span class="string">    logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @w: weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">        @b: bias, a scalar</span></span><br><span class="line"><span class="string">        @X: data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @Y_prediction: a numpy array (vector) containing all prediction</span></span><br><span class="line"><span class="string">                       (0 / 1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m =  X.shape[<span class="number">1</span>]     <span class="comment"># number of examples</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    A = sigmoid(np.add(np.dot(w.T, X), b)) <span class="comment"># (1, m)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0, i] to actual predictions p[0, i]</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &lt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><font color="#0099ff">**What you need remember:** You've implemented several functions that:</font><ul><li>initialize (w, b)</li><li>Optimize the loss iteratively to learn parameters (w, b):<ul><li>computing the cost and its gradient</li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w, b) to predict the labels for a given set of examples</li></ul><hr><h3 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h3><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p><blockquote><ul><li>Y_prediction for your predictions on the test set</li><li>Y_prediction_train for your predictions on the train set</li><li>w, costs, grads for the outputs of optimize()</li></ul></blockquote><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge all functions into a model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function</span></span><br><span class="line"><span class="string">    you have implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        @X_train: training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">        @Y_train: training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">        @X_test: test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">        @Y_test: test labels represented by a numpy array (vetcor) of shape (1, m_test)</span></span><br><span class="line"><span class="string">        @num_iterations: hyperparmeter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">        @learning_rate: hyperparmeter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">        @print_cost: Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        @d: dictionary containing information about the model</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initailize parameters with zeros</span></span><br><span class="line">    w,b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])   <span class="comment"># num_px * num_px * 3, w: (dim, 1), b: a scalar</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples </span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span> : costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span> : Y_prediction_test,</span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">"w"</span> : w,</span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span> : num_iterations</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations =<span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">Cost after iteration <span class="number">300</span>: <span class="number">0.376007</span></span><br><span class="line">Cost after iteration <span class="number">400</span>: <span class="number">0.331463</span></span><br><span class="line">Cost after iteration <span class="number">500</span>: <span class="number">0.303273</span></span><br><span class="line">Cost after iteration <span class="number">600</span>: <span class="number">0.279880</span></span><br><span class="line">Cost after iteration <span class="number">700</span>: <span class="number">0.260042</span></span><br><span class="line">Cost after iteration <span class="number">800</span>: <span class="number">0.242941</span></span><br><span class="line">Cost after iteration <span class="number">900</span>: <span class="number">0.228004</span></span><br><span class="line">Cost after iteration <span class="number">1000</span>: <span class="number">0.214820</span></span><br><span class="line">Cost after iteration <span class="number">1100</span>: <span class="number">0.203078</span></span><br><span class="line">Cost after iteration <span class="number">1200</span>: <span class="number">0.192544</span></span><br><span class="line">Cost after iteration <span class="number">1300</span>: <span class="number">0.183033</span></span><br><span class="line">Cost after iteration <span class="number">1400</span>: <span class="number">0.174399</span></span><br><span class="line">Cost after iteration <span class="number">1500</span>: <span class="number">0.166521</span></span><br><span class="line">Cost after iteration <span class="number">1600</span>: <span class="number">0.159305</span></span><br><span class="line">Cost after iteration <span class="number">1700</span>: <span class="number">0.152667</span></span><br><span class="line">Cost after iteration <span class="number">1800</span>: <span class="number">0.146542</span></span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure><p><strong>Comment:</strong> Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. </p><font color="#0099ff">Let's also plot the cost function and the gradients:</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure5.png" alt="Figure5"></p><p><strong>Interpretation</strong>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p><hr><h3 id="Further-analysis"><a href="#Further-analysis" class="headerlink" title="Further analysis"></a>Further analysis</h3><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate α.</p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate α determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p><p><strong>Code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow= <span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>Result:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="/2019/08/01/课程一(Neural Networks and Deep Learning)，第二周（Basics of Neural Network programming）—— Programming assignment 2、Logistic Regression with a Neural Network mindset/figure6.png" alt="Figure6"></p><p><strong>Interpretation</strong>:</p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you:<ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li></ul></li></ul><hr><p><a href="https://github.com/sunfeng2016/Coursera_deeplearning_ai/tree/master/course1_deep_learning_and_neural_network/assignment2_logistics_regression" target="_blank" rel="noopener">Source Code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic-Regression-with-a-Neural-Network-mindset&quot;&gt;&lt;a href=&quot;#Logistic-Regression-with-a-Neural-Network-mindset&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression with a Neural Network mindset&quot;&gt;&lt;/a&gt;&lt;strong&gt;Logistic Regression with a Neural Network mindset&lt;/strong&gt;&lt;/h3&gt;
    
    </summary>
    
      <category term="Coursera深度学习笔记" scheme="http://sunfeng.online/categories/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Neural Networks" scheme="http://sunfeng.online/tags/Neural-Networks/"/>
    
      <category term="Deep Learning" scheme="http://sunfeng.online/tags/Deep-Learning/"/>
    
      <category term="Logistic Regression" scheme="http://sunfeng.online/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Hexo教程：使用Hexo写博客</title>
    <link href="http://sunfeng.online/2019/07/31/hello-hexo-markdown/"/>
    <id>http://sunfeng.online/2019/07/31/hello-hexo-markdown/</id>
    <published>2019-07-31T12:44:47.000Z</published>
    <updated>2019-08-02T08:54:22.531Z</updated>
    
    <content type="html"><![CDATA[<p>尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。<br><a id="more"></a></p><hr><h3 id="新建文档"><a href="#新建文档" class="headerlink" title="新建文档"></a>新建文档</h3><p>假设我们新建的文章名为 “hello hexo markdown”，在命令行键入以下命令即可：</p><blockquote><p>$ hexo new “hello hexo markdown”</p></blockquote><p>上述命令的结果是在 <code>./hexo/source/_posts</code> 路径下新建了一个 <code>hello-hexo-markdown.md</code> 文件。</p><p>然后，我们就可以打开编辑器尽情地写作了。</p><hr><h3 id="文档格式"><a href="#文档格式" class="headerlink" title="文档格式"></a>文档格式</h3><p>我们使用文本编辑器打开刚刚新建的 <code>hello-hexo-markdown.md</code> 文件，会发现其中已经存在内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2019-07-31 20:44:47</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>这些内容是干嘛的呢？事实上，他们就是用于设置 MarkDown 文档在被解析为静态网页文件时的相关配置，这些配置参数一般位于文件中最上方以 <code>---</code> 分隔的区域。其中，</p><p><code>title</code>的值是当前文档名，也是将来在网页中显示的文章标题</p><p><code>date</code>值是我们新建文档时的当时地区时间</p><p><code>tags</code>值是文档的标签，我们可以随意赋值，为文档贴标签，其用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>上面的配置参数为这篇文档贴上了 <code>hello</code>、<code>hexo</code>、<code>mardown</code> 标签，如果站点使用的主题支持标签功能，MarkDown 文档被解析为静态网页文件后就可以看到效果。</p><p>除了以上这些，还有很多预先定义的参数<a href="https://hexo.io/zh-cn/docs/front-matter.html" target="_blank" rel="noopener">Front_mtter</a>，我们这里选取一个常用且较为典型的配置参数<code>categories</code>讲解一下。</p><h4 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h4><p><code>categories</code> 是用来给文章分类的，它跟 <code>tags</code> 不同的是其具有顺序性和层次性。</p><p>例如，我们写一篇关于 CSS3 动画的文章，我们可能会为其打标签 ”CSS3“、”动画“等，但是我们却会将其分在 CSS/CSS3 类别下，这个是有一定的相关性、顺序性和层次性。简单来说，<code>categories</code> 有点儿像新建文件夹对文档进行分门别类的归置。</p><p><code>categories</code> 的用法同 <code>tags</code> 一样，只不过斗个 categories 值是分先后顺序的。</p><hr><h3 id="引用资源"><a href="#引用资源" class="headerlink" title="引用资源"></a>引用资源</h3><p>写个博客，有时候我们会想添加个图片啦 O.O，或者其他形式的资源，等等。</p><p>这时，有两种解决办法：</p><ol><li>使用绝对路径引用资源，在 Web 世界中就是资源的 URL</li><li>使用相对路径引用资源</li></ol><h4 id="文章资料文件夹"><a href="#文章资料文件夹" class="headerlink" title="文章资料文件夹"></a>文章资料文件夹</h4><p>如果是使用相对路径引用资源，那么我们可以使用 Hexo 提供的资源文件夹功能。</p><p>使用文本编辑器打开站点根目录下的 <code>_ config.yml</code> 文件，将 <code>post_asset_folder</code> 值设置为 <code>true</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>上面的操作会开启 Hexo 的文章资源文件管理功能。Hexo 将会在我们每一次通过 <code>hexo new &lt;title&gt;</code> 命令创建新文章时自动创建一个同名文件夹，于是我们便可以将文章所引用的相关资源放到这个同名文件夹下，然后通过相对路径引用。</p><h4 id="相对路径引用的标签插件"><a href="#相对路径引用的标签插件" class="headerlink" title="相对路径引用的标签插件"></a>相对路径引用的标签插件</h4><p>通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。我们可以通过使用 Hexo 提供的标签插件来解决这个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>比如说：当你打开文章资源文件夹功能后，你把一个 <code>example.jpg</code> 图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 <code>![](/example.jpg)</code> ，它将 <em>不会</em> 出现在首页上。（但是它会在文章中按你期待的方式工作）</p><p><strong>！！！注意：</strong> 如果已经开启了文章的资源文件夹功能，当使用 MarkDown 语法引用相对路径下的资源时，只需 <code>./资源名称</code>，不用在引用路径中添加同名文件夹目录层级。</p><p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img example.jpg This is an example image %&#125;</span><br></pre></td></tr></table></figure><p>通过这种方式，图片将会同时出现在文章和主页以及归档页中。</p><hr><h3 id="文章摘要"><a href="#文章摘要" class="headerlink" title="文章摘要"></a>文章摘要</h3><p>有的时候，主题模板配置的不够好的话，Hexo 最终生成的静态站点是不会自动生成文章摘要的。</p><p>所以，为了保险起见，我们也自己手动设置文章摘要，这样也方便避免自动生成的摘要不优雅的情况。</p><p>设置文章摘要，我们只需在想显示为摘要的内容之后添 <code>&lt;!-- more --&gt;</code> 即可。像下面这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hello hexo markdown</span><br><span class="line">date: 2016-11-16 18:11:25</span><br><span class="line">tags:</span><br><span class="line">- hello</span><br><span class="line">- hexo</span><br><span class="line">- markdown</span><br><span class="line">---</span><br><span class="line">我是短小精悍的文章摘要(๑•̀ㅂ•́)و✧</span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line">紧接着文章摘要的正文内容</span><br></pre></td></tr></table></figure><p>这样，<code>&lt;!-- more --&gt;</code> 之前、文档配置参数之后中的内容便会被渲染为站点中的文章摘要。</p><p>注意！文章摘要在文章详情页是正文中最前面的内容。</p><hr><h3 id="生成文件"><a href="#生成文件" class="headerlink" title="生成文件"></a>生成文件</h3><h4 id="清除缓存文件"><a href="#清除缓存文件" class="headerlink" title="清除缓存文件"></a>清除缓存文件</h4><p>为了避免不必要的错误，在生成静态文件前，强烈建议先运行一下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><p>上述命令会清除在本地站点文件下的缓存文件(<code>db.json</code>)和已有的静态文件(<code>public</code>).</p><h4 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h4><p>写好MarkDown文档之后，我们就可以使用以下命令生成静态文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>然后我们就可以启动 Hexo 服务器，使用浏览器打开 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000</a> 查看效果了。</p><hr><h3 id="示范"><a href="#示范" class="headerlink" title="示范"></a>示范</h3><p>下图是一篇经过配置的简单文档，生成静态文件后在网站首页显示的结果。我们可以看到手动设置的摘要，以及打的标签生效了。</p><p><img src="/2019/07/31/hello-hexo-markdown/example.png" alt="效果截图"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管 Hexo 支持 MarkDown，但是我们却不能像写单独的 MarkDown 文档时那样肆无忌惮。由于我们所写的文档是需要被解析为静态网页文件的，所以我们必须严格遵从 Hexo 的规范，这样才能解析出条理清晰的静态网页文件。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://sunfeng.online/categories/others/"/>
    
    
      <category term="hello" scheme="http://sunfeng.online/tags/hello/"/>
    
      <category term="hexo" scheme="http://sunfeng.online/tags/hexo/"/>
    
      <category term="markdown" scheme="http://sunfeng.online/tags/markdown/"/>
    
  </entry>
  
</feed>
